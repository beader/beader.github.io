<?xml version="1.0" encoding="UTF-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title><![CDATA[Beader's blog]]></title>
  <subtitle><![CDATA[something about Statistics, Data Mining and R]]></subtitle>
  <link href="http://beader.me/atom.xml" rel="self"/>
  <link href="http://beader.me"/>
  <updated>2014-09-03T10:54:16.431Z</updated>
  <id>http://beader.me/</id>
  <author>
    <name><![CDATA[beader]]></name>
    <email><![CDATA[beader.chen@gmail.com]]></email>
  </author>
  <generator uri="http://zespia.tw/hexo">Hexo</generator>
  <entry>
    <title type="html"><![CDATA[机器学习笔记-非线性转换]]></title>
    <link href="http://beader.me/2014/08/30/nonlinear-transformation/"/>
    <id>http://beader.me/2014/08/30/nonlinear-transformation/</id>
    <published>2014-08-30T09:00:00.000Z</published>
    <updated>2014-08-31T15:51:44.000Z</updated>
    <content type="html"><![CDATA[<p>笔记整理自台大林轩田老师的开放课程-<a href="https://www.coursera.org/course/ntumlone" target="_blank" rel="external">机器学习基石</a>，笔记中所有图片来自于课堂讲义。</p>
<p>&emsp;&emsp;前面的笔记所谈到的分类模型，都是基于线性的，即我们假设数据是线性可分，或者至少看起来用一条线来做分类是不错的。但现实中我们的数据往往不那么容易得能用一条线区分开来。</p>
<p><img src="/imgs/nonlinear-transformation/linear-vs-nonlinear.png" alt=""></p>
<p><a id="more"></a><br>&emsp;&emsp;可以发现$\mathcal{D}$并不是一个线性可分的数据，但使用一个圆圈，却可以很好得把圈圈和叉叉区分开来。这个”圆圈分类器”可以是下面这种形式:</p>
<p>$$h_{SEP}(x)=sign(-x_1^2-x_2^2+0.6)$$</p>
<p>&emsp;&emsp;看样子我们需要重新设计分类模型与算法，做一个圆圈版本的PLA，圆圈版本的Regression。所以接下来几篇笔记应该是$\color{orange}{\text{Circular}}-\text{PLA}$、$\color{orange}{\text{Circular}}-\text{Regression}$。事实上是不需要这么麻烦的，下面我们来分析下那个圈圈分类器的方程：</p>
<script type="math/tex; mode=display">
\begin{aligned}
\color{orange}{h}(x) &= 
(\;
\underbrace{\color{orange}{0.6}}_{\color{orange}{\tilde{w}_0}}\cdot\underbrace{\color{purple}{1}}_{\color{purple}{z_0}} + \underbrace{(\color{orange}{-1})}_{\color{orange}{\tilde{w}_1}}\cdot\underbrace{\color{purple}{x_1^2}}_{\color{purple}{z_1}}+
\underbrace{(\color{orange}{-1})}_{\color{orange}{\tilde{w}_2}}\cdot\underbrace{\color{purple}{x_2^2}}_{\color{purple}{z_2}}
\;)
\\\
&= sign(\color{orange}{\tilde{w_0}}+\color{orange}{\tilde{w_1}}\color{purple}{z_1}+\color{orange}{\tilde{w_2}}\color{purple}{z_2})
\\\
&=sign(\color{orange}{\tilde{w}^T}\color{purple}{z})
\end{aligned}
</script>

<h2 id="非线性变换">非线性变换</h2>
<p>&emsp;&emsp;如果我们只看包含$\color{purple}{z}$的部分，它事实上依然是一个线性方程。即，$\mathcal{X}$空间下的一个圆圈，对应到$\mathcal{Z}$空间下的一条直线。</p>
<p><img src="/imgs/nonlinear-transformation/two-space.png" alt=""></p>
<p>&emsp;&emsp;这个转换的过程成为nonlinear feature transform，用符号$\phi$表示，$\phi$把两个互相独立的空间给联系了起来:</p>
<p>$$(1,x_1^2,x_2^2)=\color{purple}{\phi}(x)=\color{purple}{(z_0,z_1,z_2)}=\color{purple}{z}$$</p>
<p>&emsp;&emsp;$\mathcal{X}$空间下的每个点，都对应$\mathcal{Z}$空间下的某个点，同样的，$\mathcal{X}$空间下的二次曲线方程，都对应$\mathcal{Z}$空间下的某个一次直线方程。</p>
<p>$$h(x)=sign(\color{orange}{\tilde{w}_0}+\color{orange}{\tilde{w}_1}\color{purple}{x_1^2}+\color{orange}{\tilde{w}_2}\color{purple}{x_2^2})=sign(\color{orange}{\tilde{w}^T}\color{purple}{\phi}(x))=\color{orange}{\tilde{h}(\color{purple}{z})}$$</p>
<p>&emsp;&emsp;这样以来，两个空间，在“冥冥之中”，产生了关联，前面说的$\phi$就是这两个空间的纽带，在这个纽带下，$\mathcal{Z}$空间下的不同直线也就对应$\mathcal{X}$下的不同形态的分类器。</p>
<table>
<thead>
<tr>
<th style="text-align:center">$\tilde{w}$</th>
<th style="text-align:left">X空间下的曲线形态</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">(0.6,−1,−1)</td>
<td style="text-align:left">circle(圈圈在内部，叉叉在外部)</td>
</tr>
<tr>
<td style="text-align:center">(−0.6,+1,+1)</td>
<td style="text-align:left">circle(圈圈在外部，叉叉在内部)</td>
</tr>
<tr>
<td style="text-align:center">(0.6,−1,−2)</td>
<td style="text-align:left">ellipse</td>
</tr>
<tr>
<td style="text-align:center">(0.6,−1,+2)</td>
<td style="text-align:left">hyperbola</td>
</tr>
<tr>
<td style="text-align:center">(0.6,+1,+2)</td>
<td style="text-align:left">所有点都判断为圈圈</td>
</tr>
</tbody>
</table>
<p>&emsp;&emsp;更加一般化，我们把一次空间$\mathcal{X}$映射到二次空间$\mathcal{Z}$的时候，还会保留其一次项，即下面这个样子的映射才是完整的二次映射:</p>
<p>$$\color{purple}{\phi}(x)=(\color{purple}{1,x_1,x_2,x_1^2,x_1x_2,x_2^2})$$</p>
<p>&emsp;&emsp;这样一来，这个完整版的$\mathcal{Z}$空间的直线，就可以代表$\mathcal{X}$空间下的所有二次曲线了。</p>
<p>&emsp;&emsp;说了这么多，我们回想一下，我们为何要做这个非线性变换？逻辑是这样的：</p>
<ul>
<li>在$\mathcal{X}$空间中，我们使用一条直线，很难把圈圈和叉叉分开</li>
<li>但是如果我们使用一条曲线，可以很容易做到</li>
<li>可我们只学过找最优“直线”的算法，没有学过找最佳“曲线”的算法</li>
<li>我们有一个纽带$\phi$，它能够把$\mathcal{X}$空间中的所有曲线，对应到$\mathcal{Z}$空间中的曲线，反过来也可以根据$\mathcal{Z}$空间的一条直线，找到$\mathcal{X}$空间中对应的曲线</li>
<li>所以我们可以把$\mathcal{X}$空间下的原始数据$\mathcal{D}$映射到$\mathcal{Z}$空间下</li>
<li>然后在$\mathcal{Z}$空间下寻找最佳的“直线”，找直线这件事我们已经学过该怎么做了</li>
<li>找到这条最佳的“直线”后，把它对应回$\mathcal{X}$空间，就是我们刚开始说的需要找的那条最佳的“曲线”了。</li>
</ul>
<p><img src="/imgs/nonlinear-transformation/nonlinear-transform-steps.png" alt=""></p>
<p>&emsp;&emsp;实际操作上，分以下三步：</p>
<ol>
<li>变换原始数据 ${(x_n,y_n)}\Rightarrow {(\color{purple}{z_n}=\color{purple}{\phi}(x_n),y_n)}$</li>
<li>利用之前学过的算法，使用${(\color{purple}{z_n},y_n)}$训练线性模型，得到$\color{orange}{\tilde{w}}$</li>
<li>得到分类器方程$g(x)=sign(\color{orange}{\tilde{w}^T}\color{purple}{\phi_2}(x))$</li>
</ol>
<h2 id="非线性变换的代价">非线性变换的代价</h2>
<h3 id="Q-th_Order_Polynomial_Transform">Q-th Order Polynomial Transform</h3>
<p>&emsp;&emsp;d维向量$x$经过Q次多项式变换:</p>
<script type="math/tex; mode=display">
\begin{matrix}
\color{purple}{\phi}(x)=(&1, \\\
&\color{blue}{x_1,x_2,...,x_d,}\\\ 
&\color{blue}{x_1^2,x_1x_2,...,x_d^2},\\\ 
&...,\\\ 
&\color{blue}{x_1^Q,x_1^{Q-1}x_2,...,x_d^Q}\;)
\end{matrix}
</script>

<p>&emsp;&emsp;(以上是Q次完整变换，当然我们不一定会需要完整项，譬如之前那个圆圈，就舍弃了$x_1x_2$项)<br>&emsp;&emsp;若考虑完整变化，原来的$1+d$维向量经过变换，就成了$1+\tilde{d}=1+\binom{Q+d}{Q}$维向量，复杂度由$O(d)$变为$O(Q^d)$。</p>
<h3 id="1-_计算变复杂，需要的存储空间增大">1. 计算变复杂，需要的存储空间增大</h3>
<p>&emsp;&emsp;这点很容易理解，如之前的例子，原始数据$(1,x_1,x_2)$经过变换之后变成$(1,x_1,x_2,x_1^2,x_1x_2,x_2^2)$，需要多一倍的空间来存转换后的数据，同时参数的增加，也增大了计算量。</p>
<h3 id="2-_模型复杂度增大">2. 模型复杂度增大</h3>
<p>&emsp;&emsp;之前的笔记<a href="http://beader.me/2014/02/22/vc-dimension-three/" target="_blank" rel="external">VC Dimension, Part III</a>，有讲到自由度与VC Dimension的关系，线性模型的$d_{vc}\approx 自由度 \approx \tilde{d}+1$。如果Q非常大的话，则模型的$d_{vc}$也会非常大，我们知道$d_{vc}$越大，越容易实现小的$E_{in}$，同时，也会造成$|E_{out}-E_{in}|$的加大，即模型的泛化能力(generalization)变差。</p>
<h2 id="模型的泛化问题(Generalization_Issue)">模型的泛化问题(Generalization Issue)</h2>
<p><img src="/imgs/nonlinear-transformation/phi1-vs-phi4.png" alt=""></p>
<p>&emsp;&emsp;上图是分别使用原始数据进行训练以及进行4次非线性变换后的数据进行训练的结果对比。从视觉上看，虽然右图(经过4次非线性变换的模型)能够把圈圈叉叉完全分开，但显然这种模型过于复杂了。<br>&emsp;&emsp;很多时候我们会面临模型泛化能力和分类能力的权衡取舍，即：</p>
<table>
<thead>
<tr>
<th style="text-align:center">$\tilde{d}(Q)$</th>
<th style="text-align:center">$E_{out}(g)$能否与$E_{in}(g)$很接近?</th>
<th style="text-align:center">$E_{in}(g)$是否足够小?</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">higher</td>
<td style="text-align:center">:-(</td>
<td style="text-align:center">:-D</td>
</tr>
<tr>
<td style="text-align:center">lower</td>
<td style="text-align:center">:-D</td>
<td style="text-align:center">:-(</td>
</tr>
</tbody>
</table>
<p>(表中符号若不理解，请把屏幕右转90度。XD)</p>
<p>&emsp;&emsp;那么如何选择合适的复杂度呢？像上面一样用眼睛看？暂且不讨论10维的数据有没有办法用眼睛看，就拿前面2维的例子来说，用眼睛看来选择模型是件很危险的事情。</p>
<p>&emsp;&emsp;在你没看数据之前，你决定要进行完整的2次非线性变换，即:</p>
<script type="math/tex; mode=display">
\phi_2(x)=(1,x_1,x_2,x_1^2,x_1x_2,x_2^2),d_{vc}=6
</script>

<p>&emsp;&emsp;不过你没有忍住，偷偷先看了下数据:</p>
<p><img src="/imgs/nonlinear-transformation/circle-separable.png" alt=""></p>
<p>&emsp;&emsp;发现，用一个圆圈，就可以达到理想的效果，于是你决定放弃$\phi_2$变换中的一些维度，即使用:</p>
<script type="math/tex; mode=display">
z=(1,x_1^2,x_2^2)\;,d_{vc}=3
</script>

<p>&emsp;&emsp;或者干脆了，你觉得一个正圆就够了，使用:</p>
<script type="math/tex; mode=display">
z=(1,x_1^2+x_2^2)\;,d_{vc}=2
</script>

<p>&emsp;&emsp;最终你采用了$d_{vc}$最小的那个方案，因为根据前面说的，用这个正圆，可以完全分开，而且$d_{vc}=2$，并没有增大，真实两全其美。不过，这个时候，$d_{vc}$真的还是2吗？其实这个时候$d_{vc}$是比较难判断的，因为你是在“看过”数据之后，由你大脑选择的一个模型，这里要考虑到你大脑“选择模型”产生的一个复杂度，因为如果重新选取一部分数据，你有可能就不再挑选正圆模型了，事实上你的大脑不知不觉地参与到了模型参数估计上，偷偷地把2次变换中产生的其他项前面的系数，估计为0了。因此要切记，在$\phi$的选择之前，不可以偷偷去看数据。 倘若你在没看数据之前，就决定使用正圆，则此时，该模型的$d_{vc}=2$。</p>
<h2 id="Structured_Hypothesis_Sets">Structured Hypothesis Sets</h2>
<p>&emsp;&emsp;通常我们说的非线性变换，指的是多项式变换(Polynomial Transform)。用符号$\phi_Q$来表示$Q$次多项式变换:</p>
<script type="math/tex; mode=display">
\begin{align*}
\color{purple}{\phi_0}(x)=(1), 
\color{purple}{\phi_1}(x) &= (\phi_0(x),\color{blue}{x_1,x_2,...,x_d}) \\\ 
\color{purple}{\phi_2}(x) &= (\phi_1(x),\color{blue}{x_1^2,x_1x_2,...,x_d^2})\\\ 
\color{purple}{\phi_3}(x) &= (\phi_2(x),\color{blue}{x_1^3,x_1^2x_2,...,x_d^3})\\\ 
.&.....\\\
\color{purple}{\phi_Q}(x) &= (\phi_{Q-1}(x),\color{blue}{x_1^Q,x_1^{Q-1}x_2,...,x_d^Q})
\end{align*}
</script>

<p>&emsp;&emsp;可以发现$\phi_{i}(x)$中包含了$\phi_{i-1}(x)$，因此他们对应的Hypothesis Set也有如下关系:</p>
<script type="math/tex; mode=display">
\mathcal{H}_{\phi_0}\subset \mathcal{H}_{\phi_1} \subset \mathcal{H}_{\phi_2} \subset \mathcal{H}_{\phi_3} \subset \;...\; \subset \mathcal{H}_{\phi_Q}
</script>

<p>&emsp;&emsp;假如我们分别对原始数据进行$i$次非线性多项式变换并训练模型，$g_i$表示使用$i$次非线性多项式变换后的数据所训练出的最优模型，$\color{blue}{g_i=argmin_{h\in \mathcal{H}_i}E_{in}(h)}$则有:</p>
<script type="math/tex; mode=display">
\begin{matrix}
\mathcal{H}_{\phi_0} &\subset  &\mathcal{H}_{\phi_1}  &\subset  &\mathcal{H}_{\phi_2}  &\subset  &\mathcal{H}_{\phi_3}  &\subset  & ... \\\ 
\color{red}{d_{vc}(\mathcal{H}_0)}&\color{red}{\leq}  &\color{red}{d_{vc}(\mathcal{H}_1})  &\color{red}{\leq}  &\color{red}{d_{vc}(\mathcal{H}_2)}  &\color{red}{\leq}  &\color{red}{d_{vc}(\mathcal{H}_3)}  &\color{red}{\leq}  &\color{red}{...} \\\ 
\color{blue}{E_{in}(g_0)}&\color{blue}{\ge}  &  \color{blue}{E_{in}(g_1)}&\color{blue}{\ge}  &  \color{blue}{E_{in}(g_2)}&\color{blue}{\ge}  &  \color{blue}{E_{in}(g_3)}&\color{blue}{\ge}  & \color{blue}{...}
\end{matrix}
</script>

<p>&emsp;&emsp;通常在进行高次非线性变换的时候，应该特别小心，因为$d_{vc}$上升很快，极容易造成overfitting。比较安全的做法是，先尝试不做非线性变换，即使用$\mathcal{H}_{\phi_1}$，如果效果足够好了，就不需要进行非线性变换，如果效果不够好，再慢慢尝试使用复杂更高的模型。</p>
<p><img src="/imgs/nonlinear-transformation/model_complexity_curve.png" alt=""></p>
]]></content>
    <category scheme="http://beader.me/tags/非线性转换(Nonlinear-Transformation)/" term="非线性转换(Nonlinear Transformation)"/>
  </entry>
  <entry>
    <title type="html"><![CDATA[机器学习笔记-回顾几种线性模型]]></title>
    <link href="http://beader.me/2014/08/10/linear-model/"/>
    <id>http://beader.me/2014/08/10/linear-model/</id>
    <published>2014-08-10T05:00:00.000Z</published>
    <updated>2014-09-01T00:24:31.000Z</updated>
    <content type="html"><![CDATA[<p>笔记整理自台大林轩田老师的开放课程-<a href="https://www.coursera.org/course/ntumlone" target="_blank" rel="external">机器学习基石</a>，笔记中所有图片来自于课堂讲义。</p>
<p>&emsp;&emsp;前面的笔记介绍了三种线性模型，<a href="http://beader.me/2013/12/21/perceptron-learning-algorithm/" target="_blank" rel="external">PLA</a>、<a href="http://beader.me/2014/03/09/linear-regression/" target="_blank" rel="external">Linear Regression</a>与<a href="http://beader.me/2014/05/03/logistic-regression/" target="_blank" rel="external">Logistic Regression</a>。之所以称他们是线性模型，是因为这三种分类模型的方程中，都含有一个相同的部分，该部分是各个特征的一个线性组合，也可以称这个部分叫做线性评分方程:</p>
<p>$$\color{purple}{s}=w^Tx$$</p>
<a id="more"></a>

<h2 id="回顾三种线性模型">回顾三种线性模型</h2>
<p>&emsp;&emsp;严谨一点来说，PLA并不是一种“模型”，PLA (Perceptron Learning Algorithm) 是一种“算法”，用来寻找在“线性可分”的情况下，能够把两个类别完全区分开来的一条直线，所以我们简单的把PLA对应的那个模型就叫做Linear Classification。</p>
<p>&emsp;&emsp;下面对比下这三种模型:</p>
<p><img src="/imgs/linear-model/llloverview.png" alt="" title="llloverview.png"></p>
<ul>
<li>Linear Classification模型：取$\color{purple}{s}$的符号作为结果输出，使用0/1 error作为误差衡量方式，但它的cost function，也就是$E_{in}(w)$是一个离散的方程，并且该方程的最优化是一个NP-hard问题（简单说就是非常难解的问题）。</li>
<li>Linear Regression模型：直接输出评分方程，使用平方误差square error作为误差衡量方式，好处是其$E_{in}(w)$是一个凸二次曲线，非常方便求最优解(可通过矩阵运算一次得到结果)。</li>
<li>Logistic Regression模型：输出的是评分方程经过sigmoid的结果，使用cross-entropy作为误差衡量方式，其$E_{in}(w)$是一个凸函数，可以使用gradient descent的方式求最佳解。</li>
</ul>
<p>&emsp;&emsp;Linear Regression和Logistic Regression的输出是一个实数，而不是一个Binary的值，他们能用来解分类问题吗？可以，只要定一个阈值，高于阈值的输出+1，低于阈值的输出-1就好。既然Linear Regression和Logistic Regression都可以用来解分类问题，并且在最优化上，他们都比Linear Classification简单许多，我们能否使用这两个模型取代Linear Classification呢？</p>
<p>&emsp;&emsp;三个模型的区别在于误差的衡量，误差的衡量可以说是一个模型最重要的部分，这部分内容可以参考<a href="Noise%20and%20Error">Noise and Error</a>。</p>
<p><img src="/imgs/linear-model/lll_error_function.png" alt="" title="lll_error_function.png"></p>
<p>&emsp;&emsp;这里$y$是一个binary的值，要么是-1，要么是+1。注意到三个模型的error function都有一个$\color{blue}{y}\color{purple}{s}$的部分，也叫做分类正确性分数 (classification correctness score)。其中$\color{purple}{s}$是模型对某个样本给出的分数，$\color{blue}{y}$是该样本的真实值。</p>
<p>&emsp;&emsp;不难看出，当$\color{blue}{y}=+1$时，我们希望$\color{purple}{s}$越大越好，当$\color{blue}{y}=-1$时，我们希望$\color{purple}{s}$越小越好，所以总的来说，我们希望$\color{blue}{y}\color{purple}{s}$尽可能大。因此这里希望给较小的$\color{blue}{y}\color{purple}{s}$较大的cost，给较大的$\color{blue}{y}\color{purple}{s}$较小的cost即可。因此，不同模型的本质差异，就在于这个cost该怎么给。</p>
<p>&emsp;&emsp;既然这三个error function都与$\color{blue}{y}\color{purple}{s}$有关，我们可以以$\color{blue}{y}\color{purple}{s}$为横坐标，$err$为纵坐标，把这三个函数画出来。</p>
<p><img src="/imgs/linear-model/lll_error_function_vis.png" alt="" title="lll_error_function_vis.png"></p>
<p>&emsp;&emsp;sqr (squre error)为Linear Regression的误差函数，ce (cross entropy)为Logistic Regression的误差函数。可以看出，$\color{red}{err_{sqr}}$在$\color{blue}{y}\color{purple}{s}$较小的时候很大，但是，在$\color{blue}{y}\color{purple}{s}$较大的时候$\color{red}{err_{sqr}}$同样很大，这点不是很理想，因为我们希望$\color{blue}{y}\color{purple}{s}$大的时候cost要小，尽管如此，至少在$\color{red}{err_{sqr}}$小的时候，$\color{blue}{err_{0/1}}$也很小，因此可以拿来做error function。$err_{ce}$则是一个单调递减的函数，形态有点点像$\color{blue}{err_{0/1}}$，但来的比较平缓。注意到$err_{ce}$有一部分是小于$\color{blue}{err_{0/1}}$的，我们希望$err_{ce}$能成为$\color{blue}{err_{0/1}}$的一个upper bound（目的一会儿会说到），只要将$err_{ce}$做一个换底的动作，即：</p>
<p>$$\color{orange}{\text{scaled}}\text{ ce : err}_{\color{orange}{s}ce}(\color{purple}{s},\color{purple}{y})=\color{orange}{log_2}(1+exp(-\color{purple}{ys}))$$</p>
<p><img src="/imgs/linear-model/lll_error_function_scale.png" alt="" title="lll_error_function_scale.png"></p>
<p>&emsp;&emsp;事实上这里做scale的动作并不会影响最优化的过程，它只是让之后的推导证明更加容易一些。</p>
<p>&emsp;&emsp;现在稍稍回忆一下我们的问题是什么:</p>
<p>&emsp;&emsp;能不能拿Linear Regression或Logistic Regression来替代Linear Classification？</p>
<p>&emsp;&emsp;为什么会想做这样的替代？Linear Classification，在分类这件事上，它做的很好，但在最优化这件事上，由于是NP-hard问题，不大好做，而Linear Regression与Logistic Regression在最优化上比较容易。因此，如果他们在分类能力上的表现能够接近Linear Classification，用他们来替代Linear Classification来处理分类的问题，就是件皆大欢喜的事。这时候就可以想想刚刚为何要把$err_se$ scale 成$err_{0/1}$的upper bound，目的就是为了让这几个模型的观点在某个方向上是一致的，即：</p>
<p>&emsp;&emsp;$\color{red}{err_{sqr}}$/$err_{sce}$低的时候，$\color{blue}{err_{0/1}}$也低</p>
<p>&emsp;&emsp;通俗一点讲：</p>
<p>&emsp;&emsp;假设某种疾病有两种检测方法A和B。A方法检查结果为阳性时，则患病，为阴性时，则未患病。B方法的效率差一些，对于一部分患病的人，B方法不一定结果为阳性，但只要B的结果为阳性，再用A来检查，A的结果一定也为阳性。这么一来，我们就可以说，如果B方法的结果为阳性的时候，我们就没有必要使用A方法再检查一次了，它的效率是和A相同的。</p>
<p>&emsp;&emsp;再通俗一点讲：</p>
<p>&emsp;&emsp;如果使用$\color{red}{err_{sqr}}$/$err_{sce}$来衡量一个模型分类分得好不好的时候，如果他们认为分得好，那么如果使用$\color{blue}{err_{0/1}}$，它也会认为分得好。</p>
<p>&emsp;&emsp;对比下在处理分类问题时，使用PLA，Linear Regression以及Logistic Regression的优缺点。</p>
<p>&emsp;&emsp;<strong>PLA</strong>:  </p>
<ul>
<li>优点：数据是线性可分时，$E_{in}^{0/1}$保证可以降到最低</li>
<li>缺点：数据不是线性可分时，要额外使用pocket技巧，较难做最优化</li>
</ul>
<p>&emsp;&emsp;<strong>Linear Regression</strong>:  </p>
<ul>
<li>优点：在这三个模型中最容易做最优化</li>
<li>缺点：在$\color{blue}{y}\color{purple}{s}$很大或很小时，这个bound是很宽松的，意思就是没有办法保证$E_{in}^{0/1}$能够很小</li>
</ul>
<p>&emsp;&emsp;<strong>Logistic Regression</strong>:  </p>
<ul>
<li>优点：较容易最优化</li>
<li>缺点：当$\color{blue}{y}\color{purple}{s}$是很小的负数时，bound很宽松</li>
</ul>
<p>&emsp;&emsp;所以我们常常可以使用Linear Regresion跑出的$w$作为(PLA/Pocket/Logistic Regression)的$w_0$，然后再使用$w_0$来跑其他模型，这样可以加快其他模型的最优化速度。同时，由于拿到的数据常常是线性不可分的，我们常常会去使用Logistic Regression而不是PLA+pocket。</p>
<h2 id="Stochastic_Gradient_Descent">Stochastic Gradient Descent</h2>
<p>&emsp;&emsp;我们知道PLA与Logistic Regression都是通过迭代的方式来实现最优化的，即：</p>
<p>&emsp;&emsp;For t = 0, 1, …<br>$$w_{t+1}\leftarrow w_t + \eta v$$<br>&emsp;&emsp;when stop, return last w as g</p>
<p>&emsp;&emsp;区别在于，PLA每次迭代只需要针对一个点进行错误修正，而Logistic Regression每一次迭代都需要计算每一个点对于梯度的贡献，再把他们平均起来:</p>
<p><img src="/imgs/linear-model/pla_logistic_opt.png" alt="" title="pla_logistic_opt.png"></p>
<p>&emsp;&emsp;这样一来，数据量大的时候，由于需要计算每一个点，Logistic Regerssion就会很慢了。<a href="http://beader.me/2014/05/03/logistic-regression/" target="_blank" rel="external">上一篇</a>有讲到Logistic Regression每次是怎样迭代的：</p>
<script type="math/tex; mode=display">
w_{t+1} \leftarrow w_t + \eta \underbrace{\color{red}{\frac{1}{N}\sum_{n=1}^{N}}\color{purple}{\theta(\color{black}{-y_nw_t^Tx_n})}\color{orange}{(y_nx_n)}}_{-\color{blue}{\triangledown E_{in}(w_t)}}
</script>

<p>&emsp;&emsp;那么我可以不可以每次只看一个点，即不要公式中先求和再取平均数的那个部分呢？随机取一个点n，它对梯度的贡献为:<br>$$\color{orange}{\triangledown _w err(w,x_n,y_n)}$$</p>
<p>&emsp;&emsp;我们把它称为随机梯度，stochastic gradient。而真实的梯度，可以认为是随机抽出一个点的梯度值的期望(红色部分取平均数的动作):</p>
<script type="math/tex; mode=display">
\triangledown_w E_{in}(w) = \color{red}{\underset{random\,n}{\epsilon}}\triangledown_w \color{orange}{err(w,x_n,y_n)}
</script>

<p>&emsp;&emsp;因此我们可以把随机梯度当成是在真实梯度上增加一个均值为0的noise：<br>$$\color{orange}{\text{stochastic gradient}} = \color{blue}{\text{true gradient}} + \color{red}{\text{zero-mean ‘noise’ directions}}$$</p>
<p>&emsp;&emsp;虽然和true gradient存在一定的误差，但是可以认为在足够多的迭代次数之后，也能达到差不多好的结果。我们把这种方法成为随机梯度下降，Stochastic Gradient Descent (SGD):</p>
<script type="math/tex; mode=display">
w_{t+1} \leftarrow w_t + \eta \underbrace{\color{purple}{\theta(\color{black}{-y_nw_t^Tx_n})}\color{orange}{(y_nx_n)}}_{-\color{blue}{\triangledown_{err}(w_t,x_n,y_n)}}
</script>

<p>&emsp;&emsp;和之前说到的Gradient Descent相比，SGD的好处在于时间复杂度大幅减小(每次只随机地看一个点)，在数据量很大的时候可以很快得得到结果，当然缺点就是，如果前面说到的那个$\color{red}{\text{noise}}$很大的话，会稍稍有点不稳定。</p>
<h2 id="多类别分类">多类别分类</h2>
<p>&emsp;&emsp;我们现在已经有办法使用线性分类器解决二元分类问题，但有的时候，我们需要对多个类别进行分类，即模型的输出不再是0和1两种，而会是多个不同的类别。那么如何套用二元分类的方法来解决多类别分类的问题呢？</p>
<p>&emsp;&emsp;利用二元分类器来解决多类别分类问题主要有两种策略，OVA(One vs. ALL)和OVO(One vs. One)。</p>
<p>&emsp;&emsp;先来看看OVA，假设原问题有四个类别，那么每次我把其中一个类别当成圈圈，其他所有类别当成叉叉，建立二元分类器，循环下去，最终我们会得到4个分类器。</p>
<p><img src="/imgs/linear-model/ova.png" alt="" title="ova.png"></p>
<p>&emsp;&emsp;做预测的时候，分别使用这四个分类器进行预测，预测为圈圈的那个模型所代表的类别，即为最终的输出。譬如正方形的那个分类器输出圈圈，菱形、三角形、星型这三个分类器都说是叉叉，则我们认为它是正方形。当然这里可能遇到一个问题，就是所有模型都说不是自己的时候(都输出叉叉)，怎么办？<br>&emsp;&emsp;很简单，只要让各个分类器都输出是否为自己类别的概率值，即可，然后选择概率值最高的那个分类器所对应的类别，作为最终的输出。</p>
<p>&emsp;&emsp;在类别较多的时候，如果使用OVA方法，则又会遇到数据不平衡(unbalance)的问题，你拿一个类别作为圈圈，其他所有类别作为叉叉，那么圈圈的比例就会非常小，而叉叉的比例非常高。为了解决这个不平衡的问题，我们可以利用另外一个策略，OVO，即每次只拿两个类别的数据出来建建立分类器，如下图。</p>
<p><img src="/imgs/linear-model/ovo.png" alt="" title="ovo.png"></p>
<p>&emsp;&emsp;这个想法类似在打比赛，一笔新数据进来之后，分别使用这六个模型进行预测，得票数最多的那个类别，作为最终的输出。这样做的好处是，有效率，每次只拿两个类别的数据进行训练，每个模型训练数据量要少很多。但是缺点是，由于模型的数量增加了，将消耗更多的存储空间，会减慢预测的速度。</p>
]]></content>
    <category scheme="http://beader.me/tags/pla/" term="pla"/>
    <category scheme="http://beader.me/tags/线性回归(Linear-Regression)/" term="线性回归(Linear Regression)"/>
    <category scheme="http://beader.me/tags/Logistic-Regression/" term="Logistic Regression"/>
  </entry>
  <entry>
    <title type="html"><![CDATA[超级啰嗦版ODPS MapReduce入门]]></title>
    <link href="http://beader.me/2014/05/05/odps-mapreduce/"/>
    <id>http://beader.me/2014/05/05/odps-mapreduce/</id>
    <published>2014-05-05T14:00:00.000Z</published>
    <updated>2014-09-01T00:25:54.000Z</updated>
    <content type="html"><![CDATA[<h2 id="MapReduce_原理简介">MapReduce 原理简介</h2>
<p><img src="/imgs/odps-mapreduce/word_count.png" alt="" title="word_count.png"></p>
<p><a id="more"></a><br>&emsp;&emsp;(上图引用自 <a href="http://blog.jteam.nl/2009/08/04/introduction-to-hadoop/" target="_blank" rel="external">INTRODUCTION TO HADOOP</a> )</p>
<p>&emsp;&emsp;以MapReduce中最经典的wordcount应用为例，来分析一下MapReduce的全过程。这里我们要统计文件中每个单词出现的次数。</p>
<ul>
<li>Input就是我们要处理的原始数据，一共有3行。</li>
<li>Splitting步骤是分配任务，这里把任务分给3台机器同时处理，每台机器只负责处理一行的数据。</li>
<li>Mapping步骤就是这3台机器具体要做的事情。在这里每台机器要做的就是统计一行文字里的单词频率。这里就涉及到比较重要的一个概念，就是key和value。这里key就是单词，value就是这个单词在这一行出现的次数。</li>
<li>Shuffling步骤就是对Mapping步骤产生的9行数据，按照key进行分组。这里分成了4组，每组交给一台电脑去处理。</li>
<li>Reducing，把相同key对应的value相加，每个key最终只输出一行，依然是key,value的形式输出。</li>
<li>Final result，把Reducing的输出合并。</li>
</ul>
<p>&emsp;&emsp;(注：这里Mapping工作交给3台电脑，Reducing工作交给4台电脑的说法其实是不严谨的，具体要用多少资源来完成MapReduce由系统根据任务的状况决定，通常一台电脑需要完成多个Mapping与Reducing的工作。)</p>
<p>&emsp;&emsp;为何要如此设计？简单来说，因为MapReduce为的是能实现分布式运算，涉及到多台机器同时运算的步骤有Mapping和Reducing，参与Mapping工作的机器可以完全独立工作而不需要知道其他机器上有什么数据；参与Reducing步骤的机器，由于数据之前已经按照key进行了分组，因此其他机器上有什么数据与他毫无关系。参与计算的机器都是互相独立，完全不依赖其他机器的数据，这样就可以很方便写代码，因为所有参与Mapping工作的机器使用一模一样的代码，所有参与Reducing工作的机器也使用一模一样的代码。</p>
<p>&emsp;&emsp;我们要在ODPS上要实现MapReduce，就需要写两类代码，一类称为Mapper，另一类称为Reducer。抛开前面所说的原理，我们只需要记住以下两点：</p>
<ul>
<li>Mapper每次只处理一行数据。即Mapper的Input是数据库中的一条记录。</li>
<li>Reducer每次要处理的是相同key下的所有记录，通常会是多行的。</li>
</ul>
<h2 id="目标">目标</h2>
<p>&emsp;&emsp;通过MapReduce计算对每个user每天对不同品牌产生的4种行为(点击、购买、收藏、购物车)的次数进行统计，并且计算在某天某user对某个brand的累计点击次数：</p>
<p><strong>input table</strong></p>
<table>
<thead>
<tr>
<th style="text-align:center">user_id</th>
<th style="text-align:center">brand_id</th>
<th style="text-align:center">type</th>
<th style="text-align:center">visit_datetime</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">101</td>
<td style="text-align:center">20001</td>
<td style="text-align:center">0</td>
<td style="text-align:center">06-01</td>
</tr>
<tr>
<td style="text-align:center">101</td>
<td style="text-align:center">20001</td>
<td style="text-align:center">0</td>
<td style="text-align:center">06-01</td>
</tr>
<tr>
<td style="text-align:center">101</td>
<td style="text-align:center">20001</td>
<td style="text-align:center">0</td>
<td style="text-align:center">06-02</td>
</tr>
<tr>
<td style="text-align:center">101</td>
<td style="text-align:center">20002</td>
<td style="text-align:center">0</td>
<td style="text-align:center">06-02</td>
</tr>
<tr>
<td style="text-align:center">101</td>
<td style="text-align:center">20002</td>
<td style="text-align:center">1</td>
<td style="text-align:center">06-02</td>
</tr>
<tr>
<td style="text-align:center">101</td>
<td style="text-align:center">20003</td>
<td style="text-align:center">0</td>
<td style="text-align:center">06-02</td>
</tr>
<tr>
<td style="text-align:center">101</td>
<td style="text-align:center">20001</td>
<td style="text-align:center">0</td>
<td style="text-align:center">06-03</td>
</tr>
<tr>
<td style="text-align:center">101</td>
<td style="text-align:center">20003</td>
<td style="text-align:center">2</td>
<td style="text-align:center">06-03</td>
</tr>
<tr>
<td style="text-align:center">101</td>
<td style="text-align:center">20003</td>
<td style="text-align:center">3</td>
<td style="text-align:center">06-03</td>
</tr>
<tr>
<td style="text-align:center">101</td>
<td style="text-align:center">20003</td>
<td style="text-align:center">1</td>
<td style="text-align:center">06-04</td>
</tr>
</tbody>
</table>
<p><strong>output table</strong></p>
<table>
<thead>
<tr>
<th style="text-align:center">user_id</th>
<th style="text-align:center">brand_id</th>
<th style="text-align:center">visit_datetime</th>
<th style="text-align:center">clicks</th>
<th style="text-align:center">buy</th>
<th style="text-align:center">collect</th>
<th style="text-align:center">basket</th>
<th style="text-align:center">cum_clicks</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">101</td>
<td style="text-align:center">20001</td>
<td style="text-align:center">06-01</td>
<td style="text-align:center">2</td>
<td style="text-align:center">0</td>
<td style="text-align:center">0</td>
<td style="text-align:center">0</td>
<td style="text-align:center">2</td>
</tr>
<tr>
<td style="text-align:center">101</td>
<td style="text-align:center">20001</td>
<td style="text-align:center">06-02</td>
<td style="text-align:center">1</td>
<td style="text-align:center">0</td>
<td style="text-align:center">0</td>
<td style="text-align:center">0</td>
<td style="text-align:center">3</td>
</tr>
<tr>
<td style="text-align:center">101</td>
<td style="text-align:center">20001</td>
<td style="text-align:center">06-03</td>
<td style="text-align:center">1</td>
<td style="text-align:center">0</td>
<td style="text-align:center">0</td>
<td style="text-align:center">0</td>
<td style="text-align:center">4</td>
</tr>
<tr>
<td style="text-align:center">101</td>
<td style="text-align:center">20002</td>
<td style="text-align:center">06-02</td>
<td style="text-align:center">1</td>
<td style="text-align:center">1</td>
<td style="text-align:center">0</td>
<td style="text-align:center">0</td>
<td style="text-align:center">1</td>
</tr>
<tr>
<td style="text-align:center">101</td>
<td style="text-align:center">20003</td>
<td style="text-align:center">06-02</td>
<td style="text-align:center">1</td>
<td style="text-align:center">0</td>
<td style="text-align:center">0</td>
<td style="text-align:center">0</td>
<td style="text-align:center">1</td>
</tr>
<tr>
<td style="text-align:center">101</td>
<td style="text-align:center">20003</td>
<td style="text-align:center">06-03</td>
<td style="text-align:center">0</td>
<td style="text-align:center">0</td>
<td style="text-align:center">1</td>
<td style="text-align:center">1</td>
<td style="text-align:center">1</td>
</tr>
<tr>
<td style="text-align:center">101</td>
<td style="text-align:center">20003</td>
<td style="text-align:center">06-04</td>
<td style="text-align:center">0</td>
<td style="text-align:center">1</td>
<td style="text-align:center">0</td>
<td style="text-align:center">0</td>
<td style="text-align:center">1</td>
</tr>
</tbody>
</table>
<p>&emsp;&emsp;比如<strong>output table</strong>里第三行的意思是，<strong>user_101</strong>在<strong>06-03</strong>这天一共点了<strong>brand_20001</strong> 1次，从<strong>user_101</strong>第一次接触<strong>brand_20001</strong>以来，已经累计点了4次。</p>
<p>&emsp;&emsp;由于我们想要实现累计求和，因此我们可以在Mapping步骤中，使用<code>(user_id,brand_id)</code>作为key，而<code>(type,visit_datetime)</code>作为value。</p>
<p>&emsp;&emsp;这么一来，在Reducing步骤中，每个Reducer就能接受到某个user对某个brand的所有交互信息，这样就能衍生出我们所需的新的value，即<code>(visit_datetime, clicks, buy, collect, basket cum_clicks)</code>。</p>
<p>&emsp;&emsp;在ODPS中需要达到上述目标，需要动手实现3个类，这里我把他们命名为TestMapper，TestReducer，TestDriver，其中TestDriver用来进行一些任务的配置。下面来具体看看如何实现。</p>
<h2 id="TestDriver">TestDriver</h2>
<p>&emsp;&emsp;Driver主要用来进行一些格式设定。在此之前你需要在eclipse中新建一个ODPS项目。然后在项目的src上右键-&gt;new-&gt;other，在Aliyun Open Data Processing Service下选择MapReduce Driver…</p>
<p>&emsp;&emsp;接着eclipse会帮我们生成一个Driver的模板。官方写的很清楚了，所有的TODO部分是需要我们进行修改的。先来看第一个TODO:</p>
<figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"><span class="comment">// TODO: specify map output types</span></div><div class="line">job.setMapOutputKeySchema(SchemaUtils.fromString(<span class="string">"user_id:string,brand_id:string"</span>));</div><div class="line">job.setMapOutputValueSchema(SchemaUtils.fromString(<span class="string">"type:string,visit_datetime:string"</span>));</div></pre></td></tr></table></figure>

<p>&emsp;&emsp;这是用来设定Mapper输出的时候，key与value的格式。按照之前说的，以<code>(user_id,brand_id)</code>为key，<code>(type,visit_datetime)</code>为value。</p>
<p>&emsp;&emsp;第二个TODO:</p>
<figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">InputUtils.addTable(TableInfo.builder().tableName(<span class="keyword">args</span>[<span class="number">0</span>]).build(),job);</div><div class="line">OutputUtils.addTable(TableInfo.builder().tableName(<span class="keyword">args</span>[<span class="number">1</span>]).build(),job);</div></pre></td></tr></table></figure>

<p>&emsp;&emsp;设定input table与output table。这里把待会儿命令行调用中，第一个参数(<code>args[0]</code>)设为input table，第二个参数(<code>args[1]</code>)设为output table。待会儿会通过下面的命令在odps console中启动MapReduce任务。(暂时不需要搞清楚的地方都用*代替)</p>
<figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">jar <span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span> TestDriver t_alibaba_bigdata_user_brand_total_1 tb_output</div></pre></td></tr></table></figure>

<p>&emsp;&emsp;其中<code>args[0]</code>就指代<code>t_alibaba_bigdata_user_brand_total_1</code>，<code>args[1]</code>就指代<code>tb_output</code>。</p>
<p>&emsp;&emsp;第三个TODO:</p>
<figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"><span class="comment">// TODO: specify a mapper</span></div><div class="line">job.setMapperClass(TestMapper.<span class="keyword">class</span>)</div><div class="line">job.setReducerClass(TestReducer.<span class="keyword">class</span>)</div></pre></td></tr></table></figure>

<p>&emsp;&emsp;告诉系统这次任务要用的Mapper和Reducer是谁，按照上面的设定之后，系统就会通知所有负责Mapping工作的电脑待会儿使用<code>TestMapper</code>中的代码进行运算，通知所有负责Reducing工作的电脑待会儿使用<code>TestReducer</code>中的代码进行运算。</p>
<p>&emsp;&emsp;TestDriver完整代码如下(省略开头import的部分)</p>
<figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">public</span> <span class="keyword">class</span> TestDriver {</div><div class="line">  <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span> (String[] args) throws OdpsException {</div><div class="line">    JobConf job = <span class="keyword">new</span> JobConf();</div><div class="line"></div><div class="line">	<span class="comment">// TODO: specify map output types</span></div><div class="line">	job.setMapOutputKeySchema(SchemaUtils.fromString(<span class="string">"user_id:string,brand_id:string"</span>));</div><div class="line">	job.setMapOutputValueSchema(SchemaUtils.fromString(<span class="string">"type:string,visit_datetime:string"</span>));</div><div class="line"></div><div class="line">	<span class="comment">// TODO: specify input and output tables</span></div><div class="line">	InputUtils.addTable(TableInfo.builder().tableName(args[<span class="number">0</span>]).build(),job);</div><div class="line">	OutputUtils.addTable(TableInfo.builder().tableName(args[<span class="number">1</span>]).build(),job);</div><div class="line"></div><div class="line">	<span class="comment">// TODO: specify a mapper</span></div><div class="line">	job.setMapperClass(TestMapper.class)</div><div class="line">	job.setReducerClass(TestReducer.class)</div><div class="line"></div><div class="line">	RunningJob rj = JobClient.runJob(job);</div><div class="line">	rj.waitForCompletion();</div><div class="line">  }</div><div class="line">}</div></pre></td></tr></table></figure>

<h2 id="TestMapper">TestMapper</h2>
<p>&emsp;&emsp;之前说过，Mapper的任务就是对读入的一行数据，接着输出key和value。key和value都属于Record类，并且key和value都可以由单个或者多个字段构成，在我们这个任务中，key由<code>(user_id,brand_id)</code>两个字段构成，value由<code>(type,visit_datetime)</code>构成。</p>
<p>&emsp;&emsp;与创建TestDriver的步骤类似，使用官方的模板创建一个名为TestMapper的java代码，同样官方模板把大多数代码都生成好了。</p>
<figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">setup</span>(TaskContext context) throws IOException {</div><div class="line">	key = context.createMapOutputKeyRecord();</div><div class="line">	<span class="keyword">value</span> = context.createMapOutputValueRecord();</div><div class="line">}</div></pre></td></tr></table></figure>

<p><code>setup</code>当中是对key和value进行初始化。其中key使用<code>createMapOutputKeyRecord()</code>进行初始化，value使用<code>createMapOutputValueRecord()</code>进行初始化。</p>
<p>&emsp;&emsp;在map函数中，record代表读入的一行数据，比如<code>101, 20001, 0, 06-01</code>，我们可以通过<code>record.get(n)</code>方法获取该行记录第n列的数据。并且方便的是，这里可以直接对读入的数据进行一个类型转换。例如<code>record.getString()</code>会把读入的数据转为字串，<code>record.getBigInt()</code>则会把读入的数据转为Long型整数。</p>
<p>&emsp;&emsp;在TestDriver的设定当中，我们已经把Mapper输出的key定为<code>(user_id,brand_id)</code>，value定为<code>(type,visit_datetime)</code>，在map函数中，我们可以使用<code>key.set()</code>与<code>value.set()</code>来分别赋予这4个值。</p>
<p>&emsp;&emsp;最后<code>context.write(key, value)</code>的意思是输出这条key-value，如果不写这行，Mapper就什么都不输出。一个Mapper可以有0个或多个key-value的输出，每调用一次<code>context.write(key,value)</code>就会输出一行。</p>
<p>&emsp;&emsp;TestMapper完整代码如下(省略import部分)</p>
<figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">TestMapper</span> <span class="keyword">extends</span> <span class="title">MapperBase</span></span>{</div><div class="line">  Record key;</div><div class="line">  Record value;</div><div class="line"></div><div class="line">  <span class="annotation">@Override</span></div><div class="line">  <span class="keyword">public</span> <span class="keyword">void</span> <span class="title">setup</span>(TaskContext context) <span class="keyword">throws</span> IOException {</div><div class="line">	key = context.createMapOutputKeyRecord();</div><div class="line">	value = context.createMapOutputValueRecord();</div><div class="line">  }</div><div class="line"></div><div class="line">  <span class="annotation">@Override</span></div><div class="line">  <span class="keyword">public</span> <span class="keyword">void</span> <span class="title">map</span>(<span class="keyword">long</span> recordNum, Record record, TaskContext context)</div><div class="line">	<span class="keyword">throws</span> IOException {</div><div class="line"></div><div class="line">	key.set(<span class="string">"user_id"</span>, record.getString(<span class="number">0</span>));</div><div class="line">	key.set(<span class="string">"brand_id"</span>, record.getString(<span class="number">1</span>));</div><div class="line"></div><div class="line">	value.set(<span class="string">"type"</span>, record.getString(<span class="number">2</span>));</div><div class="line">	value.set(<span class="string">"visit_datetime"</span>, record.getString(<span class="number">3</span>));</div><div class="line"></div><div class="line">	context.write(key, value);</div><div class="line">  }</div><div class="line">}</div></pre></td></tr></table></figure>

<h2 id="TestReducer">TestReducer</h2>
<p>&emsp;&emsp;通常Driver和Mapper方面都很简单，大多情况下，计算工作都在Reducing步骤完成，因此Reducer的代码会略多一些。同样按照前面的方法生成名为TestReducer的Reducer类。</p>
<p>&emsp;&emsp;因为我们要按天来汇总四类行为出现的次数，因此这里我使用一个<code>TreeMap</code>来存储每天四种行为出现的次数。</p>
<figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">Map&lt;<span class="built_in">String</span>, <span class="built_in">Long</span>[]&gt; typeCounter = <span class="keyword">new</span> TreeMap&lt;<span class="built_in">String</span>, <span class="built_in">Long</span>[]&gt;();</div></pre></td></tr></table></figure>

<p>&emsp;&emsp;这里又再啰嗦一遍，Mapper每次只处理一行数据，而Reducer通常处理的不止一行，而是会处理属于相同key的所有数据。翻到文章开头的那张图片，图中第二个Reducer，所有key为Car的记录，全部交给该一个Reducer处理。</p>
<p>&emsp;&emsp;因此reduce函数当中的<code>values</code>参数是一个<code>Iterator</code>，通过调用<code>values.next()</code>来读取所有属于该key的记录。每读取一行记录，都对计数器<code>typeCounter</code>进行对应<code>type</code>的累加操作。</p>
<p>&emsp;&emsp;Reducer的output是一个Record类，可以通过<code>output.set(n)</code>来设定该output第n列的数值，同样使用<code>context.write(output)</code>输出一行数据。在本文的例子中，对于一个key(user_id,brand_id)而言，有N个不同的<code>visit_datetime</code>，最终就会输出N行数据。因此可以看到TestReducer的<code>context.write(output)</code>是写在一个for-loop里的，会被调用多次，每次会输出一行。类似的操作如果使用SQL实现，就非常的费神了，而使用MapReduce，反而简单许多。</p>
<p>&emsp;&emsp;TestReducer的完整代码(省略模板中的import部分)</p>
<figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div></pre></td><td class="code"><pre><div class="line">import java.util.Map;</div><div class="line">import java.util.TreeMap;</div><div class="line"></div><div class="line"><span class="keyword">public</span> <span class="keyword">class</span> TestReducer extends ReducerMap{</div><div class="line">  Record output;</div><div class="line"></div><div class="line">  @Override</div><div class="line">  <span class="keyword">public</span> void setup(TaskContext context) throws IOException {</div><div class="line">	output = context.createOutputRecord();</div><div class="line">  }</div><div class="line"></div><div class="line">  @Override</div><div class="line">  <span class="keyword">public</span> void reduce(Record key, Iterator&lt;Record&gt; values, TaskContext context)</div><div class="line">	throws IOException {</div><div class="line"></div><div class="line">	Map&lt;<span class="built_in">String</span>, Long[]&gt; typeCounter = <span class="keyword">new</span> TreeMap&lt;<span class="built_in">String</span>, Long[]&gt;();</div><div class="line"></div><div class="line">	<span class="keyword">while</span> (values.hasNext()) {</div><div class="line">	  Record val = values.<span class="keyword">next</span>();</div><div class="line">	  <span class="built_in">String</span> <span class="built_in">date</span> = val.getString(<span class="string">"visit_datetime"</span>);</div><div class="line">	  <span class="built_in">int</span> type = Integer.parseInt(val.getString(<span class="string">"type"</span>));</div><div class="line"></div><div class="line">	  <span class="keyword">if</span> (typeCounter.containsKey(<span class="built_in">date</span>)) {</div><div class="line">		typeCounter.<span class="keyword">get</span>(<span class="built_in">date</span>)[type]++;</div><div class="line">	  } <span class="keyword">else</span> {</div><div class="line">		Long[] counter = <span class="keyword">new</span> Long[]{<span class="number">0</span>L, <span class="number">0</span>L, <span class="number">0</span>L, <span class="number">0</span>L};</div><div class="line">		counter[type]++;</div><div class="line">		typeCounter.put(<span class="built_in">date</span>, counter);</div><div class="line">	  }</div><div class="line">	}</div><div class="line"></div><div class="line">	output.<span class="keyword">set</span>(<span class="number">0</span>, key.getString(<span class="string">"user_id"</span>));</div><div class="line">	output.<span class="keyword">set</span>(<span class="number">1</span>, key.getString(<span class="string">"brand_id"</span>));</div><div class="line">	Long cumClicks = <span class="number">0</span>L;</div><div class="line">	</div><div class="line">	<span class="keyword">for</span> (<span class="built_in">String</span> <span class="built_in">date</span> : typeCounter.keySet()){</div><div class="line">	  output.<span class="keyword">set</span>(<span class="number">2</span>, <span class="built_in">date</span>);</div><div class="line">	  output.<span class="keyword">set</span>(<span class="number">3</span>, typeCounter.<span class="keyword">get</span>(<span class="built_in">date</span>)[<span class="number">0</span>]);</div><div class="line">	  output.<span class="keyword">set</span>(<span class="number">4</span>, typeCounter.<span class="keyword">get</span>(<span class="built_in">date</span>)[<span class="number">1</span>]);</div><div class="line">	  output.<span class="keyword">set</span>(<span class="number">5</span>, typeCounter.<span class="keyword">get</span>(<span class="built_in">date</span>)[<span class="number">2</span>]);</div><div class="line">	  output.<span class="keyword">set</span>(<span class="number">6</span>, typeCounter.<span class="keyword">get</span>(<span class="built_in">date</span>)[<span class="number">3</span>]);</div><div class="line">	  cumClicks += typeCounter.<span class="keyword">get</span>(<span class="built_in">date</span>)[<span class="number">0</span>];</div><div class="line">	  output.<span class="keyword">set</span>(<span class="number">7</span>, cumClicks);</div><div class="line">	  context.write(output);</div><div class="line">	}</div><div class="line">  }</div><div class="line">}</div></pre></td></tr></table></figure>

<h2 id="打包、上传、建表、运行">打包、上传、建表、运行</h2>
<p>&emsp;&emsp;1. 在Package Explorer中你之前建立的ODPS项目下的src上右键，选择Export，然后选择Java底下的JAR file。接着设定下JAR包存放的位置与文件名。这里假设我们放在<code>C:\TOOLS\test.jar</code>，然后点Finish。</p>
<p>&emsp;&emsp;2. 打开odps console，新建一个resource。</p>
<figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line"><span class="input"><span class="prompt">odps:tianchi_1234&gt;</span> create resource jar <span class="constant">C</span><span class="symbol">:/tools/test</span>.jar -f</span></div></pre></td></tr></table></figure>

<p>&emsp;&emsp;3. 在实际运行之前，需要先建立一个表，作为结果输出的位置。这里我们就叫它<code>tb_output</code>好了。<br>&emsp;&emsp;进入sql，建立表格</p>
<figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"><span class="input"><span class="prompt">odps:tianchi_1234&gt;</span> sql</span></div><div class="line"><span class="input"><span class="prompt">odps:sql:tianchi_1234&gt;</span> drop table if exists tb_output;</span></div><div class="line"><span class="symbol">odps:</span><span class="symbol">sql:</span>tianchi_1234&gt; create table tb_output (user_id string, brand_id string, visit_datetime string, clicks bigint,</div><div class="line">                       buy bigint, collect bigint, basket bigint, cum_clicks bigint);</div></pre></td></tr></table></figure>

<p>&emsp;&emsp;4. 在odps console下，执行MapReduce任务</p>
<figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line"><span class="input"><span class="prompt">odps:tianchi_1234&gt;</span> jar -resources test.jar --classpath <span class="symbol">c:</span>/tools/test.jar <span class="constant">TestDriver</span> t_alibaba_bigdata_user_brand_total_1 tb_output;</span></div></pre></td></tr></table></figure>

<h2 id="结尾">结尾</h2>
<ol>
<li>本人几乎是第一次写Java，当中如有一些不规范的地方，希望您不吝赐教。</li>
<li>因为每次MapReduce的执行时间都会很长，建议每次做MapReduce任务的时候，可以先产生一份非常小的table，先拿这个小table做实验，确定结果正确后，再对整张表进行操作。</li>
<li>odps官方文档的例子中，Mapper，Reducer，Driver是写在同一个文件下的，这样做也可以，但不建议这样写。之前我尝试这么做，当Mapper或者Reducer有错误的时候，无法提示错误在第几行。</li>
</ol>
]]></content>
    <category scheme="http://beader.me/tags/MapReduce/" term="MapReduce"/>
    <category scheme="http://beader.me/tags/ODPS/" term="ODPS"/>
  </entry>
  <entry>
    <title type="html"><![CDATA[机器学习笔记-Logistic Regression]]></title>
    <link href="http://beader.me/2014/05/03/logistic-regression/"/>
    <id>http://beader.me/2014/05/03/logistic-regression/</id>
    <published>2014-05-03T08:00:00.000Z</published>
    <updated>2014-09-01T00:28:26.000Z</updated>
    <content type="html"><![CDATA[<p>笔记整理自台大林轩田老师的开放课程-<a href="https://www.coursera.org/course/ntumlone" target="_blank" rel="external">机器学习基石</a>，笔记中所有图片来自于课堂讲义。</p>
<p>&emsp;&emsp;<a href="http://beader.me/2014/03/09/linear-regression/" target="_blank" rel="external">上一篇</a>比较深入地去理解了线性回归的思想和算法。分类和回归是机器学习中很重要的两大内容。而本篇要讲的Logistic Regression，名字上看是回归，但实际上却又和分类有关。</p>
<p>&emsp;&emsp;之前提过的二元分类器如PLA，其目标函数为， $f(x)=sign(w^Tx)\in{-1,+1}$，输出要么是-1要么是+1，是一个“硬”的分类器。而Logistic Regression是一个“软”的分类器，它的输出是$y=+1$的概率，因此Logistic Regression的目标函数是 $\color{purple}{f}(x)=\color{orange}{P(+1|x)}\in [0,1]$。</p>
<p><a id="more"></a></p>
<h2 id="方程的形式">方程的形式</h2>
<script type="math/tex; mode=display">
h(x)=\frac{1}{1+exp(-w^Tx)}
</script>

<p>&emsp;&emsp;上面的方程背后有什么逻辑呢？</p>
<p>&emsp;&emsp;假设医院知道一个病人的年龄、性别、血压、胆固醇水平，可以为他计算他得某种病的概率。最简单的做法就是对这几个特征进行加权求和：</p>
<script type="math/tex; mode=display">
\color{purple}{s}=\sum_{i=\color{red}{0}}^d\color{orange}{w_i}{x_i}=w^Tx
</script>

<p>&emsp;&emsp;但这里有个问题，就是 $\color{purple}{s}$ 的取值范围是$[-\infty,+\infty]$，而我们希望输出的是对该病人患病概率的一个估计，就需要把输出空间$[-\infty,+\infty]$转换到$[0,1]$上。如何变换？通过sigmoid函数 $\color{blue}{\theta}$ 。</p>
<p>$$\color{blue}{\theta}(\color{purple}{s})=\frac{e^\color{purple}{s}}{1+e^\color{purple}{s}}=\frac{1}{1+e^\color{purple}{-s}}$$</p>
<p><img src="/imgs/logistic-regression/sigmoid_function.png" alt="" title="sigmoid_function.png"></p>
<p>&emsp;&emsp;因此，我们就可以利用经过sigmoid变换后的方程来对患病概率进行一个估计。</p>
<h2 id="误差的衡量_-_Cross_Entropy_Error">误差的衡量 - Cross Entropy Error</h2>
<p>&emsp;&emsp;有了方程的形式，我们就需要一个误差的衡量方式。<a href="http://beader.me/2014/03/09/linear-regression/" target="_blank" rel="external">上一篇</a>我们讲到Linear Regression所使用的是平方误差，那么Logistic 可以使用平方误差吗？当然可以，error是人定的，你爱怎么定就怎么定，但是使用平方误差好不好，不好。为什么呢？</p>
<p>&emsp;&emsp;如果使用平方误差，每个点产生的误差是：</p>
<script type="math/tex; mode=display">
\begin{aligned}
err(h,x_n,y_n) &=
\left\{
\begin{matrix}
(\color{blue}{\theta}(w^Tx)-0)^2 & y_n=0 \\\
(1-\color{blue}{\theta}(w^Tx))^2 & y_n=1
\end{matrix}\right. \\\
&=y_n(1-\color{blue}{\theta}(w^Tx))^2+(1-y_n)\color{blue}{\theta}^2(w^Tx)
\end{aligned}
</script>

<p>&emsp;&emsp;此时cost function，$E_{in}(w)=\sum{err}$就是一个关于$w$的非凸函数(non-convex)：</p>
<p><img src="/imgs/logistic-regression/non_convex.png" alt="" title="non_convex.png"></p>
<p>&emsp;&emsp;非凸函数由于存在很多个局部最小点，因此很难去做最优化(解全局最小)。所以Logistic Regression没有使用平方误差来定义error，而是使用极大似然法来估计模型的参数。那么我们就要先来了解一下这个似然性(likelihood)。<br>&emsp;&emsp;Logistic Regression的目标函数的输出是，在已知$x$的条件下，$y=+1$的概率，因此在已知$x$的条件下，$y=+1$的概率是$f(x)$，$y=-1$的概率是$1-f(x)$:</p>
<script type="math/tex; mode=display">
\color{purple}{f}(x)=\color{orange}{P(}\color{blue}{+1}\color{orange}{|x)} \Leftrightarrow \color{orange}{P(y|x)}=
\left\{\begin{matrix}
\color{purple}{f}\color{blue}{(x)} & \color{blue}{\text{for } y=+1}\\\ 
\color{red}{1-}\color{purple}{f}\color{red}{(x)} & \color{red}{\text{for }y=-1}
\end{matrix}\right.
</script>



<p>&emsp;&emsp;考虑我们的训练样本$\mathcal{D}={(x_1,\color{blue}{+1}),(x_2,\color{red}{-1}),…,(x_N,\color{red}{-1})}$，并不是每次抽样都能抽到一模一样的$\mathcal{D}$，抽到这么一份样本是由于各种的机缘巧合。那么我们能抽到这么一份$\mathcal{D}$的概率取决于两部分：1、抽到样本$x_1,…,x_N$的概率；2、这些样本对应的$y_1,…,y_N$等于$\color{red}{+1}$的概率。</p>
<p><img src="/imgs/logistic-regression/probs_and_likelihood.png" alt="" title="probs_and_likelihood.png"></p>
<p>&emsp;&emsp;对于目标函数 $\color{purple}{f}$，抽到$\mathcal{D}$的概率只取决于第1部分，而我们无法知道 $\color{purple}{f}$，即第2部分也是未知的，因此我们称在 $\color{orange}(h)$的作用下抽出$\mathcal{D}$的概率为“似然性”。如果 $\color{orange}{h}\approx\color{purple}{f}$，则 $likelihood(\color{orange}{h})\approx \text{probability using }\color{purple}{f}$，并且我们认为在 $\color{purple}{f}$的作用下，产生$\mathcal{D}$这样的样本的概率通常是非常的大的。</p>
<p><img src="/imgs/logistic-regression/probs_and_likelihood_2.png" alt="" title="probs_and_likelihood_2.png"></p>
<p>&emsp;&emsp;所以有：</p>
<p>$$\text{if } \color{orange}{h}\approx\color{purple}{f}\text{, then }\; likelihood(\color{orange}{h})\approx(\text{probability using }\color{purple}{f})\approx\color{purple}{\text{large}}$$</p>
<p>&emsp;&emsp;则理想的hypothesis就是能使得似然函数最大的那个$h$：<br>$$g=\underset{\color{orange}{h}}{argmax}\;likelihood(\color{orange}{h})$$</p>
<p>&emsp;&emsp;当$\color{orange}{h}$是logistic函数的时候，即$h(x)=\theta(w^Tx)$，由于logistic函数的中心对称性，有:<br>$$1-h(x)=h(-x)$$</p>
<p>&emsp;&emsp;所以有:</p>
<script type="math/tex; mode=display">
\begin{aligned}
likelihood(\color{orange}{h})&=\color{grey}{P(x_1)}\color{orange}{h}\color{blue}{(x_1)}\color{grey}{\times P(x_2)}\color{red}{(1-\color{orange}{h}(x_2))}\color{grey}{\times ...\times P(x_N)}\color{red}{(1-\color{orange}{h}(x_N))} \\\
&= \color{grey}{P(x_1)}\color{orange}{h}\color{blue}{(x_1)}\color{grey}{\times P(x_2)}\color{red}{(\color{orange}{h}(-x_2))}\color{grey}{\times ...\times P(x_N)}\color{red}{(\color{orange}{h}(-x_N))} \\\
&= \color{grey}{P(x_1)}\color{orange}{h}\color{blue}{(x_1)}\color{grey}{\times P(x_2)}\color{red}{(\color{orange}{h}(y_2x_2))}\color{grey}{\times ...\times P(x_N)}\color{red}{(\color{orange}{h}(y_Nx_N))}
\end{aligned}
</script>

<p>&emsp;&emsp;因此有这么一个相似性:</p>
<p>$$likelihood(logistic\;\color{orange}{h})\propto \prod_{n=1}^{N}\color{orange}{h}(y_nx_n)$$</p>
<p>&emsp;&emsp;我们的目标是想找到一个似然性最大的方程:</p>
<p>$$\underset{\color{orange}{h}}{max}\;\;\color{grey}{likelihood(logistic\;\color{orange}{h}) \propto}\prod_{n=1}^{N}\color{orange}{h}(y_nx_n)$$</p>
<p>&emsp;&emsp;转化成与参数$w$有关的形式:</p>
<script type="math/tex; mode=display">
\begin{aligned}
\underset{\color{orange}{w}}{max}\;\;\color{grey}{likelihood(\color{orange}{w})} &\propto\prod_{n=1}^{N}\theta(y_n\color{orange}{w}^Tx_n) \\\
&\propto ln\prod_{n=1}^{N}\theta(y_n\color{orange}{w}^Tx_n) \\\
&\propto \color{grey}{\frac{1}{N}}\sum_{n=1}^{N}ln\,\theta(y_n\color{orange}{w}^Tx_n)
\end{aligned} 
</script>

<p>&emsp;&emsp;求解上式最大值，等价于求解下式的最小值:</p>
<script type="math/tex; mode=display">
\begin{aligned}
&\;\;\;\;\underset{\color{orange}{w}}{min}\;\color{grey}{\frac{1}{N}}\sum_{n=1}^{N}-ln\,\theta(y_n\color{orange}{w}^Tx_n) \\\
&\Rightarrow \underset{\color{orange}{w}}{min}\;\color{grey}{\frac{1}{N}}\sum_{n=1}^{N}ln(1+exp(-y_n\color{orange}{w}^Tx_n))
\end{aligned}
</script>

<p>&emsp;&emsp;求和符号后面的部分就是在极大似然估计下，logistic方程的误差函数，这种形式的误差函数称为cross entropy error:</p>
<p>$$err(\color{orange}{w},x,y)=ln(1+exp(-y\color{orange}{w}x))\\<br>\color{blue}{\text{cross-entropy error}}$$</p>
<h2 id="Cost_function">Cost function</h2>
<p>&emsp;&emsp;有了误差函数后，我们就可以定出Cost function:</p>
<script type="math/tex; mode=display">
E_{in}(\color{orange}{w})=\frac{1}{N}\sum_{n=1}^{N}ln(1+exp(-y_n\color{orange}{w}^Tx_n))
</script>

<p><img src="/imgs/logistic-regression/costf_logistic.png" alt="" title="costf_logistic.png"></p>
<p>&emsp;&emsp;该函数是连续，可微，并且是凸函数(二次微分矩阵是正定的)。</p>
<h2 id="如何最小化$E_{in}(w)$">如何最小化$E_{in}(w)$</h2>
<p>&emsp;&emsp;那么如何能够最小化$E_{in}(w)$呢？按照之前Linear Regression的逻辑，由于它是凸函数，如果我们能解出一阶微分(梯度)为0的点，这个问题就解决了。</p>
<p>&emsp;&emsp;先来看看$E_{in}(w)$在$w_i$方向上的偏微分：</p>
<p><img src="/imgs/logistic-regression/deriv_costf_logistic.png" alt="" title="deriv_costf_logistic.png"></p>
<p>&emsp;&emsp;再把偏微分方程中的$x_{n,i}$换成向量的形式，就得到$E_{in}(w)$的一阶微分:</p>
<script type="math/tex; mode=display">
\triangledown E_{in}(w)=\frac{1}{N}\sum_{n=1}^{N}\color{purple}{\theta(\color{black}{-y_nw^Tx_n})}\color{orange}{(-y_nx_n)}
</script>

<p>&emsp;&emsp;和之前的Linear Regression不同，它不是一个线性的式子，要求解$\triangledown E_{in}(w)=0$这个式子，是困难的。那么该使用何种方法实现$E_{in}(w)$最小化呢？</p>
<p>&emsp;&emsp;这里可以使用类似<a href="http://beader.me/2013/12/21/perceptron-learning-algorithm/" target="_blank" rel="external">PLA</a>当中的，通过迭代的方式来求解，这种方法又称为梯度下降法(Gradient Descent)。</p>
<p>&emsp;&emsp;For t = 0, 1, …<br>$$w_{t+1} \leftarrow w_t + \color{red}{\eta}\color{blue}{v}$$<br>&emsp;&emsp;when stop, return $\color{purple}{\text{last w as g}}$</p>
<p>&emsp;&emsp;其中$\color{red}{\eta}$为每步更新的大小(step size)，$\color{blue}{v}$是单位向量，表示每次更新的方向。</p>
<p><img src="/imgs/logistic-regression/iterative_opt.png" alt="" title="iterative_opt.png"></p>
<p>&emsp;&emsp;有点类似一个小球，往山谷方向滚，直至山谷。每一步我们只要决定两个东西：1、滚动的方向；2、滚动的步长。</p>
<p>&emsp;&emsp;滚动的方向好决定，即在该点一阶微分后的向量所指的方向：</p>
<script type="math/tex; mode=display">
\color{blue}{v}=-\frac{\triangledown E_{in}(w_t)}{||\triangledown E_{in}(w_t)||}
</script>

<p>&emsp;&emsp;步长 $\color{red}{\eta}$比较难决定，太小了，更新太慢，太大了，容易矫枉过正:</p>
<p><img src="/imgs/logistic-regression/choise_of_eta.png" alt="" title="choise_of_eta.png"></p>
<p>&emsp;&emsp;一个比较好的做法是让 $\color{red}{\eta}$ 与 $\color{blue}{||\triangledown E_{in}(w_t)||}$ 成一定的比例，让新的和$\color{blue}{||\triangledown E_{in}(w_t)||}$成比例的$\color{purple}{\text{紫色的 }\eta}$ 来代替原来$\color{red}{\text{红色的 }\eta}$：</p>
<script type="math/tex; mode=display">
\begin{aligned}
w_{t+1} \leftarrow & w_t - \color{red}{\eta} \color{blue}{\frac{\triangledown E_{in}(w_t)}{||\triangledown E_{in}(w_t)||}} \\\
&\;\;\;\;\;\;\;\;\;\parallel \\\
&w_t - \color{purple}{\eta}\,\color{blue}{\triangledown E_{in}(w_t)}
\end{aligned}
</script>

<p>&emsp;&emsp;我们称这个$\color{purple}{\text{紫色的 }\eta}$ 为 $\color{purple}{\text{fixed learning rate}}$。</p>
<p>&emsp;&emsp;再来完整的梳理下梯度下降法(Gradient Descent):</p>
<p>&emsp;&emsp;initialize $w_0$</p>
<p>&emsp;&emsp;For t = 0, 1, …</p>
<p>&emsp;&emsp;1. compute</p>
<script type="math/tex; mode=display">
\triangledown E_{in}(w)=\frac{1}{N}\sum_{n=1}^{N}\color{purple}{\theta(\color{black}{-y_nw^Tx_n})}\color{orange}{(-y_nx_n)}
</script>

<p>&emsp;&emsp;2. update by</p>
<script type="math/tex; mode=display">
w_t - \color{purple}{\eta}\,\color{blue}{\triangledown E_{in}(w_t)}
</script>

<p>&emsp;&emsp;…until $\color{orange}{E_{in}(w_{t+1})=0}$ or enough iterations</p>
<p>&emsp;&emsp;return $\color{purple}{\text{last }w_{t+1}\text{ as }g}$</p>
]]></content>
    <category scheme="http://beader.me/tags/Machine-Learning/" term="Machine Learning"/>
    <category scheme="http://beader.me/tags/Logistic-Regression/" term="Logistic Regression"/>
  </entry>
  <entry>
    <title type="html"><![CDATA[阿里大数据 - 中国好大学]]></title>
    <link href="http://beader.me/2014/03/23/good-universities/"/>
    <id>http://beader.me/2014/03/23/good-universities/</id>
    <published>2014-03-23T09:00:00.000Z</published>
    <updated>2014-09-01T00:44:28.000Z</updated>
    <content type="html"><![CDATA[<p>&emsp;&emsp;最近阿里巴巴办了个数据挖掘竞赛-<a href="http://102.alibaba.com/competition/addDiscovery/index.htm" target="_blank" rel="external">阿里巴巴大数据竞赛</a>，题目是根据天猫用户4个月的行为记录来预测用户下一个月会买什么东西，参赛对象为高校在校学生。由于奖金数额十分巨大，因此比赛规模可以说是空前绝后的，短短2周就有4000多支队伍报名。比赛过程中，每队每周可以提交一次结果，组委会每周日统一计算各队的分数并公布排行榜(top 500)。</p>
<p>&emsp;&emsp;噢，忘了说了这篇文章是关于R语言抓数据以及画图的，与比赛木有关系。本篇的内容纯粹just for fun，不具任何实际价值。是对我最近在cos.name上混来的一些R语言技巧的复习。<br><a id="more"></a></p>
<p>&emsp;&emsp;好了继续。当然我也参加了比赛，但是很不幸第二周就被挤了出来，于是乎对着这个top 500看了好久，想了好久，睡了好久……</p>
<p>&emsp;&emsp;由于参赛队员都是在校学生，而且又是规模空前，几乎全国稍微有点名气的学校都参加了，那么这些学校的表现如何呢？一所大学的综合排名能否很好的表现在比赛中呢？这次的文章就在尝试能否用数据来帮助理解前面这些问题。</p>
<p>&emsp;&emsp;首先我们需要获取这份榜单，地址是:<br>&emsp;&emsp;<a href="http://102.alibaba.com/competition/addDiscovery/totalRank.htm" target="_blank" rel="external">http://102.alibaba.com/competition/addDiscovery/totalRank.htm</a>。<br>&emsp;&emsp;打开发现每页只能显示20支队伍，一共25页。一开始的想法是利用R的XML包，读取HTML TABLE，但是点击第2页的时候，发现网址居然是不会变化的，因此就没有办法根据网址一页一页下载下来。怀疑应该是在你点击下一页的时候，通过一个API获取下一页的信息，再利用javascript来更新表格。</p>
<p>&emsp;&emsp;此时派chrome上场，在页面上点右键-审查元素，打开develop tools:</p>
<p><img src="/imgs/good-universities/chrome_dev_tools.png" alt="" title="chrome_dev_tools.png"></p>
<p>&emsp;&emsp;选择Network，然后在排行榜的页面庄重的点”下一页”。然后developer tools就会抓到刚刚点了“下一页”之后发生的http请求。</p>
<p><img src="/imgs/good-universities/chrome_dev_tool_network.png" alt="" title="chrome_dev_tool_network.png"></p>
<p>&emsp;&emsp;<a href="http://102.alibaba.com/competition/addDiscovery/queryTotalRank.json" target="_blank" rel="external">http://102.alibaba.com/competition/addDiscovery/queryTotalRank.json</a> 就是网页获取排名数据的API了，使用POST方法，带上pageIndex以及pageSize两个参数，就可以获取排名信息。接下来就要请RCurl上场了。</p>
<figure class="highlight r"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">library</span>(RCurl)</div><div class="line"><span class="keyword">library</span>(rjson)</div><div class="line"><span class="keyword">library</span>(plyr)</div></pre></td></tr></table></figure>



<figure class="highlight r"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line">api &lt;- <span class="string">"http://102.alibaba.com/competition/addDiscovery/queryTotalRank.json"</span></div><div class="line"><span class="comment"># 发现其实可以抓到所有队伍的排名，不止是top 500，把pageSize设很大就行了</span></div><div class="line">result &lt;- postForm(api, pageIndex = <span class="number">1</span>, pageSize = <span class="number">5000</span>)</div><div class="line">resjson &lt;- fromJSON(result)</div><div class="line">ranks &lt;- (resjson$returnValue$datas)</div><div class="line">leaderBoard &lt;- ldply(ranks, unlist)</div><div class="line">leaderBoard &lt;- leaderBoard[, c(<span class="string">"rank"</span>, <span class="string">"teamName"</span>, <span class="string">"university"</span>, <span class="string">"score"</span>, <span class="string">"dateString"</span>)]</div><div class="line">leaderBoard$score &lt;- as.numeric(leaderBoard$score)</div><div class="line">leaderBoard$rank &lt;- as.integer(leaderBoard$rank)</div><div class="line">head(leaderBoard)</div></pre></td></tr></table></figure>

<table>
<thead>
<tr>
<th style="text-align:left"></th>
<th style="text-align:left">teamName</th>
<th style="text-align:left">university</th>
<th style="text-align:left">score</th>
<th style="text-align:left">dateString</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">1</td>
<td style="text-align:left">cucyyyy</td>
<td style="text-align:left">中国传媒大学</td>
<td style="text-align:left">0.0717</td>
<td style="text-align:left">2014-03-23</td>
</tr>
<tr>
<td style="text-align:left">2</td>
<td style="text-align:left">花莲</td>
<td style="text-align:left">南京大学</td>
<td style="text-align:left">0.0694</td>
<td style="text-align:left">2014-03-23</td>
</tr>
<tr>
<td style="text-align:left">3</td>
<td style="text-align:left">CUCkdd</td>
<td style="text-align:left">中国传媒大学</td>
<td style="text-align:left">0.0692</td>
<td style="text-align:left">2014-03-23</td>
</tr>
<tr>
<td style="text-align:left">4</td>
<td style="text-align:left">WYZ</td>
<td style="text-align:left">清华大学</td>
<td style="text-align:left">0.0685</td>
<td style="text-align:left">2014-03-23</td>
</tr>
<tr>
<td style="text-align:left">5</td>
<td style="text-align:left">車輪戦</td>
<td style="text-align:left">哈尔滨工业大学</td>
<td style="text-align:left">0.0684</td>
<td style="text-align:left">2014-03-16</td>
</tr>
</tbody>
</table>
<p>&emsp;&emsp;先来看看各个学校都有多少队参加比赛。</p>
<figure class="highlight r"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">library</span>(ggplot2)</div><div class="line"><span class="comment"># 因为这次想用刚学的ggplot2画图，这里设定下ggplot2的全局参数，</span></div><div class="line"><span class="comment"># 把字体设为宋体，以正常显示中文</span></div><div class="line">theme_set(theme_grey(base_family = <span class="string">"Songti SC"</span>, base_size = <span class="number">15</span>))</div></pre></td></tr></table></figure>



<figure class="highlight r"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line">school_summary &lt;- ddply(leaderBoard, .(university), <span class="keyword">function</span>(x) c(num_teams=nrow(x)))</div><div class="line">school_summary &lt;- school_summary[order(school_summary$num_teams,decreasing=<span class="literal">T</span>),]</div><div class="line">school_summary$university &lt;- factor(school_summary$university,levels=school_summary$university)</div><div class="line">ggplot(school_summary[<span class="number">1</span>:<span class="number">10</span>,], aes(x=university, y=num_teams)) +</div><div class="line">  geom_bar(stat=<span class="string">"identity"</span>, fill=<span class="string">"steelblue"</span>) + </div><div class="line">  geom_text(aes(label=num_teams, vjust=-<span class="number">.3</span>)) +</div><div class="line">  labs(title=<span class="string">"参赛队伍数最多的10所大学\n"</span>,x=<span class="string">""</span>,y=<span class="string">"参赛队伍数"</span>) +</div><div class="line">  theme(axis.text=element_text(size=<span class="number">14</span>,angle=<span class="number">20</span>))</div></pre></td></tr></table></figure>

<p><img src="/imgs/good-universities/school_summary.png" alt="" title="school_summary.png"></p>
<p>&emsp;&emsp;看来浙大很具主场优势啊，中科院也是来势汹汹。这里参赛队伍数是报名并且成功提交过结果的队伍数，截止3.23凌晨，共有1383队提交过结果。</p>
<p>&emsp;&emsp;那么如何衡量学校的表现呢？先来看看哪些学校的牛人最牛。我这里用的方法是计算每个学校最强的5支队伍的平均分数。</p>
<figure class="highlight r"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line">mean_top5 &lt;- ddply(leaderBoard,.(university), <span class="keyword">function</span>(x){</div><div class="line">  scores &lt;- x$score</div><div class="line">  scores &lt;- scores[order(scores,decreasing=<span class="literal">T</span>)]</div><div class="line">  c(average=mean(scores[<span class="number">1</span>:<span class="number">5</span>],na.rm=<span class="literal">T</span>),</div><div class="line">    num_teams=nrow(x))</div><div class="line">})</div><div class="line">mean_top5 &lt;- mean_top5[order(mean_top5$average,decreasing=<span class="literal">T</span>),]</div><div class="line">mean_top5 &lt;- mean_top5[mean_top5$num_team &gt;= <span class="number">5</span>,]</div><div class="line">rownames(mean_top5) &lt;- <span class="number">1</span>:nrow(mean_top5)</div><div class="line">mean_top5</div></pre></td></tr></table></figure>

<p>&emsp;&emsp;为了减少极端值的影响，这里把参赛队伍数少于5的学校给过滤掉了。</p>
<p>&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;根据每个学校前5名平均分排序</p>
<table>
<thead>
<tr>
<th style="text-align:right"></th>
<th style="text-align:left">university</th>
<th style="text-align:right">average</th>
<th style="text-align:right">num_teams </th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:right">1</td>
<td style="text-align:left">哈尔滨工业大学</td>
<td style="text-align:right">0.0662</td>
<td style="text-align:right">45</td>
</tr>
<tr>
<td style="text-align:right">2</td>
<td style="text-align:left">浙江大学</td>
<td style="text-align:right">0.0661</td>
<td style="text-align:right">97</td>
</tr>
<tr>
<td style="text-align:right">3</td>
<td style="text-align:left">清华大学</td>
<td style="text-align:right">0.0651</td>
<td style="text-align:right">37</td>
</tr>
<tr>
<td style="text-align:right">4</td>
<td style="text-align:left">南京大学</td>
<td style="text-align:right">0.0645</td>
<td style="text-align:right">40</td>
</tr>
<tr>
<td style="text-align:right">5</td>
<td style="text-align:left">中国科学院大学</td>
<td style="text-align:right">0.0645</td>
<td style="text-align:right">76</td>
</tr>
<tr>
<td style="text-align:right">6</td>
<td style="text-align:left">中国科学技术大学</td>
<td style="text-align:right">0.0643</td>
<td style="text-align:right">47</td>
</tr>
<tr>
<td style="text-align:right">7</td>
<td style="text-align:left">北京邮电大学</td>
<td style="text-align:right">0.0639</td>
<td style="text-align:right">87</td>
</tr>
<tr>
<td style="text-align:right">8</td>
<td style="text-align:left">北京理工大学</td>
<td style="text-align:right">0.0628</td>
<td style="text-align:right">25</td>
</tr>
<tr>
<td style="text-align:right">9</td>
<td style="text-align:left">华中师范大学</td>
<td style="text-align:right">0.0623</td>
<td style="text-align:right">10</td>
</tr>
<tr>
<td style="text-align:right">10</td>
<td style="text-align:left">北京航空航天大学</td>
<td style="text-align:right">0.0622</td>
<td style="text-align:right">22</td>
</tr>
<tr>
<td style="text-align:right">11</td>
<td style="text-align:left">北京大学</td>
<td style="text-align:right">0.0620</td>
<td style="text-align:right">39</td>
</tr>
<tr>
<td style="text-align:right">12</td>
<td style="text-align:left">电子科技大学</td>
<td style="text-align:right">0.0619</td>
<td style="text-align:right">56</td>
</tr>
<tr>
<td style="text-align:right">13</td>
<td style="text-align:left">西南交通大学</td>
<td style="text-align:right">0.0613</td>
<td style="text-align:right">11</td>
</tr>
<tr>
<td style="text-align:right">14</td>
<td style="text-align:left">大连理工大学</td>
<td style="text-align:right">0.0604</td>
<td style="text-align:right">19</td>
</tr>
<tr>
<td style="text-align:right">15</td>
<td style="text-align:left">西安电子科技大学</td>
<td style="text-align:right">0.0600</td>
<td style="text-align:right">37</td>
</tr>
<tr>
<td style="text-align:right">16</td>
<td style="text-align:left">中山大学</td>
<td style="text-align:right">0.0597</td>
<td style="text-align:right">35</td>
</tr>
<tr>
<td style="text-align:right">17</td>
<td style="text-align:left">华南理工大学</td>
<td style="text-align:right">0.0591</td>
<td style="text-align:right">28</td>
</tr>
<tr>
<td style="text-align:right">18</td>
<td style="text-align:left">山东大学</td>
<td style="text-align:right">0.0590</td>
<td style="text-align:right">14</td>
</tr>
<tr>
<td style="text-align:right">19</td>
<td style="text-align:left">武汉大学</td>
<td style="text-align:right">0.0588</td>
<td style="text-align:right">19</td>
</tr>
<tr>
<td style="text-align:right">20</td>
<td style="text-align:left">香港科技大学</td>
<td style="text-align:right">0.0574</td>
<td style="text-align:right">15</td>
</tr>
<tr>
<td style="text-align:right">21</td>
<td style="text-align:left">东南大学</td>
<td style="text-align:right">0.0571</td>
<td style="text-align:right">27</td>
</tr>
<tr>
<td style="text-align:right">22</td>
<td style="text-align:left">中国人民大学</td>
<td style="text-align:right">0.0570</td>
<td style="text-align:right">9</td>
</tr>
<tr>
<td style="text-align:right">23</td>
<td style="text-align:left">厦门大学</td>
<td style="text-align:right">0.0569</td>
<td style="text-align:right">15</td>
</tr>
<tr>
<td style="text-align:right">24</td>
<td style="text-align:left">华东师范大学</td>
<td style="text-align:right">0.0565</td>
<td style="text-align:right">14</td>
</tr>
<tr>
<td style="text-align:right">25</td>
<td style="text-align:left">上海交通大学</td>
<td style="text-align:right">0.0565</td>
<td style="text-align:right">20</td>
</tr>
<tr>
<td style="text-align:right">26</td>
<td style="text-align:left">华中科技大学</td>
<td style="text-align:right">0.0562</td>
<td style="text-align:right">35</td>
</tr>
<tr>
<td style="text-align:right">27</td>
<td style="text-align:left">东北大学</td>
<td style="text-align:right">0.0559</td>
<td style="text-align:right">17</td>
</tr>
<tr>
<td style="text-align:right">28</td>
<td style="text-align:left">同济大学</td>
<td style="text-align:right">0.0557</td>
<td style="text-align:right">15</td>
</tr>
<tr>
<td style="text-align:right">29</td>
<td style="text-align:left">复旦大学</td>
<td style="text-align:right">0.0530</td>
<td style="text-align:right">25</td>
</tr>
<tr>
<td style="text-align:right">30</td>
<td style="text-align:left">南开大学</td>
<td style="text-align:right">0.0520</td>
<td style="text-align:right">11</td>
</tr>
<tr>
<td style="text-align:right">31</td>
<td style="text-align:left">天津大学</td>
<td style="text-align:right">0.0498</td>
<td style="text-align:right">12</td>
</tr>
<tr>
<td style="text-align:right">32</td>
<td style="text-align:left">西安交通大学</td>
<td style="text-align:right">0.0480</td>
<td style="text-align:right">12</td>
</tr>
<tr>
<td style="text-align:right">33</td>
<td style="text-align:left">东华大学</td>
<td style="text-align:right">0.0467</td>
<td style="text-align:right">11</td>
</tr>
<tr>
<td style="text-align:right">34</td>
<td style="text-align:left">广东工业大学</td>
<td style="text-align:right">0.0460</td>
<td style="text-align:right">8</td>
</tr>
<tr>
<td style="text-align:right">35</td>
<td style="text-align:left">北京交通大学</td>
<td style="text-align:right">0.0459</td>
<td style="text-align:right">7</td>
</tr>
<tr>
<td style="text-align:right">36</td>
<td style="text-align:left">北京师范大学</td>
<td style="text-align:right">0.0459</td>
<td style="text-align:right">6</td>
</tr>
<tr>
<td style="text-align:right">37</td>
<td style="text-align:left">湖南大学</td>
<td style="text-align:right">0.0450</td>
<td style="text-align:right">6</td>
</tr>
<tr>
<td style="text-align:right">38</td>
<td style="text-align:left">南京邮电大学</td>
<td style="text-align:right">0.0440</td>
<td style="text-align:right">8</td>
</tr>
<tr>
<td style="text-align:right">39</td>
<td style="text-align:left">西北农林科技大学</td>
<td style="text-align:right">0.0430</td>
<td style="text-align:right">5</td>
</tr>
<tr>
<td style="text-align:right">40</td>
<td style="text-align:left">西北工业大学</td>
<td style="text-align:right">0.0426</td>
<td style="text-align:right">8</td>
</tr>
<tr>
<td style="text-align:right">41</td>
<td style="text-align:left">南京航空航天大学</td>
<td style="text-align:right">0.0413</td>
<td style="text-align:right">6</td>
</tr>
<tr>
<td style="text-align:right">42</td>
<td style="text-align:left">中国科学院软件研究所</td>
<td style="text-align:right">0.0403</td>
<td style="text-align:right">5</td>
</tr>
<tr>
<td style="text-align:right">43</td>
<td style="text-align:left">燕山大学</td>
<td style="text-align:right">0.0378</td>
<td style="text-align:right">5</td>
</tr>
<tr>
<td style="text-align:right">44</td>
<td style="text-align:left">吉林大学</td>
<td style="text-align:right">0.0366</td>
<td style="text-align:right">6</td>
</tr>
<tr>
<td style="text-align:right">45</td>
<td style="text-align:left">北京工业大学</td>
<td style="text-align:right">0.0358</td>
<td style="text-align:right">5</td>
</tr>
<tr>
<td style="text-align:right">46</td>
<td style="text-align:left">哈尔滨工业大学（威海）</td>
<td style="text-align:right">0.0356</td>
<td style="text-align:right">6</td>
</tr>
<tr>
<td style="text-align:right">47</td>
<td style="text-align:left">中南大学</td>
<td style="text-align:right">0.0315</td>
<td style="text-align:right">6</td>
</tr>
<tr>
<td style="text-align:right">48</td>
<td style="text-align:left">浙江工商大学</td>
<td style="text-align:right">0.0283</td>
<td style="text-align:right">5</td>
</tr>
<tr>
<td style="text-align:right">49</td>
<td style="text-align:left">四川大学</td>
<td style="text-align:right">0.0267</td>
<td style="text-align:right">6</td>
</tr>
<tr>
<td style="text-align:right">50</td>
<td style="text-align:left">武汉理工大学</td>
<td style="text-align:right">0.0209</td>
<td style="text-align:right">6</td>
</tr>
<tr>
<td style="text-align:right">51</td>
<td style="text-align:left">福建师范大学</td>
<td style="text-align:right">0.0041</td>
<td style="text-align:right">5</td>
</tr>
</tbody>
</table>
<p>&emsp;&emsp;上面的学校排名符合您心目中的预期吗？</p>
<p>&emsp;&emsp;那么如何去衡量一个学校所有队伍的综合表现呢？当分数的分布有偏的时候，不太好直接使用全部队伍得分的平均数来对比两个学校的表现，而且各个学校参赛队伍数差异很大。中位数是个不错的衡量指标，但它会忽略这个学校最强和最弱的这两群学生。</p>
<p>&emsp;&emsp;一个比较直觉的想法是，如果这个学校所有队伍的排名整体比较靠前，则该学校整体实力较强。如果这个学校所有队伍的排名整体比较靠后，则该学校整体实力较弱。</p>
<p>&emsp;&emsp;这里可以拿两所学校做个对比。为何会选这两所学校呢？很简单，博主来自于厦大，另外下面那所学校是随机挑的。</p>
<figure class="highlight r"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">tsinghua &lt;- leaderBoard[leaderBoard$university==<span class="string">"清华大学"</span>,]</div><div class="line">xmu &lt;- leaderBoard[leaderBoard$university==<span class="string">"厦门大学"</span>,]</div><div class="line">ggplot(rbind(tsinghua,xmu),aes(rank,fill=university)) + </div><div class="line">  geom_density(color=<span class="string">"transparent"</span>,alpha=<span class="number">0.5</span>) +</div><div class="line">  labs(x=<span class="string">"名次"</span>,y=<span class="string">"密度"</span>,title=<span class="string">"两个学校名次分布对比\n"</span>) +</div><div class="line">  facet_grid(university~.)</div></pre></td></tr></table></figure>

<p><img src="/imgs/good-universities/density_of_ranks.png" alt="" title="density_of_ranks.png"></p>
<p>&emsp;&emsp;两个学校的整体实力很容易就能看出来了。清华大学的队大部分集中在前500名，表现较好。</p>
<p>&emsp;&emsp;另外还想到的就是可能可以利用ROC曲线来衡量一个学校的整体表现。关于ROC的具体介绍，可以看<a href="http://beader.me/2013/12/15/auc-roc/" target="_blank" rel="external">这里</a>。继续以清华为例子，画出它的ROC曲线。</p>
<figure class="highlight r"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">library</span>(ROCR)</div></pre></td></tr></table></figure>



<figure class="highlight r"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line">probs &lt;- seq(<span class="number">1</span>,<span class="number">0</span>,length.out=nrow(leaderBoard))</div><div class="line">labels &lt;- rep(<span class="number">0</span>,nrow(leaderBoard))</div><div class="line">labels[tsinghua$rank] = <span class="number">1</span></div><div class="line">pred &lt;- prediction(probs ,labels)</div><div class="line">perf &lt;- performance(pred,<span class="string">"tpr"</span>,<span class="string">"fpr"</span>)</div><div class="line">perf &lt;- data.frame(FPR = perf@x.values[[<span class="number">1</span>]], TPR = perf@y.values[[<span class="number">1</span>]])</div><div class="line">ggplot(perf, aes(FPR,TPR)) + </div><div class="line">  labs(title=<span class="string">"ROC Curve\n"</span>) +</div><div class="line">  geom_density(stat=<span class="string">"identity"</span>,</div><div class="line">               fill=<span class="string">"steelblue"</span>,</div><div class="line">               color=<span class="string">"transparent"</span>,alpha=<span class="number">0.6</span>)</div></pre></td></tr></table></figure>

<p><img src="/imgs/good-universities/roc_tsinghua.png" alt="" title="roc_tsinghua.png"></p>
<p>&emsp;&emsp;这个ROC曲线下的面积成为AUC (area under curve)，该部分面积越大，代表整体排名越前。可以利用ROCR包很容易的计算出AUC:</p>
<figure class="highlight r"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">performance(pred,<span class="string">"auc"</span>)@y.values[[<span class="number">1</span>]]</div></pre></td></tr></table></figure>



<figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line"><span class="preprocessor">## [1] 0.6233</span></div></pre></td></tr></table></figure>

<p>&emsp;&emsp;AUC的值超过0.5，说明该校整体表现高于平均水准；小于0.5，说明整体表现低于平均水准。</p>
<p>&emsp;&emsp;写一个function来计算所有学校的AUC：</p>
<figure class="highlight r"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div></pre></td><td class="code"><pre><div class="line">auc &lt;- <span class="keyword">function</span>(ranks, n) {</div><div class="line">    probs &lt;- seq(<span class="number">1</span>, <span class="number">0</span>, length.out = n)</div><div class="line">    labels &lt;- rep(<span class="number">0</span>, n)</div><div class="line">    labels[ranks] &lt;- <span class="number">1</span></div><div class="line">    pred &lt;- prediction(probs, labels)</div><div class="line">    perf &lt;- performance(pred, <span class="string">"auc"</span>)</div><div class="line">    perf@y.values[[<span class="number">1</span>]]</div><div class="line">}</div><div class="line">auc_leaderboard &lt;- ddply(leaderBoard, .(university), <span class="keyword">function</span>(x) {</div><div class="line">    c(auc = auc(x$rank, nrow(leaderBoard)), num_teams = nrow(x))</div><div class="line">})</div><div class="line">auc_leaderboard &lt;- auc_leaderboard[order(auc_leaderboard$auc, decreasing = <span class="literal">T</span>), ]</div><div class="line">auc_leaderboard &lt;- auc_leaderboard[auc_leaderboard$num_teams &gt;= <span class="number">8</span>, ]</div><div class="line">rownames(auc_leaderboard) &lt;- <span class="number">1</span>:nrow(auc_leaderboard)</div><div class="line">auc_leaderboard</div></pre></td></tr></table></figure>

<p>&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;根据AUC排序的榜单</p>
<table>
<thead>
<tr>
<th style="text-align:right"></th>
<th style="text-align:left">university</th>
<th style="text-align:right">auc</th>
<th style="text-align:right">num_teams</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:right">1</td>
<td style="text-align:left">西南交通大学</td>
<td style="text-align:right">0.7116</td>
<td style="text-align:right">11</td>
</tr>
<tr>
<td style="text-align:right">2</td>
<td style="text-align:left">华中师范大学</td>
<td style="text-align:right">0.7084</td>
<td style="text-align:right">10</td>
</tr>
<tr>
<td style="text-align:right">3</td>
<td style="text-align:left">中国人民大学</td>
<td style="text-align:right">0.6417</td>
<td style="text-align:right">9</td>
</tr>
<tr>
<td style="text-align:right">4</td>
<td style="text-align:left">清华大学</td>
<td style="text-align:right">0.6233</td>
<td style="text-align:right">37</td>
</tr>
<tr>
<td style="text-align:right">5</td>
<td style="text-align:left">香港科技大学</td>
<td style="text-align:right">0.6228</td>
<td style="text-align:right">15</td>
</tr>
<tr>
<td style="text-align:right">6</td>
<td style="text-align:left">北京航空航天大学</td>
<td style="text-align:right">0.6092</td>
<td style="text-align:right">22</td>
</tr>
<tr>
<td style="text-align:right">7</td>
<td style="text-align:left">东北大学</td>
<td style="text-align:right">0.6062</td>
<td style="text-align:right">17</td>
</tr>
<tr>
<td style="text-align:right">8</td>
<td style="text-align:left">华东师范大学</td>
<td style="text-align:right">0.5975</td>
<td style="text-align:right">14</td>
</tr>
<tr>
<td style="text-align:right">9</td>
<td style="text-align:left">中国科学技术大学</td>
<td style="text-align:right">0.5848</td>
<td style="text-align:right">47</td>
</tr>
<tr>
<td style="text-align:right">10</td>
<td style="text-align:left">北京大学</td>
<td style="text-align:right">0.5785</td>
<td style="text-align:right">39</td>
</tr>
<tr>
<td style="text-align:right">11</td>
<td style="text-align:left">武汉大学</td>
<td style="text-align:right">0.5680</td>
<td style="text-align:right">19</td>
</tr>
<tr>
<td style="text-align:right">12</td>
<td style="text-align:left">北京理工大学</td>
<td style="text-align:right">0.5594</td>
<td style="text-align:right">25</td>
</tr>
<tr>
<td style="text-align:right">13</td>
<td style="text-align:left">北京邮电大学</td>
<td style="text-align:right">0.5489</td>
<td style="text-align:right">87</td>
</tr>
<tr>
<td style="text-align:right">14</td>
<td style="text-align:left">浙江大学</td>
<td style="text-align:right">0.5391</td>
<td style="text-align:right">97</td>
</tr>
<tr>
<td style="text-align:right">15</td>
<td style="text-align:left">西北工业大学</td>
<td style="text-align:right">0.5118</td>
<td style="text-align:right">8</td>
</tr>
<tr>
<td style="text-align:right">16</td>
<td style="text-align:left">哈尔滨工业大学</td>
<td style="text-align:right">0.5106</td>
<td style="text-align:right">45</td>
</tr>
<tr>
<td style="text-align:right">17</td>
<td style="text-align:left">广东工业大学</td>
<td style="text-align:right">0.5073</td>
<td style="text-align:right">8</td>
</tr>
<tr>
<td style="text-align:right">18</td>
<td style="text-align:left">上海交通大学</td>
<td style="text-align:right">0.5047</td>
<td style="text-align:right">20</td>
</tr>
<tr>
<td style="text-align:right">19</td>
<td style="text-align:left">大连理工大学</td>
<td style="text-align:right">0.5006</td>
<td style="text-align:right">19</td>
</tr>
<tr>
<td style="text-align:right">20</td>
<td style="text-align:left">南京大学</td>
<td style="text-align:right">0.4901</td>
<td style="text-align:right">40</td>
</tr>
<tr>
<td style="text-align:right">21</td>
<td style="text-align:left">同济大学</td>
<td style="text-align:right">0.4881</td>
<td style="text-align:right">15</td>
</tr>
<tr>
<td style="text-align:right">22</td>
<td style="text-align:left">厦门大学</td>
<td style="text-align:right">0.4865</td>
<td style="text-align:right">15</td>
</tr>
<tr>
<td style="text-align:right">23</td>
<td style="text-align:left">南开大学</td>
<td style="text-align:right">0.4817</td>
<td style="text-align:right">11</td>
</tr>
<tr>
<td style="text-align:right">24</td>
<td style="text-align:left">电子科技大学</td>
<td style="text-align:right">0.4809</td>
<td style="text-align:right">56</td>
</tr>
<tr>
<td style="text-align:right">25</td>
<td style="text-align:left">东南大学</td>
<td style="text-align:right">0.4769</td>
<td style="text-align:right">27</td>
</tr>
<tr>
<td style="text-align:right">26</td>
<td style="text-align:left">中国科学院大学</td>
<td style="text-align:right">0.4751</td>
<td style="text-align:right">76</td>
</tr>
<tr>
<td style="text-align:right">27</td>
<td style="text-align:left">中山大学</td>
<td style="text-align:right">0.4747</td>
<td style="text-align:right">35</td>
</tr>
<tr>
<td style="text-align:right">28</td>
<td style="text-align:left">南京邮电大学</td>
<td style="text-align:right">0.4724</td>
<td style="text-align:right">8</td>
</tr>
<tr>
<td style="text-align:right">29</td>
<td style="text-align:left">天津大学</td>
<td style="text-align:right">0.4632</td>
<td style="text-align:right">12</td>
</tr>
<tr>
<td style="text-align:right">30</td>
<td style="text-align:left">山东大学</td>
<td style="text-align:right">0.4617</td>
<td style="text-align:right">14</td>
</tr>
<tr>
<td style="text-align:right">31</td>
<td style="text-align:left">东华大学</td>
<td style="text-align:right">0.4585</td>
<td style="text-align:right">11</td>
</tr>
<tr>
<td style="text-align:right">32</td>
<td style="text-align:left">西安交通大学</td>
<td style="text-align:right">0.4277</td>
<td style="text-align:right">12</td>
</tr>
<tr>
<td style="text-align:right">33</td>
<td style="text-align:left">复旦大学</td>
<td style="text-align:right">0.4062</td>
<td style="text-align:right">25</td>
</tr>
<tr>
<td style="text-align:right">34</td>
<td style="text-align:left">西安电子科技大学</td>
<td style="text-align:right">0.4049</td>
<td style="text-align:right">37</td>
</tr>
<tr>
<td style="text-align:right">35</td>
<td style="text-align:left">华南理工大学</td>
<td style="text-align:right">0.4048</td>
<td style="text-align:right">28</td>
</tr>
<tr>
<td style="text-align:right">36</td>
<td style="text-align:left">华中科技大学</td>
<td style="text-align:right">0.3995</td>
<td style="text-align:right">35</td>
</tr>
</tbody>
</table>
<p>&emsp;&emsp;上面的学校排名有更符合您心目中的预期吗？</p>
]]></content>
    <category scheme="http://beader.me/tags/R/" term="R"/>
    <category scheme="http://beader.me/tags/ggplot2/" term="ggplot2"/>
    <category scheme="http://beader.me/tags/plyr/" term="plyr"/>
    <category scheme="http://beader.me/tags/rcurl/" term="rcurl"/>
  </entry>
  <entry>
    <title type="html"><![CDATA[机器学习笔记-Linear Regression]]></title>
    <link href="http://beader.me/2014/03/09/linear-regression/"/>
    <id>http://beader.me/2014/03/09/linear-regression/</id>
    <published>2014-03-09T06:00:00.000Z</published>
    <updated>2014-09-01T00:45:44.000Z</updated>
    <content type="html"><![CDATA[<p>笔记整理自台大林轩田老师的开放课程-<a href="https://www.coursera.org/course/ntumlone" target="_blank" rel="external">机器学习基石</a>，笔记中所有图片来自于课堂讲义。</p>
<p>&emsp;&emsp;向所有坚持用$\LaTeX$手打公式而不是直接使用截图的偏执狂致敬！</p>
<a id="more"></a>

<p>&emsp;&emsp;前面花了很大篇幅在说机器为何能学习，接下来要说的是机器是怎么学习的，进入算法$\mathcal{A}$的部分。<a href="http://beader.me/2014/03/02/noise-and-error/" target="_blank" rel="external">上一篇</a>稍微提到了几个error的衡量方式，接下来的几篇笔记要讲的就是各种error measurement的区别以及针对它们如何设计最优化的算法。通过设计出来的算法，使得机器能够从$\mathcal{H}$(Hypothesis Set)当中挑选可以使得cost function最小的$h$作为$g$输出。</p>
<p>&emsp;&emsp;本篇以众所周知的线性回归为例，从方程的形式、误差的衡量方式、如何最小化$E_{in}$的角度出发，并简单分析了Hat Matrix的性质与几何意义，希望对线性回归这一简单的模型有个更加深刻的理解。</p>
<h1 id="方程的形式：">方程的形式：</h1>
<script type="math/tex; mode=display">
h(x)=\sum_{i=\color{red}{0}}^d w_ix_i= w^Tx \\\
</script>

<p>&emsp;&emsp;长得很像perceptron(都是直线嘛)，perceptron是$h(x)=sign(w^Tx)$。</p>
<h1 id="误差的衡量_—_平方误差(squared_error)：">误差的衡量 — 平方误差(squared error)：</h1>
<script type="math/tex; mode=display">
\begin{matrix}
err(\hat{y}_n,y_n) = (\hat{y}_n-y_n)^2\\\
(\hat{y}_n\text{为预测值，}y_n\text{为真实值})
\end{matrix}
</script>

<h1 id="Cost_function：">Cost function：</h1>
<script type="math/tex; mode=display">
E_{in}(w)=\frac{1}{N}\sum_{n=1}^N(\hat{y}_n - y_n)=\frac{1}{N}\sum_{n=1}^N(w^Tx_n-y_n)^2
</script>

<p>&emsp;&emsp;$h(x)$是一个以$x$为变量的方程，而$E_{in}(w)$变成了一个以$w$为变量的方程。这样一来，我们就把“在$\mathcal{H}$中寻找能使平均误差最小的方程”这个问题，转换为“求解一个函数的最小值”的问题。使得$E_{in}(w)$最小的$w$，就是我们要寻找的那个最优方程的参数。</p>
<h1 id="如何最小化$E_{in}(w)$：">如何最小化$E_{in}(w)$：</h1>
<p>&emsp;&emsp;用矩阵形式表示：</p>
<script type="math/tex; mode=display">
\begin{aligned}
E_{in}(\color{blue}{w}) &= \frac{1}{N}\sum_{n=1}^{N}(\color{blue}{w^T}\color{red}{x_n}-\color{purple}{y_n})^2=\frac{1}{N}\sum_{n=1}^{N}(\color{red}{x_n^T}\color{blue}{w}-\color{purple}{y_n})^2 \\\

&=\frac{1}{N}\begin{Vmatrix}
\color{red}{x_1^T}\color{blue}{w}-\color{purple}{y_1}\\\ 
\color{red}{x_2^T}\color{blue}{w}-\color{purple}{y_2}\\\ 
...\\\ 
\color{red}{x_N^T}\color{blue}{w}-\color{purple}{y_N}
\end{Vmatrix}^2 \\\

&=\frac{1}{N}\begin{Vmatrix}
\color{red}{\begin{bmatrix}
--x_1^T--\\\ 
--x_2^T--\\\ 
...\\\
--x_N^T--
\end{bmatrix}}
\color{blue}{w} - 
\color{purple}{\begin{bmatrix}
y_1\\\ 
y_2\\\ 
...\\\
y_3
\end{bmatrix}}
\end{Vmatrix}^2 \\\

&=\frac{1}{N}||
\underbrace{\color{red}{X}}_{N\times d+1}\;\;\;
\underbrace{\color{blue}{w}}_{d+1\times 1} \; - \;
\underbrace{\color{purple}{y}}_{N\times 1}
||^2

\end{aligned}
</script>

<p>&emsp;&emsp;$\color{red}{X}$与$\color{purple}{y}$来源于$\mathcal{D}$，是固定不变的，因此它是一个以$\color{blue}{w}$为变量的函数。我们需要解使得$E_{in}$最小的$\color{blue}{w}$，即$\underset{\color{blue}{w}}{min}\,E_{in}(\color{blue}{w})=\frac{1}{N}\begin{Vmatrix}\color{red}{X}\color{blue}{w}-\color{purple}{y}\end{Vmatrix}^2$。这个$E_{in}(\color{blue}{w})$是一个连续(continuous)、处处可微(differentiable)的凸函数(convex)：</p>
<p>  <img src="/imgs/linear-regression/wlin.png" alt="" title="wlin.png"> &emsp;&emsp;</p>
<p>&emsp;&emsp;对于这一类函数，只需要解其一阶导数为0时的解即可。</p>
<script type="math/tex; mode=display">\nabla E_{in}(\color{blue}{w})\equiv \begin{bmatrix}
\frac{\partial E_{in}}{\partial \color{blue}{w}_0}(\color{blue}{w})\\\ 
\frac{\partial E_{in}}{\partial \color{blue}{w}_1}(\color{blue}{w})\\\ 
...\\\
\frac{\partial E_{in}}{\partial \color{blue}{w}_d}(\color{blue}{w})
\end{bmatrix}=\begin{bmatrix}
\color{orange}{0}\\\
\color{orange}{0}\\\
...\\\
\color{orange}{0}
\end{bmatrix}</script>

<p>&emsp;&emsp;关于多元函数的求导，就是线性代数的范畴了：</p>
<script type="math/tex; mode=display">
\boxed
{
\begin{matrix}
\text{一元的情况}\\\
\\\
E_{in}(\color{blue}{w})=\frac{1}{N}(\color{red}{a}\color{blue}{w^2}-2\color{brown}{b}\color{blue}{w}+\color{purple}{c})\\\ 
\nabla E_{in}(\color{blue}{w})=\frac{1}{N}(2\color{red}{a}\color{blue}{w}-2\color{brown}{b})
\end{matrix} 
}
\xrightarrow{\text{推广至}}
\boxed{
\begin{matrix}
\text{多元的情况}\\\
\\\
E_{in}(\color{blue}{w})=\frac{1}{N}(\color{blue}{w^T}\color{red}{A}\color{blue}{w}-2\color{blue}{w^T}\color{brown}{b}+\color{purple}{c})\\\ 
\nabla E_{in}(\color{blue}{w})=\frac{1}{N}(2\color{red}{A}\color{blue}{w}-2\color{brown}{b})
\end{matrix}
}
</script>

<p>&emsp;&emsp;所以有：</p>
<script type="math/tex; mode=display">
\begin{aligned}
\nabla E_{in}(\color{blue}{w}) &=\nabla \frac{1}{N}(\color{blue}{w^T}\color{red}{X^TX}\color{blue}{w}-2\color{blue}{w^T}\color{brown}{X^Ty}+\color{purple}{y^Ty}) \\\
&=\frac{2}{N}(\color{red}{X^TX}\color{blue}{w}-\color{brown}{X^Ty})
\end{aligned}

</script>

<p>&emsp;&emsp;令$\nabla E_{in}(\color{blue}{w})=0$，可得最佳解：</p>
<script type="math/tex; mode=display">
\color{blue}{w_{LIN}}=\underbrace{(\color{red}{X^TX})^{-1}\color{red}{X^T}}_{pseudo-inverse\;\color{red}{X^{\dagger}}}\;\;\;\color{purple}{y} = \color{red}{X^{\dagger}} \color{purple}{y}
</script>

<p>&emsp;&emsp;当$\color{red}{X^TX}$可逆的时候用它作为pseudo-inverse矩阵$\color{red}{X^{\dagger}}$，当$\color{red}{X^TX}$不可逆的时候，再用其他方式定义$\color{red}{X^{\dagger}}$，这里就不详述了。</p>
<p>&emsp;&emsp;用以$\color{blue}{w_{LIN}}$为参数的线性方程对原始数据做预测，可以得到拟合值$\hat{y}=\color{red}{X}\color{blue}{w_{LIN}}=\color{red}{XX^{\dagger}}\color{purple}{y}$。这里又称$\color{orange}{H}=\color{red}{XX^{\dagger}}$为Hat Matrix，帽子矩阵，$\color{orange}{H}$为$\color{purple}{y}$带上了帽子，成为$\hat{y}$，很形象吧。</p>
<h1 id="Hat_Matrix_的几何意义">Hat Matrix 的几何意义</h1>
<p><img src="/imgs/linear-regression/geoview_hatmatrix.png" alt="" title="geoview_hatmatrix.png"></p>
<p>&emsp;&emsp;这张图展示的是在N维实数空间$\mathbb{R}^N$中，注意这里是N=数据笔数，$\color{purple}{y}$中包含所有真实值，$\hat{y}$中包含所有预测值，与之前讲的输入空间是d+1维是不一样的噢。$\color{red}{X}$中包含d+1个column：</p>
<ul>
<li>$\hat{y}=\color{red}{X}\color{blue}{w_{LIN}}$是$\color{red}{X}$的一个线性组合，$\color{red}{X}$中每个column对应$\mathbb{R}^N$下的一个向量，共有d+1个这样的向量，因此$\hat{y}$在这d+1个向量所构成的$\color{red}{span}$(平面)上。</li>
<li>事实上我们要做的就是在这个平面上找到一个向量$\hat{y}$使得他与真实值之间的距离$|\color{green}{y-\hat{y}}|$最短。不难发现当$\hat{y}$是$\color{purple}{y}$在这个平面上的投影时，即$\color{green}{y-\hat{y}}\perp \color{red}{span}$时，$|\color{green}{y-\hat{y}}|$最短。</li>
<li>所以之前说过的Hat Matrix $\color{orange}{H}$，为$\color{purple}{y}$戴上帽子，所做的就是投影这个动作，寻找$\color{red}{span}$上$\color{purple}{y}$的投影。</li>
<li>$\color{orange}{H}\color{purple}{y}=\hat{y}$，$(I-\color{orange}{H})\color{purple}{y}=\color{green}{y-\hat{y}}$。($I$为单位矩阵)</li>
</ul>
<p>&emsp;&emsp;下面来探究一下$\color{orange}{H}$的性质，这个很重要噢。</p>
<p>$$\text{Hat Matrix }\color{orange}{H} = \color{red}{X(X^TX)}^{-1}\color{red}{X^T}:$$</p>
<ul>
<li>对称性(symetric)，即$\color{orange}{H}=\color{orange}{H^T}$：</li>
</ul>
<script type="math/tex; mode=display">
\begin{aligned}
\color{orange}{H^T} &= (\color{red}{X(X^TX)}^{-1}\color{red}{X^T})^T \\\
&=\color{red}{X({(X^TX)}^{-1})^TX^T} \\\
&=\color{red}{\color{red}{X(X^TX)}^{-1}\color{red}{X^T}}\\\
&=\color{orange}{H}
\end{aligned}
</script>

<ul>
<li>幂等性(idempotent)，即$\color{orange}{H^2}=\color{orange}{H}$：</li>
</ul>
<script type="math/tex; mode=display">
\begin{aligned}
\color{orange}{H^2} &= (\color{red}{X(X^TX)}^{-1}\color{red}{X^T})(\color{red}{X(X^TX)}^{-1}\color{red}{X^T})\\\
&=\color{red}{X\;}\underbrace{\color{red}{(X^TX)}^{-1}\color{red}{(X^TX)}}_{I}\;\color{red}{(X^TX)}^{-1}\color{red}{X^T} \\\
&=\color{red}{X}\color{red}{(X^TX)}^{-1}\color{red}{X^T}\\\
&=\color{orange}{H}
\end{aligned}
</script>


<ul>
<li>半正定(positive semi-definite)，即所有特征值为非负数：<br>(以下$\lambda$为特征值，$b$为对应的特征向量)<script type="math/tex; mode=display">
\begin{aligned}
\color{orange}{H}b&=\lambda b\\\
\color{orange}{H^2}b&=\lambda \color{orange}{H}b \\\
&=\lambda (\lambda b)\\\
\text{(因为}\color{orange}{H^2}&=\color{orange}{H}\text{)}\\\
\color{orange}{H^2}b&=\color{orange}{H}b=\lambda b\\\
\text{所以}&:\\\
\lambda ^2b&=\lambda b \\\
\text{即}&:\\\
\lambda (\lambda -1)b&=0 \\\
\lambda = 0 &\text{ or } \lambda=1
\end{aligned}
</script>

</li>
</ul>
<p>&emsp;&emsp;林老师在课堂上讲到：</p>
<p>$$trace(I-\color{orange}{H}) = N-(d+1)$$</p>
<p>&emsp;&emsp;$trace$为矩阵的迹。这条性质很重要，但是为什么呢？证明过程有点多，以后有机会再补充，心急的同学可以看这里<a href="http://www.stat.berkeley.edu/~census/general.pdf" target="_blank" rel="external">General formulas for bias and variance in OLS</a>。一个矩阵的$trace$等于该矩阵的所有特征值(Eigenvalues)之和。</p>
<p><img src="/imgs/linear-regression/geoview_hatmatrix_noise.png" alt="" title="geoview_hatmatrix_noise.png"></p>
<p>&emsp;&emsp;假设$\color{purple}{y}$由$\color{red}{f(X)\in span}+noise$构成的。有$\color{purple}{y}=\color{red}{f(X)}+noise$。之前讲到$\color{orange}{H}$作用于某个向量，会得到该向量在$\color{red}{span}$上的投影，而$I-\color{orange}{H}$作用于某个向量，会得到那条与$\color{red}{span}$垂直的向量，在这里就是图中的$\color{green}{y-\hat{y}}$，即$(I-\color{orange}{H})noise=\color{green}{y-\hat{y}}$。</p>
<p>&emsp;&emsp;这个$\color{green}{y-\hat{y}}$是真实值与预测值的差，其长度就是就是所有点的平方误差之和。于是就有：</p>
<script type="math/tex; mode=display">
\begin{aligned}
E_{in}(\color{blue}{w_{LIN}})&=\frac{1}{N}||\color{green}{y-\hat{y}}||^2\\\
&=\frac{1}{N}||(I-\color{orange}{H})noise||^2 \\\
&=\frac{1}{N}trace(I-\color{orange}{H})||noise||^2 \\\
&=\frac{1}{N}(N-(d+1))||noise||^2
\end{aligned}
</script>

<p>&emsp;&emsp;上面的证明不太好整理进来，依然可以参考<a href="http://www.stat.berkeley.edu/~census/general.pdf" target="_blank" rel="external">General formulas for bias and variance in OLS</a>。</p>
<p>&emsp;&emsp;因此，就平均而言，有：</p>
<script type="math/tex; mode=display">
\begin{aligned}
\color{red}{\overline{E_{in}}}&=\text{noise level}\cdot(1-\frac{d+1}{N})\\\
\color{blue}{\overline{E_{out}}}&=\text{noise level}\cdot(1+\frac{d+1}{N}) \;\;\;(后面这个不懂证了。)
\end{aligned}
</script>

<p>&emsp;&emsp;花这么大力气是为了什么，又回到之前learning可行性的话题了。</p>
<p>  <img src="/imgs/linear-regression/linear_regression_learning_curve.png" alt="" title="linear_regression_learning_curve.png"></p>
<p>&emsp;&emsp;$\color{red}{\overline{E_{in}}}$和$\color{blue}{\overline{E_{out}}}$都向$\sigma ^2$(noise level)收敛，并且他们之间的差异被$\frac{2(d+1)}{N}$给bound住了。有那么点像VC bound，不过要比VC bound来的更严格一些。</p>
]]></content>
    <category scheme="http://beader.me/tags/Machine-Learning/" term="Machine Learning"/>
    <category scheme="http://beader.me/tags/线性回归(Linear-Regression)/" term="线性回归(Linear Regression)"/>
    <category scheme="http://beader.me/tags/帽子矩阵(Hat-Matrix)/" term="帽子矩阵(Hat Matrix)"/>
  </entry>
  <entry>
    <title type="html"><![CDATA[机器学习笔记-Noise and Error]]></title>
    <link href="http://beader.me/2014/03/02/noise-and-error/"/>
    <id>http://beader.me/2014/03/02/noise-and-error/</id>
    <published>2014-03-02T06:00:00.000Z</published>
    <updated>2014-09-01T00:47:09.000Z</updated>
    <content type="html"><![CDATA[<p>笔记整理自台大林轩田老师的开放课程-<a href="https://www.coursera.org/course/ntumlone" target="_blank" rel="external">机器学习基石</a>，笔记中所有图片来自于课堂讲义。</p>
<p>&emsp;&emsp;这篇笔记是阐述“为什么机器能够学习”这个话题的最后一篇，林老师用了4周时间在告诉我们什么时候机器可以学习以及机器为什么能够学习，对比<a href="https://www.coursera.org/course/ml" target="_blank" rel="external">Ng</a>的那门课，第4周已经在讲类神经网络了。为什么要花这么大的篇幅来搞清楚这些那么理论的东西呢？原因很简单，Ng那门课教的是剑法招式，而林帮主这门课教的是内功。郭靖当年跟着江南七怪习武10多年，进步缓慢，难有所成，但跟着马钰只学了个呼吸吐纳之术，武功却不自觉突飞猛进，原来难以完成的动作突然可以轻松完成了。想要修得真功夫，欲速则不达，林帮主的良苦用心，不知各位同学能否体会。</p>
<p><a id="more"></a></p>
<h1 id="确定性_v-s_概率性_(deterministic_v-s_probabilistic)">确定性 v.s 概率性 (deterministic v.s probabilistic)</h1>
<p>&emsp;&emsp;让我们来回忆一下机器学习的基础架构：</p>
<p>&emsp;&emsp;<img src="/imgs/noise-and-error/learning_flow_with_noise.png" alt="" title="learning_flow_with_noise.png"></p>
<p>&emsp;&emsp;首先我们认为存在一个未知的真理 $f$，认为 $\mathcal{D}$中的$y$就是 $f$作用于$x$产生的，因此虽然无法直接得到 $f$，但若能找到一个和 $f$表现差不多的函数，也算是能学到东西。但在现实世界中，我们拿到的 $\mathcal{D}$并不是完美的，会有noise的存在，什么是noise呢？我们在用收音机听新闻的时候，往往会听到背景有个“沙沙”的声音，信号不好的时候，这种“沙沙”声会越来越大，这个“沙沙”声就是<a href="http://www.simplynoise.com/" target="_blank" rel="external">noise</a>。我们耳朵听到的是 $\mathcal{D}$(收音机放出来的东西)，我们希望通过 $\mathcal{D}$理解到 $f$(播音员真实要表达的)，这个noise就会干扰这个理解的过程， noise越大，我们就越难分辨播音员到底在说什么。在Learning中，noise主要表现为以下几种形式：</p>
<ul>
<li>noise in y : 本来应该是圈圈的，却被标记为叉叉</li>
<li>noise in y : 输入$\mathcal{X}$完全相同的点，既有被标记圈圈的，也有被标记叉叉的</li>
<li>noise in x : 输入$\mathcal{X}$本身就存在问题，譬如100被写成了100万</li>
</ul>
<p>&emsp;&emsp; $f$ 是一个“确定性”(deterministic)的模型，但$noise$是一个随机发生的东西，他们两个共同作用的结果，就成了一个“概率性”(probabilistic)的东西：</p>
<script type="math/tex; mode=display">
\left.\begin{matrix}
\text{idea mini-target }\;\; f(x)=1\\\
\text{'flipping' noise } level=0.3
\end{matrix}\right\}
\Rightarrow 
\begin{matrix}
P(+1|x) = 0.7\\\
P(-1|x) = 0.3
\end{matrix}
</script>

<p>&emsp;&emsp;对于某个样本 $x$，理想状态下，应该有$y=f(x)=+1$，但由于某种noise的存在，该noise会有30%的概率会转换$f(x)$的结果(把+1变成-1或把-1变成+1)。因此在$\mathcal{D}$中，该样本有70%的概率表现出$y=+1$，30%的概率表现出$y=-1$.</p>
<p>&emsp;&emsp;再举个例子来对比下不考虑noise与考虑noise的情形有什么不同。假设不存在noise的情况下，某个hypothesis $h$在$\mathcal{D}$中的错误率$E_{in}=\mu$。现在考虑加上一个 <em>‘flipping’ noise level</em> $=1-\lambda$的noise，即：</p>
<script type="math/tex; mode=display">
P(y|x)=
\left\{\begin{matrix}
\lambda & y=f(x)\\\
1-\lambda & y\neq f(x)
\end{matrix}\right.
</script>

<p>&emsp;&emsp;考虑到了noise，$h$判断正确的样本当中，实际只有$\lambda$这么多的比例是真的判断正确的，同理$h$判断错误的样本中，实际也只有$\lambda$这么多的比例是真的判断错误。假设$\mathcal{D}$中样本量为N：</p>
<script type="math/tex; mode=display">
\begin{matrix}
 & \text{no noise} & \text{noise}\\\ 
h\text{ on }\mathcal{D} & \left\{\begin{matrix}
(1-\mu)N & right\\ 
\\
\\
\mu N& wrong 
\end{matrix}\right. & \begin{matrix}
\left\{\begin{matrix}
\lambda(1-\mu)N & \text{true right}\\ 
(1-\lambda)(1-\mu)N & \text{false right}
\end{matrix}\right.\\ 
\\
\left\{\begin{matrix}
\lambda \mu N & \text{true wrong}\\ 
(1-\lambda)\mu N & \text{false wrong}
\end{matrix}\right.
\end{matrix}
\end{matrix}
</script>

<p>&emsp;&emsp;因此，如果存在noise，则$h$判断错误的数量实际应该为$\lambda \mu N + (1-\lambda)(1-\mu)N$这么多，$h$真实的$E_{in}=\lambda \mu + (1-\lambda)(1-\mu)$。稍微做个合并，得到：</p>
<p>$$\text{true  }E_{in} = (2\lambda - 1)\mu + (1-\lambda)$$</p>
<p>&emsp;&emsp;假设通过学习我们得到一个$g$，并且有$E_{in}(g)=0.2$(即$\mu = 0.2$)，若学习所用的$\mathcal{D}$中存在 <em>‘flipping’ level=0.1</em> 的noise(即$<br>\lambda = 0.9$)，则真实的$E_{in}(g)=0.26$。若 <em>‘flipping’ level</em> 达到0.2，则真实的$E_{in}(g) = 0.32$。可见noise的存在对learning是有一定影响的，在noise较大的情况下，算出的$E_{in}$与真实的$E_{in}$也会有很大的差别。因此想要学得好，$\mathcal{D}$的质量非常重要。</p>
<h1 id="误差的衡量(Error_Measure)">误差的衡量(Error Measure)</h1>
<p>&emsp;&emsp;在把learning的工作交给机器的时候，必须让机器明白你学习的目标，譬如你想让什么什么最大化，或者什么什么最小化。通常的做法是把每一个预测值与真实值之间的误差(error)看成一种成本，机器要做的，就是在$\mathcal{H}$中，挑选一个能使总成本最低的函数。</p>
<p>&emsp;&emsp;之前提到的二元分类问题，就是对判断错误的点，记误差为1，判断正确的点，记误差为0：</p>
<script type="math/tex; mode=display">
\text{error of h on }x_n
\left\{\begin{matrix}
1 & h(x_n)\neq y_n\\\
0 & h(x_n) = y_n
\end{matrix}\right.
</script>

<p>&emsp;&emsp;不管是把$y=+1$的猜错成$-1$，或是把$y=-1$的猜错成$+1$，其产生的误差都为1。</p>
<p>&emsp;&emsp;在实际应用中，这个误差的定义可以很灵活，例如下面两个指纹验证的例子：</p>
<ol>
<li><p>超市利用指纹识别判断某个人是否是他们的会员，若是会员会给相应的折扣。这种情形下，可能做出两种不同的错误判断，把非会员错认为是会员，把会员错认为是非会员。但对于超市来说，这两种错误的成本应该是不同的。把非会员错认为是会员，无非损失些许的折扣；但若是把会员识别为非会员从而不给折扣，就会导致顾客的不满，从而损失掉了未来的生意。针对这种需求，或许下面这个error的衡量办法会更加合理一些。把+1(会员)错判为-1(非会员)的error为10，把-1错判为+1的error为1。</p>
<p> <img src="/imgs/noise-and-error/cost_supermarket.png" alt="" title="cost_supermarket.png"><br> (f代表真实值，g代表预测值)</p>
</li>
<li><p>中情局的门禁系统，利用指纹判断是内部工作人员，才允许进入。这种情形下，若是把好人当坏人，代价并不高，无非就是请工作人员多按一次指纹的功夫，但如果把坏人当好人，损失可就大了。针对这种需求，下面这个error的衡量办法可能更加合理。</p>
<p> <img src="/imgs/noise-and-error/cost_cia.png" alt="" title="cost_cia.png"></p>
</li>
</ol>
<p>&emsp;&emsp;之前一直说的$E_{in}(h)$，就是$h$作用于$\mathcal{D}$中每一笔数据，所产生的成本之和：</p>
<script type="math/tex; mode=display">
E_{in}(h)=\frac{1}{N}\sum_{n=1}^{N}err(h,x_n,y_n)
</script>

<p>&emsp;&emsp;对于上面中情局的例子，$err(h,x_n,y_n)$的定义如下：</p>
<script type="math/tex; mode=display">
err(h,x_n,y_n)=\left\{\begin{matrix}
1 & h(x_n)\neq y_n,y_n=+1 \\\
1000 & h(x_n)\neq y_n,y_n=-1
\end{matrix}\right.
</script>

<p>&emsp;&emsp;这种误差衡量方式称为”pointwise measure”，即对每个点记录误差，总误差为所有点产生的误差之和。在Ng那门课上，这个$E_{in}(h)$被称为cost function，通过cost function可以计算出当前$h$作用于$\mathcal{D}$所造成的总成本，通过learning找到一个能够使总成本最小的$h$，就完成了学习的过程。</p>
<p>&emsp;&emsp;针对不同的问题与不同的使用环境，我们可以设计不同的误差衡量方法，下面是集中常见的误差的定义：</p>
<ul>
<li><p>0/1 error ，通常用于分类问题：<br>$err(\widetilde{y},y)=[\widetilde{y}\neq y]$</p>
</li>
<li><p>squared error，通常用于回归问题：<br>$err(\widetilde{y},y)=(\widetilde{y}-y)^2$</p>
</li>
<li><p>absolute error：<br>$err(\widetilde{y},y)=abs(\widetilde{y}-y)$</p>
</li>
</ul>
<p>&emsp;&emsp;总结一下，先根据问题的不同选择合适的误差衡量方式，0/1 error还是squared error或者是其他针对某一场景特殊设计的error？把$h$作用于$\mathcal{D}$中所有点的error加总起来就成了一个cost function，也就是$E_{in}(h)$，接着要设计一个最优化算法$\mathcal{A}$，它能够从$\mathcal{H}$中挑选出能够使$E_{in}$最小的方程$g$，learning就完成了。对于不同类型的cost function，通常会使用不同的最优化算法。对于某些cost function，很容易实现$E_{in}$最小，比如之后会说的线性回归。对于某些cost function，寻找最小的$E_{in}$是困难的，回忆之前说的<a href="http://beader.me/2013/12/21/perceptron-learning-algorithm/" target="_blank" rel="external">PLA</a>，用0/1 error来衡量误差，要minimize $E_{in}$就是个NP Hard问题。</p>
<p>&emsp;&emsp;当然除此之外，cost function中还可以增加一些来自于error之外的成本，以达到限制模型复杂度方面的目的，如ridge regression、lasso等，这些以后有机会都会提到。</p>
]]></content>
    <category scheme="http://beader.me/tags/Machine-Learning/" term="Machine Learning"/>
    <category scheme="http://beader.me/tags/Noise/" term="Noise"/>
    <category scheme="http://beader.me/tags/Error-Measure/" term="Error Measure"/>
  </entry>
  <entry>
    <title type="html"><![CDATA[机器学习笔记-VC Dimension, Part III]]></title>
    <link href="http://beader.me/2014/02/22/vc-dimension-three/"/>
    <id>http://beader.me/2014/02/22/vc-dimension-three/</id>
    <published>2014-02-22T04:00:00.000Z</published>
    <updated>2014-09-01T00:48:34.000Z</updated>
    <content type="html"><![CDATA[<p>笔记整理自台大林轩田老师的开放课程-<a href="https://www.coursera.org/course/ntumlone" target="_blank" rel="external">机器学习基石</a>，笔记中所有图片来自于课堂讲义。</p>
<p>&emsp;&emsp;<a href="http://beader.me/2014/02/15/vc-dimension-two/" target="_blank" rel="external">上一篇</a>讲到了VC Dimension以及VC Bound。VC Bound所描述的是在给定数据量N以及给定的Hypothesis Set的条件下，遇到坏事情的概率的上界，即$E_{in}$与$E_{out}$差很远的概率，最多是多少。VC Bound用公式表示就是：</p>
<script type="math/tex; mode=display">
\begin{aligned}
\mathbb{P}[BAD] &= \mathbb{P}[\exists h \in \mathcal{H}\text{ s.t. } |E_{in}(h)-E_{out}(h)|\gt \epsilon] \\\
&\leq 4m_{\mathcal{H}}(2N)exp(-\frac{1}{8}\epsilon^2N)
\end{aligned}
</script>

<p><a id="more"></a><br>&emsp;&emsp;其中$m_{\mathcal{H}}(N)$为Hypothesis Set的成长函数，有：</p>
<script type="math/tex; mode=display">
\begin{aligned}
\\m_{\mathcal{H}}(N)\leq \sum_{i=0}^{d_{vc}}\binom {N}{i}\leq N^{d_{vc}} \\\
\textit{( for }N\geq 2, d_{vc}\geq 2\textit{ )}
\end{aligned}
</script>

<p>&emsp;&emsp;因为寻找所有Hypothesis Set的成长函数是困难的，因此我们再利用$N^{d_{vc}}$来bound住所有VC Dimension为$d_{vc}$的Hypothesis Set的成长函数。所以对于任意一个从$\mathcal{H}$中的$g$来说，有：</p>
<script type="math/tex; mode=display">
\begin{aligned}
&\;\;\;\,\mathbb{P}[|E_{in}(g) - E_{out}(g)\gt \epsilon|] \\\
&\leq \mathbb{P}[BAD]\\\
&= \mathbb{P}[\exists h \in \mathcal{H}\text{ s.t. } |E_{in}(h)-E_{out}(h)|\gt \epsilon] \\\
&\leq 4m_{\mathcal{H}}(2N)exp(-\frac{1}{8}\epsilon^2N) \\\
&\leq 4(2N)^{d_{vc}}exp(-\frac{1}{8}\epsilon^2N) \\\
&\textit{( if }d_{vc}\textit{ is finite )}
\end{aligned}
</script>

<p>&emsp;&emsp;因此说想让机器真正学到东西，并且学得好，有三个条件：</p>
<ol>
<li>$\mathcal{H}$的$d_{vc}$是有限的，这样VC Bound才存在。(good $\mathcal{H}$)</li>
<li>$N$足够大(对于特定的$d_{vc}而言$)，这样才能保证上面不等式的bound不会太大。(good $\mathcal{D}$)</li>
<li>算法$\mathcal{A}$有办法在$\mathcal{H}$中顺利地挑选一个使得$E_{in}$最小的方程$g$。(good $\mathcal{A}$)</li>
</ol>
<p>&emsp;&emsp;为什么要费那么大的力气来讲这个VC Bound和VC Dimension呢？因为对于初学者来说，最常犯的错误就是只考虑到了第3点，而忽略掉了前两点，往往能在training set上得到极好的表现，但是在test set中表现却很烂。关于算法$\mathcal{A}$的部分会在后续的笔记当中整理，目前我们只关心前面两点。</p>
<h2 id="几种Hypothesis_Set的VC_Dimension">几种Hypothesis Set的VC Dimension</h2>
<p>&emsp;&emsp;对于以下几个$\mathcal{H}$，由于之前我们已经知道了他们的成长函数(见<a href="http://beader.me/2014/01/23/vc-dimension-one/" target="_blank" rel="external">机器学习笔记-VC Dimension, Part I</a>)，因此可以根据$m_{\mathcal{H}}(N)\leq N^{d_{vc}}$，直接得到他们的VC Dimension：</p>
<ul>
<li>positive rays: $m_{\mathcal{H}}(N)=N+1$，看N的最高次项的次数，知道$d_{vc}=1$</li>
<li>positive intervals: $m_{\mathcal{H}}(N)=\frac{1}{2}N^2+\frac{1}{2}N+1$，$d_{vc}=2$</li>
<li>convex sets: $m_{\mathcal{H}}(N)=2^N$，$d_{vc}=\infty$</li>
<li>2D Perceptrons: $m_{\mathcal{H}}(N)\leq N^3\;for\;N\geq 2$，所以$d_{vc}=3$</li>
</ul>
<p>&emsp;&emsp;由于convex sets的$d_{vc}=\infty$，不满足上面所说的第1个条件，因此不能用convex sets这个$\mathcal{H}$来学习。</p>
<p>&emsp;&emsp;但这里要回归本意，通过成长函数来求得$d_{vc}$没有太大的意义，引入$d_{vc}$很大的一部分原因是，我们想要得到某个Hypothesis Set的成长函数是困难的，希望用$N^{d_{vc}}$来bound住对应的$m_{\mathcal{H}}(N)$。对于陌生的$\mathcal{H}$，如何求解它的$d_{vc}$呢？</p>
<h2 id="某个$\mathcal{H}$的VC_Dimension_-_从”shatter”的角度">某个$\mathcal{H}$的VC Dimension - 从”shatter”的角度</h2>
<p>&emsp;&emsp;Homework当中的某题，求解简化版决策树的VC Dimension：</p>
<p>Consider the <strong>simplified decision trees</strong> hypothesis set on $\mathbb{R}^d$, which is given by</p>
<script type="math/tex; mode=display">
\begin{aligned}
\mathcal{H} = \{&h_{t,S}\; |\; h_{t,S} = 2[v \in S] - 1, \text{where } v_i=[x_i\gt t_i], \\\
&\textbf{S}\text{ is a collection of vectors in }\{0,1\}^d, \textbf{t}\in \mathbb{R}^d \}
\end{aligned}
</script>

<p>That is, each hypothesis makes a prediction by first using the $d$ thresholds $t_i$ to locate $x$ to be within one of the $2^d$ hyper-rectangular regions, and looking up $S$ to decide whether the region should be +1 or −1. What is the VC-dimension of the <strong>simplified decision trees</strong> hypothesis set?</p>
<p>&emsp;&emsp;如何去理解题意呢？用一个2维的图来帮助理解：</p>
<p><img src="/imgs/vc-dimension-three/2d_sim_decision_tree.png" alt="" title="2d_sim_decision_tree.png"></p>
<p>&emsp;&emsp;首先把二维实数空间$\mathbb{R}^2$中的向量$x$，通过各个维度上的阈值$t_i$，转换到${{0,1}}^2$空间下的一个点$v$，规则为$v_i=[x_i\gt t_i]$。譬如对于$t=[5,10]$，$x=[6,8]$可以转换为新的空间下的$[1,0]$。这样一来，原来的$\mathbb{R}^2$空间就可以被划分为4个区块$S_1$~$S_4$（hyper-rectangular regions）。$\mathcal{H}$中每一个方程$h$代表着一种对这4块区域是”圈圈“还是”叉叉“的决策(decision)，并且这4块区域的决策是互相独立的，$S_1$的决策是”圈圈“还是”叉叉“和$S_2,S_3,S_4$都没有关系。</p>
<p>&emsp;&emsp;由于这4块区域的决策是互相独立的，那么它最多最多能shatter掉多少个点呢？4个，(当这4个点分别属于这4块区域的时候)，即这4块hyper-rectangular regions所代表的类别可以是(o,o,o,o)、(o,o,o,x)、(o,o,x,o)、…、(x,x,x,x),共16种可能，因此它能够shatter掉4个点。</p>
<p>&emsp;&emsp;由上面2维的例子我们可以看出，<strong>simplified decision trees</strong>的VC Dimension，等于hyper-rectangular regions的个数。$d$维空间$\mathbb{R}^d$可以用$d$条直线切分出$2^d$个互相独立的hyper-rectangular regions，即最多最多可以shatter掉$2^d$个点，因此<strong>simplified decision trees</strong>的$d_{vc}=2^d$。</p>
<p>&emsp;&emsp;我们再来回顾一下Positive Intervals： </p>
<p><img src="/imgs/vc-dimension-three/positive_intervals.png" alt="" title="positive_intervals.png"></p>
<p>&emsp;&emsp;也可以按照上面的方法去理解，Positive Intervals有两个thresholds，把直线切分为3块空间。但这3块空间并不是相互独立，中间的部分永远是+1，左右两边永远是-1，所以还要具体看它能够shatter掉多少个点，这里最多最多只能shatter掉2个点，它的$d_{vc}=2$。</p>
<h2 id="某个$\mathcal{H}$的VC_Dimension_-_从”自由度”的角度">某个$\mathcal{H}$的VC Dimension - 从”自由度”的角度</h2>
<p>&emsp;&emsp;对于$d_{vc}$较小的$\mathcal{H}$，可以从它最多能够shatter的点的数量，得到$d_{vc}$，但对于一些较为复杂的模型，寻找能够shatter掉的点的数量，就不太容易了。此时我们可以通过模型的自由度，来近似的得到模型的$d_{vc}$。</p>
<p>&emsp;&emsp;维基百科上有不止一个关于自由度的定义，每种定义站在的角度不同。在这里，我们定义自由度是，模型当中可以自由变动的参数的个数，即我们的机器需要通过学习来决定模型参数的个数。</p>
<p>&emsp;&emsp;譬如：</p>
<ul>
<li>Positive Rays，需要确定1个threshold，这个threshold就是机器需要根据$\mathcal{D}$来确定的一个参数，则Positive Rays中自由的参数个数为1，</li>
<li>Positive Intervals，需要确定左右2个thresholds，则可以由机器自由决定的参数的个数为2，$d_{vc}=2$</li>
<li>d-D Perceptrons，$d$维的感知机，可以由机器通过学习自由决定的参数的个数为$d+1$（别忘了还有个$w_0$），$d_{vc}=d+1$</li>
</ul>
<h2 id="多个$\mathcal{H}$的并集的VC_Dimension">多个$\mathcal{H}$的并集的VC Dimension</h2>
<p>&emsp;&emsp;Homework当中某题，求$K$个Hypothesis Set的并集$d_{vc}(\cup_{k=1}^{K}\mathcal{H}_k)$的VC Dimension的上下界。下界比较好判断，是$max\{d_{vc}(\mathcal{H}_k)\}_{k=1}^K$，即所有的$\mathcal{H}$都包含于$d_{vc}$最大的那个$\mathcal{H}$当中的时候。上界则出现在各个$\mathcal{H}$互相都没有交集的时候，我们不妨先来看看$K=2$的情况：</p>
<p>&emsp;&emsp;求$d_{vc}(\mathcal{H}_1\cup \mathcal{H}_2)$的上界，已知$d_{vc}(\mathcal{H}_1)=d_1$，$d_{vc}(\mathcal{H}_2)=d_2$。</p>
<p>&emsp;&emsp;从成长函数上看，有 $m_{\mathcal{H}_1\cup \mathcal{H}_2}(N) \leq m_{\mathcal{H}_1}(N) + m_ {\mathcal{H}_2}(N)$，把成长函数展开，有</p>
<script type="math/tex; mode=display">
m _ {\mathcal{H}_1\cup \mathcal{H}_2}(N) \leq \sum _ {i=0} ^ {d_1} \binom{N}{i} + \sum _ {i=0} ^ {d_2} \binom{N}{i}
</script>

<p>&emsp;&emsp;用$\binom{N}{i}=\binom{N}{N-i}$替换RHS，有</p>
<script type="math/tex; mode=display">
m _ {\mathcal{H}_1\cup \mathcal{H}_2}(N) \leq \sum _ {i=0} ^ {d_1} \binom{N}{i} + \sum _ {i=0} ^ {d_2} \binom{N}{N-i} \leq \sum _ {i=0} ^ {d_1} \binom{N}{i} + \sum _ {i=N-d_2} ^ {N} \binom{N}{i}
</script>

<p>&emsp;&emsp;我们可以尝试寻找下上面这个成长函数有可能的最大的break point，让$N$不断增大，直到出现$m_{\mathcal{H}_1\cup \mathcal{H}_2}(N)\lt 2^N$的时候，这个$N$就是break point。那么$N$要多大才够呢？</p>
<p>&emsp;&emsp;$N=d_1$够大吗？不够，因为：</p>
<script type="math/tex; mode=display">
\sum _ {i=0} ^ {d_1} \binom{N}{i} + \sum _ {i=N-d_2} ^ {N} \binom{N}{i} = 2^N + \sum _ {i=N-d_2} ^ {N} \binom{N}{i} \gt 2^N
</script>

<p>&emsp;&emsp;$N=d_1+d_2+1$够大吗？还是不够，因为：</p>
<script type="math/tex; mode=display">
\sum _ {i=0} ^ {d_1} \binom{N}{i} + \sum _ {i=N-d_2} ^ {N} \binom{N}{i} = \sum _ {i=0} ^ {d_1} \binom{N}{i} + \sum _ {i=d_1+1} ^ {N} \binom{N}{i} = 2^N
</script>

<p>&emsp;&emsp;$N=d_1+d_2+2$够大吗？够大了，因为：</p>
<script type="math/tex; mode=display">
\sum _ {i=0} ^ {d_1} \binom{N}{i} + \sum _ {i=N-d_2} ^ {N} \binom{N}{i} = \sum _ {i=0} ^ {d_1} \binom{N}{i} + \sum _ {i=d_1+2} ^ {N} \binom{N}{i} = 2^N - \binom{N}{d_1+1} \lt 2^N
</script>

<p>&emsp;&emsp;所以$m_{\mathcal{H}_1\cup \mathcal{H}_2}(N)$的break point最大可以是$d_1+d_2+2$，此时$d_{vc}(\mathcal{H}_1\cup \mathcal{H}_2)=d_1+d_2+1$。</p>
<p>&emsp;&emsp;因此两个$\mathcal{H}$的并集的VC Dimension的上界为$d_{vc}(\mathcal{H}_1)+d_{vc}(\mathcal{H}_2)+1$。利用此方法，就很容易可以推出$K$个$\mathcal{H}$的并集的情况。</p>
<h2 id="简单_v-s_复杂">简单 v.s 复杂</h2>
<p>&emsp;&emsp;<a href="http://beader.me/2014/01/23/vc-dimension-one/" target="_blank" rel="external">机器学习笔记-VC Dimension, Part I</a>一开始就提到，learning的问题应该关注的两个最重要的问题是：1.能不能使$E_{in}$与$E_{out}$很接近，2.能不能使$E_{in}$足够小。</p>
<ul>
<li>对于相同的$\mathcal{D}$而言，$d_{vc}$小的模型，其VC Bound比较小，比较容易保证$E_{in}$与$E_{out}$很接近，但较难做到小的$E_{in}$，试想，对于2D Perceptron，如果规定它一定要过原点($d_{vc}=2$)，则它就比没有规定要过原点($d_{vc}=3$)的直线更难实现小的$E_{in}$，因为可选的方程更少。2维平面的直线，就比双曲线($d_{vc}=6$)，更难实现小的$E_{in}$。</li>
<li>对于相同的$\mathcal{D}$而言，$d_{vc}$大的模型，比较容易实现小的$E_{in}$，但是其VC Bound就会很大，很难保证模型对$\mathcal{D}$之外的世界也能有同样强的预测能力。</li>
</ul>
<p>&emsp;&emsp;令之前得到的VC Bound为$\delta$，坏事情$[|E_{in}(g)-E_{out}(g)|\gt \epsilon]$发生的概率小于$\delta$，则好事情$[|E_{in}(g)-E_{out}(g)|\leq \epsilon]$发生的概率就大于$1-\delta$，这个$1-\delta$在统计学中又被称为置信度，或信心水准。</p>
<script type="math/tex; mode=display">
\begin{aligned}
\text{set}\;\;\;\;\delta &= 4(2N)^{d_{vc}}exp(-\frac{1}{8}\epsilon^2N)\\\
\sqrt{\frac{8}{N}ln(\frac{4(2N)^{d_{vc}}}{\delta})} &= \epsilon
\end{aligned}
</script>

<p>&emsp;&emsp;因此$E_{in}$、$E_{out}$又有下面的关系：</p>
<script type="math/tex; mode=display">E_{in}(g)-\sqrt{\frac{8}{N}ln(\frac{4(2N)^{d_{vc}}}{\delta})} \leq E_{out}(g) \leq E_{in}(g)+\sqrt{\frac{8}{N}ln(\frac{4(2N)^{d_{vc}}}{\delta})}</script>

<p>&emsp;&emsp;令$\Omega (N,\mathcal{H},\delta)=\sqrt{…}$，即上式的根号项为来自模型复杂度的，模型越复杂，$E_{in}$与$E_{out}$离得越远。</p>
<p><img src="/imgs/vc-dimension-three/model_complexity_curve.png" alt="" title="model_complexity_curve.png"></p>
<p>&emsp;&emsp;随着$d_{vc}$的上升，$E_{in}$不断降低，而$\Omega$项不断上升，他们的上升与下降的速度在每个阶段都是不同的，因此我们能够寻找一个二者兼顾的，比较合适的$d_{vc}^{*}$，用来决定应该使用多复杂的模型。</p>
<p>&emsp;&emsp;反过来，如果我们需要使用$d_{vc}=3$这种复杂程度的模型，并且想保证$\epsilon = 0.1$，置信度$1-\delta =90\%$，我们也可以通过VC Bound来求得大致需要的数据量$N$。通过简单的计算可以得到理论上，我们需要$N\approx 10,000d_{vc}$笔数据，但VC Bound事实上是一个极为宽松的bound，因为它对于任何演算法$\mathcal{A}$，任何分布的数据，任何目标函数$f$都成立，所以经验上，常常认为$N\approx 10d_{vc}$就可以有不错的结果。</p>
]]></content>
    <category scheme="http://beader.me/tags/Machine-Learning/" term="Machine Learning"/>
    <category scheme="http://beader.me/tags/VC-Dimension，自由度(degree-of-freedom)/" term="VC Dimension，自由度(degree of freedom)"/>
  </entry>
  <entry>
    <title type="html"><![CDATA[机器学习笔记-VC Dimension, Part II]]></title>
    <link href="http://beader.me/2014/02/15/vc-dimension-two/"/>
    <id>http://beader.me/2014/02/15/vc-dimension-two/</id>
    <published>2014-02-15T04:00:00.000Z</published>
    <updated>2014-09-01T00:49:48.000Z</updated>
    <content type="html"><![CDATA[<p>笔记整理自台大林轩田老师的开放课程-<a href="https://www.coursera.org/course/ntumlone" target="_blank" rel="external">机器学习基石</a>，笔记中所有图片来自于课堂讲义。</p>
<p>&emsp;&emsp;<a href="http://beader.me/2014/01/23/vc-dimension-one/" target="_blank" rel="external">上一篇</a>用成长函数$m_{\mathcal{H}}(N)$来衡量Hypotheses Set $\mathcal{H}$中有效的方程的数量(Effective Number of Hypotheses)，以取代Hoeffding’s Inequality中的大$M$，并用一种间接的方式 —- break point，来寻找$m_{\mathcal{H}}(N)$的上界，从而避免了直接研究$\mathcal{H}$的成长函数的困难。</p>
<p><a id="more"></a></p>
<h2 id="学习所需”维他命”(The_VC_Dimension)">学习所需”维他命”(The VC Dimension)</h2>
<p>$$m_{\mathcal{H}}(N)\leq \sum_{i=0}^{k-1}\binom {N}{i}$$<br>&emsp;&emsp;根据之前得到的式子，我们知道如果一个$\mathcal{H}$存在break point，我们就有办法保证学出来的东西能够“举一反三”(good generalization)。一般来说break point越大的$\mathcal{H}$，其复杂度也更高，我们可以使用vc dimension来描述一个$\mathcal{H}$的复杂程度，这个vc dimension来自Vladimir Vapnik与Alexey Chervonenkis所提出的<a href="http://en.wikipedia.org/wiki/Vapnik%E2%80%93Chervonenkis_theory" target="_blank" rel="external">VC Theory</a>。</p>
<p>&emsp;&emsp;根据定义，一个$\mathcal{H}$的vc dimension(记为$d_{vc}(\mathcal{H})$)，是这个$\mathcal{H}$最多能够shatter掉的点的数量 (the largest value of N for which $m_{\mathcal{H}}(N)=2^N$)，如果不管多少个点$\mathcal{H}$都能够shatter他们，则$d_{vc}(H)=\infty$。不难看出$d_{vc}$与break point k的关系，有$k=d_{vc}+1$，因此我们用这个$d_{vc}$来描述成长函数的上界：<br>$$m_{\mathcal{H}}(N)\leq \sum_{i=0}^{d_{vc}} \binom {N}{i}$$<br>&emsp;&emsp;上式右边(RHS)事实上是最高项为$d_{vc}$的多项式，利用数学归纳法可得：<br>$$m_{\mathcal{H}}(N)\leq \sum_{i=0}^{d_{vc}} \binom {N}{i} \leq N^{d_{vc}}+1$$</p>
<h2 id="更加一般化的Bound_(The_VC_Generalization_Bound)">更加一般化的Bound (The VC Generalization Bound)</h2>
<p>&emsp;&emsp;上一篇的末尾我们设想利用有限的$m_{\mathcal{H}}(N)$来替换无限的大$M$，得到$\mathcal{H}$遇到Bad Sample的概率上界：<br>$$\mathbb{P}_\mathcal{D}[BAD\ D]\leq 2m_{\mathcal{H}}(N)\cdot exp(-2\epsilon ^2N)$$<br>&emsp;&emsp;其中$\mathbb{P}_\mathcal{D}[BAD\ D]$是$\mathcal{H}$中所有有效的方程(Effective Hypotheses)遇到Bad Sample的联合概率，即$\mathcal{H}$中存在一个方程遇上bad sample，则说$\mathcal{H}$遇上bad sample。用更加精准的数学符号来表示上面的不等式：<br>$$\mathbb{P}[\exists h \in \mathcal{H}\text{ s.t. } |E_{in}(h)-E_{out}(h)|\gt \epsilon]\leq 2m_{\mathcal{H}}(N)\cdot exp(-2\epsilon ^2N)$$<br>&emsp;&emsp;注：$\exists h \in \mathcal{H}\text{ s.t. }$ - $\mathcal{H}$中存在($\exists$)满足($\text{ s.t }$)…的$h$</p>
<p>&emsp;&emsp;但事实上上面的不等式是不严谨的，为什么呢？$m_{\mathcal{H}}(N)$描述的是$\mathcal{H}$作用于数据量为$N$的资料$\mathcal{D}$，有效的方程数，因此$\mathcal{H}$当中每一个$h$作用于$\mathcal{D}$都能算出一个$E_{in}$来，一共能有$m_{\mathcal{H}}(N)$个不同的$E_{in}$，是一个有限的数。但在out of sample的世界里(总体)，往往存在无限多个点，平面中任意一条直线，随便转一转动一动，就能产生一个不同的$E_out$来。$E_{in}$的可能取值是有限个的，而$E_{out}$的可能取值是无限的，无法直接套用union bound，我们得先把上面那个无限多种可能的$E_{out}$换掉。那么如何把$E_{out}$变成有限个呢？<br>&emsp;&emsp;假设我们能从总体当中再获得一份$N$笔的验证资料(verification set)$\mathcal{D}’$，对于任何一个$h$我们可以算出它作用于$\mathcal{D}’$上的$E_{in}^{‘}$，由于$\mathcal{D}’$也是总体的一个样本，因此如果$E_{in}$和$E_{out}$离很远，有非常大的可能$E_{in}$和$E_{in}^{‘}$也会离得比较远。 </p>
<p><img src="/imgs/vc-dimension-two/pdf_of_ein.png" alt="" title="pdf_of_ein.png"></p>
<p>&emsp;&emsp;事实上当N很大的时候，$E_{in}$和$E_{in}^{‘}$可以看做服从以$E_{out}$为中心的近似正态分布(Gaussian)，如上图。$[|E_{in}-E_{out}|\text{ is large}]$这个事件取决于$\mathcal{D}$，如果$[|E_{in}-E_{out}|\text{ is large}]$，则如果我们从总体中再抽一份$\mathcal{D}^{‘}$出来，有50%左右的可能性会发生$[|E_{in}-E_{in}^{‘}|\text{ is large}]$，还有大约50%的可能$[|E_{in}-E_{in}^{‘}|\text{ is not large}]$。<br>&emsp;&emsp;因此，我们可以得到$\mathbb{P}[|E_{in}-E_{out}|\text{ is large}]$的一个大概的上界可以是$2\mathbb{P}[|E_{in}-E_{in}^{‘}|\text{ is large}]$，以此为启发去寻找二者之间的关系。</p>
<p>&emsp;&emsp;引理：</p>
<p>$$(1-2e^{-\frac{1}{2}\epsilon^2N})\mathbb{P}[\underset{h\in \mathcal{H}}{sup}\ |E_{in}(h)-E_{out}(h)| \gt \epsilon]\leq \mathbb{P}[\underset{h\in \mathcal{H}}{sup}\ |E_{in}(h)-E_{in}^{‘}(h)| \gt \frac{\epsilon}{2}]$$</p>
<p>&emsp;&emsp;上面的不等式是从何而来的呢？我们先从RHS出发：</p>
<script type="math/tex; mode=display">
\begin{aligned}
&\;\;\;\,\mathbb{P}[\underset{h\in \mathcal{H}}{sup}\ |E_{in}(h)-E_{in}^{'}(h)| \gt \frac{\epsilon}{2}] \\\
&\geq \mathbb{P}[\underset{h\in \mathcal{H}}{sup}\ |E_{in}(h)-E_{in}^{'}(h)| \gt \frac{\epsilon}{2} \mathbf{\;and\;} \underset{h\in \mathcal{H}}{sup}\ |E_{in}(h)-E_{out}(h)| \gt \epsilon] \\\
&=\mathbb{P}[\underset{h\in \mathcal{H}}{sup}\ |E_{in}(h)-E_{out}(h)| \gt \epsilon] \;\times \\\
&\;\;\;\,\mathbb{P}[\underset{h\in \mathcal{H}}{sup}\ |E_{in}(h)-E_{in}^{'}(h)| \gt \frac{\epsilon}{2}\;\;|\;\;\underset{h\in \mathcal{H}}{sup}\ |E_{in}(h)-E_{out}(h)| \gt \epsilon] \\\
&\;\;\;\,\text{(注：sup - 上确界，最小上界。)}
\end{aligned}
</script>

<p>&emsp;&emsp;上式第二行的不等号可以由$\mathbb{P}[\mathcal{B}_1]\geq \mathbb{P}[\mathcal{B}_1 \textbf{ and } \mathcal{B}_2]$得到，第三、四行则是贝叶斯公式，联合概率等于先验概率与条件概率之积。</p>
<p>&emsp;&emsp;下面来看看不等式的最后一项$\mathbb{P}[\underset{h\in \mathcal{H}}{sup}\ |E_{in}(h)-E_{in}^{‘}(h)| \gt \frac{\epsilon}{2}\;\;|\;\;\underset{h\in \mathcal{H}}{sup}\ |E_{in}(h)-E_{out}(h)| \gt \epsilon]$。对于一个固定的data set $\mathcal{D}$来说，我们任选一个$h^{*}$使得$|E_{in}(h^{*})-E_{out}(h^{*})|\gt \epsilon$，注意到这个$h^{*}$只依赖于$\mathcal{D}$而不依赖于$\mathcal{D}^{‘}$噢，对于$\mathcal{D}^{‘}$来说可以认为这个$h^{*}$ is forced to pick out。  </p>
<p>&emsp;&emsp;由于$h^{*}$是对于$\mathcal{D}$来说满足$|E_{in}-E_{out}|\gt \epsilon$的任意一个hypothesis，因此可以把式子中的上确界(sup)先去掉。</p>
<script type="math/tex; mode=display">
\begin{aligned}
&\;\;\;\,\mathbb{P}[\underset{h\in \mathcal{H}}{sup}\ |E_{in}(h)-E_{in}^{'}(h)| \gt \frac{\epsilon}{2}\;\;|\;\;\underset{h\in \mathcal{H}}{sup}\ |E_{in}(h)-E_{out}(h)| \gt \epsilon] \\\
&\geq \mathbb{P}[|E_{in}(h^{*})-E_{in}^{'}(h^{*})| \gt \frac{\epsilon}{2}\;\;|\;\;\underset{h\in \mathcal{H}}{sup}\ |E_{in}(h)-E_{out}(h)| \gt \epsilon]
\end{aligned}
</script>

<p>&emsp;&emsp;这里就要稍微出动一下前人的智慧了：</p>
<script type="math/tex; mode=display">
\left.\begin{matrix}
|E_{in}^{'} - E_{out}|\leq \frac{\epsilon}{2}\\\
|E_{in}-E_{out}| \gt \epsilon
\end{matrix}\right\}
\Rightarrow 
|E_{in}-E_{in}^{'}| \gt \frac{\epsilon}{2}
</script>

<p>&emsp;&emsp;为了直观一点$h^{*}$就不写了。经过各种去掉绝对值符号又加上绝对值符号的运算，可以发现LHS的两个不等式是RHS那个不等式的充分非必要条件。而LHS第二个不等式是已知的，对于$h^{*}$必成立的。因此我们拿LHS这个充分非必要条件去替换RHS这个不等式，继续前面的不等式：</p>
<script type="math/tex; mode=display">
\begin{aligned}
&\;\;\;\,\mathbb{P}[\underset{h\in \mathcal{H}}{sup}\ |E_{in}(h)-E_{in}^{'}(h)| \gt \frac{\epsilon}{2}\;\;|\;\;\underset{h\in \mathcal{H}}{sup}\ |E_{in}(h)-E_{out}(h)| \gt \epsilon] \\\
&\geq \mathbb{P}[|E_{in}(h^{*})-E_{in}^{'}(h^{*})| \gt \frac{\epsilon}{2}\;\;|\;\;\underset{h\in \mathcal{H}}{sup}\ |E_{in}(h)-E_{out}(h)| \gt \epsilon] \\\
&\geq \mathbb{P}[|E_{in}^{'}(h^{*})-E_{out}(h^{*})| \leq \frac{\epsilon}{2}\;\;|\;\;\underset{h\in \mathcal{H}}{sup}\ |E_{in}(h)-E_{out}(h)| \gt \epsilon] \\\
&\geq 1-2e^{-\frac{1}{2}\epsilon^2N}
\end{aligned}
</script>

<p>&emsp;&emsp;最后一个不等号动用了Hoeffding Inequality：</p>
<script type="math/tex; mode=display">
\begin{aligned}
&\;\;\;\,\mathbb{P}[|...|\gt \epsilon]\leq 2Mexp(-2\epsilon^2N) \\\
&\Leftrightarrow 1-\mathbb{P}[|...|\gt \epsilon]\geq 1-2Mexp(-2\epsilon^2N) \\\
&\Leftrightarrow \mathbb{P}[|...|\leq \epsilon]\geq 1-2Mexp(-2\epsilon^2N)
\end{aligned}
</script>

<p>&emsp;&emsp;之前说过对于$\mathcal{D}^{‘}$来说，$h^{*}$ is forced to pick out，因此$M=1$。接着把$\epsilon$替换为$\frac{\epsilon}{2}$，就成了$\mathbb{P}[|…|\lt \frac{\epsilon}{2}]\geq 2exp(-\frac{1}{2}\epsilon^2N)$。则我们可以得到引理中的不等式。</p>
<p>&emsp;&emsp;对于$e^{-\frac{1}{2}e^2N}$，一个比较合理的要求是$e^{-\frac{1}{2}\epsilon^2N}\lt \frac{1}{4}$，譬如我们有400笔资料，想要$E_{in}$和$E_{out}$相差不超过0.1。注意到这只是一个bound，只要要求不太过分，也不能太宽松即可，适当的宽松一点是OK的。当然这里也是想跟之前所说的 “$\mathbb{P}[|E_{in}-E_{out}|\text{ is large}]$的一个大概的上界可以是$2\mathbb{P}[|E_{in}-E_{in}^{‘}|\text{ is large}]$” 当中的2倍有所结合。</p>
<p>&emsp;&emsp;所以就有$1-2e^{-\frac{1}{2}e^2N}\gt \frac{1}{2}$。带回引理，可得：</p>
<p>$$\mathbb{P}[\underset{h\in \mathcal{H}}{sup}\ |E_{in}(h)-E_{out}(h)| \gt \epsilon]\leq 2\,\mathbb{P}[\underset{h\in \mathcal{H}}{sup}\ |E_{in}(h)-E_{in}^{‘}(h)| \gt \frac{\epsilon}{2}]$$</p>
<p>&emsp;&emsp;这样一来我们就把无限多种的$E_{out}$换成了有限多种的$E_{in}$，因为$\mathcal{D}$与$\mathcal{D}^{‘}$的大小相等，都为$N$，因此我们手中一共有$2N$笔数据，这样$\mathcal{H}$作用于$\mathcal{D}+\mathcal{D}^{‘}$最多能产生$m_{\mathcal{H}}(2N)$种dichotomies。此时我们针对上面的不等式，就又可以使用union bound了。(关于union bound，可以参考上一篇<a href="http://beader.me/2014/01/23/vc-dimension-one/" target="_blank" rel="external">VC Dimension, Part I</a>)</p>
<script type="math/tex; mode=display">
\begin{aligned}
\mathbb{P}[BAD] &\leq 2\,\mathbb{P}[\underset{h\in \mathcal{H}}{sup}\ |E_{in}(h)-E_{in}^{'}(h)| \gt \frac{\epsilon}{2}] \\\
&\leq 2\,m_{\mathcal{H}}(2N)\,\mathbb{P}[\text{fixed } \textit{h} \text{ s.t. } |E_{in}(h)-E_{in}^{'}(h)| \gt \frac{\epsilon}{2}] \\\
&\text{(用固定的hypothesis去看$E_{in}$与$E_{in}^{'}$的差别。)}
\end{aligned}
</script>

<p>&emsp;&emsp;前面的动作相当于先从总体中抽出$2N$笔数据，把这$2N$笔数据当成一个比较小的bin，然后在这个bin中抽取$N$笔作为$\mathcal{D}$，剩下的$N$笔作为$\mathcal{D}^{‘}$，$\mathcal{D}$和$\mathcal{D}^{‘}$之间是没有交集的。在我们想象出来的这个small bin当中，整个bin的错误率为$\frac{E_{in}+E_{out}}{2}$，又因为：</p>
<p>$$|E_{in}-E_{in}^{‘}|\gt \frac{\epsilon}{2} \Leftrightarrow |E_{in} - \frac{E_{in}+E_{in}^{‘}}{2}|\gt \frac{\epsilon}{4}$$</p>
<p>&emsp;&emsp;所以用RHS替换LHS之后，前面不等式就又可以使用Hoeffding inequality了：</p>
<script type="math/tex; mode=display">
\begin{aligned}
\mathbb{P}[BAD] 
&\leq 2\,m_{\mathcal{H}}(2N)\,\mathbb{P}[\text{fixed } \textit{h} \text{ s.t. } |E_{in}(h)-E_{in}^{'}(h)| \gt \frac{\epsilon}{2}] \\\
&=2\,m_{\mathcal{H}}(2N)\,\mathbb{P}[\text{fixed } \textit{h} \text{ s.t. } |E_{in}(h)-\frac{E_{in}(h)+E_{in}^{'}(h)}{2}| \gt \frac{\epsilon}{4}]\\\
&\;\;\;\text{(Hoeffding without replacement)} \\\
&\leq 2\,m_{\mathcal{H}}(2N)\cdot 2\,exp(-2(\frac{\epsilon}{4})^2N)
\end{aligned}
</script>

<p>&emsp;&emsp;这上面千辛万苦得出来的这个bound就叫做Vapnik-Chervonenkis (VC) bound：</p>
<script type="math/tex; mode=display">
\begin{aligned}
\mathbb{P}[BAD] &= \mathbb{P}[\exists h \in \mathcal{H}\text{ s.t. } |E_{in}(h)-E_{out}(h)|\gt \epsilon] \\\
&\leq 4m_{\mathcal{H}}(2N)exp(-\frac{1}{8}\epsilon^2N)
\end{aligned}
</script>

]]></content>
    <category scheme="http://beader.me/tags/Machine-Learning/" term="Machine Learning"/>
    <category scheme="http://beader.me/tags/VC-Dimension，成长函数-(growth-function)/" term="VC Dimension，成长函数 (growth function)"/>
  </entry>
  <entry>
    <title type="html"><![CDATA[蛇年summary]]></title>
    <link href="http://beader.me/2014/01/27/2013-summary/"/>
    <id>http://beader.me/2014/01/27/2013-summary/</id>
    <published>2014-01-27T05:00:00.000Z</published>
    <updated>2014-09-01T01:07:07.000Z</updated>
    <content type="html"><![CDATA[<p>&emsp;&emsp;原计划此篇应该要整理机器学习笔记-VC Dimension, Part II，但动笔整理之后才发现，其实自己果真没有理解得太透彻，以至于把林老师的视频和讲义看了好几遍，只能到不明觉厉的地步，但见武功招式，却还无法深入其内功心法。所以希望还是能多磨一些时日，争取把它搞懂。<br><a id="more"></a><br>&emsp;&emsp;我在大学是学统计的，在厦门大学，统计系是被安排在经济学院底下，大学四年有很大一部分的时间是在学习经济这方面的学问，所以以前的学长姐还有自己的同学，毕业之后大多数人的选择是往银行、证券、财务等方向的职业发展。但我选择了偏技术的路线，算是非常地与众不同了吧。<br>&emsp;&emsp;近几年来有一个东西变得越来越火，越来越被人挂在嘴边，那就是数据(data)。随着人类手上的数据量越来越大，数据种类越来越多，人们就越迫切去思考这个问题，我们该如何更好地利用这些数据，让他们产生价值呢？在需求的驱动下，一个神秘而又性感的职业诞生了— data scientist。这是一个把统计理论与计算机应用高度结合起来的职业，因此对于统计出身又对计算机很感兴趣的我，不能更有诱惑力了，我便把它当做接下来的一段时间所要为之奋斗的东西。<br>&emsp;&emsp;前一段时间听说大学的小伙伴小榴莲同学收到了google的offer，真心为你感到高兴，我也因此感到一阵心潮澎湃。想起当年周边的人都在忙着考各种银行、证券、会计从业资格证的时候，我们几个异类选择了选择了自己的兴趣，而不是“大流”，每天宅在软件园里面一块不大的小天地里面，梦想开创属于自己的事业，那段日子真是这辈子最快乐的时光。我们都应当感谢创始人老邱，你仍然是我最敬佩的人，在摩移不到一年的时光，虽然辛苦，但收获颇多，暑假意外得知自己当年写了一半的app上架了，我高兴了好一阵。真的非常期待，我们下一次的团聚。<br>&emsp;&emsp;写这个总结之前我心里一直很虚，我这个小菜鸟有什么好总结的，怕写出来的东西太水太烂被人笑话。但一直以来我却很想写写这个东西，写就写吧，全当是给自己看咯，给未来的自己看看小菜鸟今年都做了些什么。</p>
<h2 id="MOOC">MOOC</h2>
<p>&emsp;&emsp;今年4月份偶然跟高中同班的一位同学用QQ聊了几句(这人现在港科大读PHD，主攻AI方向，真正的大神)，他向我推荐了一个好东西—<a href="https://www.coursera.org/" target="_blank" rel="external">Coursera</a>。Coursera是由stanford大学教授创办的大规模开放在线课堂(MOOC)，和苹果较早推出的iTunes U很类似，只不过Coursera提供更加丰富的学习体验。在Coursera上每门课需要完成一定量的作业，如果达到开课老师规定的毕业标准，在课程结束之后可以获得一份电子版的证书 (Statement of Accomplishment)，类似下面这个东西：</p>
<p><img src="/imgs/2013-summary/da_accomplishment.png" alt="" title="da_accomplishment.png"></p>
<p>&emsp;&emsp;更加有趣的是，在Coursera的讨论区中，可以直接与一起修课的同学，助教，甚至是老师进行交流。 不需要花钱就能享受世界上最棒的那些学校的优质课程，何乐而不为？于是这大半年，有很大一部分精力都花在了Coursera上面。这大半年我一共学了这些课：</p>
<ol>
<li><a href="https://www.coursera.org/course/progfun" target="_blank" rel="external">Functional Programming Principles in Scala</a>（获得证书）。这是一门讲述函数式编程(Functional Programming)的课。函数式编程与指令式编程(Imperative Programming)相比，更容易写出安全、精简并且优雅的代码。除此之外，使用函数式编程更加容易编写用于平行运算(parallel)的代码，由于单个处理器的性能瓶颈，近年来越来越流行的做法是使用多核心处理器，甚至是多台计算机进行平行运算，以一种更加弹性(scalable)的做法提升运算性能，如果使用传统的指令式编程，实现平行化运算，十分困难。这门课的老师是Scala的发明者，说话和蔼亲切，我从中学到了不少。</li>
<li><a href="https://www.coursera.org/course/ml" target="_blank" rel="external">Machine Learning</a>（获得证书）。这里不需要再说machin learning是什么的。这门课算是ML的入门课程，不会涉及到过多数学问题，开课老师是Ng，他就是Coursera的创始人。这么课涵盖的内容蛮丰富的，当中涵盖了很多ML的方法，学完之后也能够开始做一些事情了。记得Ng在课上说，学完了Logistic Regression，你就打败了很多在硅谷从事ML工作的人了。不管你信不信，我不太信。</li>
<li><a href="https://www.coursera.org/course/compinvesting1" target="_blank" rel="external">Computational Investing, Part I</a>（获得证书）。因为大学专业的原因，学过不少经济、金融方面的内容。曾经也有一段时光憧憬过纽约那条无数人所向往的华尔街。因此对量化交易、自动化交易这块，也还保有一丝的兴趣。开课老师Dr. Tucker Balch拥有丰富的业界经验，他是Lucena Research的创始人，该公司为对冲基金提供自动化交易提供技术支持。</li>
<li><a href="https://www.coursera.org/course/compdata" target="_blank" rel="external">Computing for Data Analysis</a>（获得证书）。此门课是Johns Hopkins所开的R语言入门课程。Johns Hopkins在医疗与生物统计方面很有名气，旗下提供大量关于医疗与统计方面的课程，甚至推出了一个<a href="https://www.coursera.org/specialization/jhudatascience/1?utm_medium=listingPage" target="_blank" rel="external">data science系列课程</a>。R语言就不多做介绍了，搞统计的人一定会知道。这门课不长，只有4周，但所学知识对之后学习其他课程将有巨大的帮助。</li>
<li><a href="https://www.coursera.org/course/dataanalysis" target="_blank" rel="external">Data Analysis</a>（获得证书）。此课程可以说是Computing for Data Analysis那门课的延续。这是一门专注于数据分析方面的应用型统计的课。这门课会传授许多做数据分析的方法与技巧，以及如何撰写统计分析报告。值得一提的是，这门课想要顺利毕业，除了完成每周的quiz外，还需要根据教授给定的题目以及数据，撰写两篇1500字左右的小论文，并且每一次提交论文之后，你还需要为班上另外四位同学的论文进行评分。这个方式很新颖，但如果能坚持下来，收获会很大，尤其是自己的英文。</li>
<li><a href="https://www.coursera.org/course/ntumlone" target="_blank" rel="external">機器學習基石 (Machine Learning Foundations)</a>（获得证书）。这门课为国立台湾大学林轩田教授所开，是精品中的精品。他们团队的科研能力是全世界有目共睹的，台湾大学在2010至2013连续获得了四年(六座)的KDDCup世界冠军(真的很crazy)。这门课会涉及到较多理论层面的东西，需要比较好的数学与统计学的基础。这门课的作业难度较高，需要花费大量时间做题，除了理论推导题还有编程题，但是完成之后，内功必有大长。对于想要彻底了解ML背后的深厚理论支撑的同学，严重推荐这门课。但还是要说Foundations不是Basic，建议先选Ng的ML之后，再来学习林教授这门课，会比较合适。</li>
<li><a href="https://www.coursera.org/course/startup" target="_blank" rel="external">Startup Engineering</a>（学完但未获证书）。这门课专注于传授如何创办一个科技型公司，或者说互联网公司。这几年互联网的发展速度让人咋舌，通过一个手机APP发家的故事比比皆是，那么在进行互联网创业之前需要学习些什么呢？该课程涵盖内容甚广，包括如何使用amazon ec2虚拟机、如何把利用heroku部署网页应用、Unix命令行、emacs、git版本控制、github、HTML/CSS/Javascript、node.js、APIs、以及一些软件工程、市场研究、融资(如何接受比特币赞助)方面的知识，算得上是非常浓缩的一门课。毕业门槛也比较高，需要自己制定一个创业计划，并把此计划的大致框架给搭建好，录制宣传视频，然后再到众筹阶段，接受投资，最后还有一个排行榜，展示各位学员在结课时的表现，筹到多少钱，在facebook和twitter上的流行程度等等。虽然没有获得证书但从中受益匪浅，尤其是学了unix以及github。</li>
<li><a href="https://www.coursera.org/course/reactive" target="_blank" rel="external">Principles of Reactive Programming</a>（学了两周实在撑不下去）。这门课由Martin Odersky (Scala创始人)以及其他两位同事所教。听了实在是云里雾里，太难。响应式编程(Reactive Programming)一门结合了并发(concurrency)、事件驱动(event-based)以及异步(asynchronous)系统的新兴学科，在编写网页应用和分散式系统(distributed systems)需要大量用到此方面的知识。学习此门课要求较为深厚的functional programming的根基，只可惜我根基不实，再加上当时同时有3门课在身，无法坚持下去，因此放弃。不过未来有机会，可以再回来搞定它。</li>
</ol>
<h2 id="SAS_Data_Mining_比赛">SAS Data Mining 比赛</h2>
<p>&emsp;&emsp;今年有一件比较得意的事情就是，暑假参加了<a href="http://www.sas.com/offices/asiapacific/taiwan/" target="_blank" rel="external">台湾SAS</a>举办的<a href="http://sasmining.com.tw/" target="_blank" rel="external">「第二屆SAS校園資料採礦競賽」</a>，拿到了第二名(银牌奖)。因为每一届比赛是用同一个比赛网站，所以这个需要截图纪念一下：</p>
<p><img src="/imgs/2013-summary/sasmining_post.png" alt="" title="sasmining_post.png"></p>
<p>&emsp;&emsp;决赛结果：</p>
<p><img src="/imgs/2013-summary/sasmining_result.png" alt="" title="sasmining_result.png"></p>
<p>&emsp;&emsp;此次比赛共有86队325名同学参加，题目是利用玉山银行房贷申请的历史数据，建制信用风险模型，预测申请户发生房贷违约的风险。初赛从86支队伍中选出准确度最高的前20队进入复赛(我们在准确度上排名第2)，复赛阶段评审由各队所提交的模型建制说明ppt中，选出流程合理、理论完善的10队进入决赛，决赛则展现的是各队的现场报告能力，以及回答评委发问的应变能力。最终我们获得了银牌，拿走了10万台币的奖金，结果还是值得感到欣喜的。参加比赛的时候刚好修完Ng开的Machine Learning课，因此许多灵感都来源于那门课。我希望今年找个时间，把参赛的经验整理成一篇blog。</p>
<h2 id="Proposal">Proposal</h2>
<p>&emsp;&emsp;今年比较头大的事情是提论文计划。毕竟已经到了研二了，要想毕业就得写论文。不过我对写论文这方面的事总提不起太大的兴致。不过再纠结了一段时间，认真读了和我研究相关的一些papers之后，也提出了自己的方法。有了前面比赛的一些经验，我论文研究的方向也是与信用风险评分(credit scoring)有关。祈求今年做实验的时候也能会有好结果出来吧。</p>
<h2 id="Beader’s_Blog">Beader’s Blog</h2>
<p>&emsp;&emsp;尽管这是个无人知晓的博客，连百度和google都嫌弃，但我觉得这仍然是我本年度干的最牛逼的一件事儿。作为一个伪geek，我没有直接把blog写在新浪，网易或者啥啥博客上面，而是选择自己搭一个，谁说小菜鸟不可以？<br>&emsp;&emsp;整个blog是用markdown来写的，所以格式什么的也很简单，字都只有一个颜色，好处在于简洁专注，不需要太去思考排版(习惯word的人换用markdown来写确实有耳目一新的感觉)。还有就是想在文章内插入公式也比较方便，但也辛苦，起初因为不懂markdown和mathjax的冲突问题，需要耗费大量时间才能使公式正常显示出来，曾经光为了几个公式就折腾了我两整天，就是为了公式能够更漂亮的展示出来，你会发现这里面所有公式都不是图片，都是可以放大缩小的噢。不过偶然的一次总算被我发现问题所在，所以基本现在markdown和mathjax也比较顺利能够和谐相处了。过段时间准备写一篇来说说如何让他们和谐相处的故事。<br>&emsp;&emsp;blog是利用hexo生成出静态网页并同步到github上，因此不需要花费额外的费用，除了每年几十块的域名(beader.me)费用。</p>
<h2 id="“马上”要做什么？">“马上”要做什么？</h2>
<p>&emsp;&emsp;马年马上就要到了，马年有什么计划吗？马年最大的心愿当然是马上有工作啦。对于找工作还是弱弱地有点担忧，貌似自己水平还远远不够。不管怎么说，加油吧，当然不能忘记继续学习充电。我发现在学习ML方面的课程时，线性代数真的太重要了，尤其是之后学习一些较为复杂的ML方法时，需要非常深厚的线性代数知识。希望我能有时间把国立交通大学周志成教授的线性代数学一遍，并且多逛逛他的blog-<a href="http://ccjou.wordpress.com/" target="_blank" rel="external">线代启示录</a>。另外就是把这些根基打牢之后，能够往当下很火热的deep learning方向探一探足。想要做的事情还有很多，说多了便不容易做了，希望都能一一实现。<br>&emsp;&emsp;最后当然是希望自己的家人朋友同学老师以及看到此处的你，能够平安幸福。</p>
]]></content>
  </entry>
  <entry>
    <title type="html"><![CDATA[机器学习笔记-VC Dimension, Part I]]></title>
    <link href="http://beader.me/2014/01/23/vc-dimension-one/"/>
    <id>http://beader.me/2014/01/23/vc-dimension-one/</id>
    <published>2014-01-23T05:00:00.000Z</published>
    <updated>2014-09-01T00:52:55.000Z</updated>
    <content type="html"><![CDATA[<p>笔记整理自台大林轩田老师的开放课程-<a href="https://www.coursera.org/course/ntumlone" target="_blank" rel="external">机器学习基石</a>，笔记中所有图片来自于课堂讲义。</p>
<p>&emsp;&emsp;<a href="http://beader.me/2014/01/15/is-learning-feasible/" target="_blank" rel="external">上一篇</a>讲到，learning的时候如果遇上bad sample，如果遇上bad sample我们就无法保证$E_{in}$和$E_{out}$很接近。我们用了一个不等式来衡量遇上bad sample的概率：</p>
<p>$$\mathbb{P}_\mathcal{D}[BAD\ D]\leq 2Mexp(-2\epsilon ^2N)$$</p>
<p><a id="more"></a><br>&emsp;&emsp;因此如果Hypothesis Set中方程的数量$|\mathcal{H}|=M$是有限个，并且N足够大的时候，我们就有很大的概率保证不会遇上bad sample，即不管$\mathcal{A}$挑选了任意一个$g$，都可以保证$E_{out}(g) \approx E_{in}(g)$。因此learning的问题可以聚焦到以下两个问题：</p>
<ol>
<li>能不能确保$E_{in}(g)$和$E_{out}(g)$很接近？(Hypothesis Set中方程的个数会不会太多，用来训练的数据量够不够大)</li>
<li>能不能使$E_{in}(g)$足够小？(Hypothesis Set中方程的个数会不会太少)  </li>
</ol>
<p>&emsp;&emsp;这里我们便遇到了一个两难的选择：</p>
<ul>
<li>如果M很小，根据$\mathbb{P}[BAD]\leq 2Mexp(…)$，第1个问题，“很接近”，做的不错。但是对于第2个问题，由于选择性太少，很难找到$E_{in}(g)$较小的$g$。(想象一下如果数据是由一个二次方程所产生，而Hypothesis Set当中只有直线方程可以选)</li>
<li>如果M很大，我们选择方程的时候更加自由(自由度)，更有机会选到$E_{in}$很低的方程。但这个时候根据公式，我们遇到bad sample的可能性也大大增加。</li>
</ul>
<p>&emsp;&emsp;因此这篇笔记主要围绕公式中这个$M$展开：</p>
<p>$$\mathbb{P}[| E_{in}(g) - E_{out}(g) | \gt \epsilon] \leq 2\cdot M \cdot exp(-2\epsilon ^2N)$$</p>
<h2 id="有效的方程_(Effective_Number_of_Hypotheses)">有效的方程 (Effective Number of Hypotheses)</h2>
<p>&emsp;&emsp;让我们来回忆下这个M是从哪里来的。记$\mathcal{H}$中第m个方程遇到bad sample为事件$\mathcal{B}_m:|E_{in}(h_m) - E_{out}(h_m)| \gt \epsilon$，则$\mathcal{H}$遇到bad sample的概率为其所有方程遇到bad sample概率的联合概率。如果每个方程遇上bad sample这件事是互相独立的，则$\mathcal{H}$遇上bad sample的概率是各方程遇上bad sample的概率之和，因此他们的联合概率一定小于等于各个事件单独发生的概率之和。</p>
<p>$$\mathbb{P}[\mathcal{B}_1 or \mathcal{B}_2 or … or \mathcal{B}_M] \leq \mathbb{P}[\mathcal{B}_1] + \mathbb{P}[\mathcal{B}_2] + … + \mathbb{P}[\mathcal{B}_M]$$</p>
<p>&emsp;&emsp;但事实上bad event并不是完全独立的。想象$\mathcal{H}$两个非常类似的方程$h_1\approx h_2$，他们遇到bad sample分别为事件$\mathcal{B}_1$与$\mathcal{B}_2$，因为这两个方程很接近，则往往$\mathcal{B}_1$发生时，$\mathcal{B}_2$也会发生，可以说$\mathcal{B}_1$与$\mathcal{B}_2$的重合度很高(overlapping)。</p>
<p><img src="/imgs/vc-dimension-one/hypothesis_overlap.png" alt="" title="hypothesis_overlap.png"></p>
<p>&emsp;&emsp;那么我们就会想，我们能不能把结果接近的那些方程看成一类，譬如有些方程他们的预测结果总是相同或是很接近的。</p>
<p>&emsp;&emsp;假设我们的算法要在平面上挑选一条直线方程作为$g$,$\mathcal{H}={all\ lines\ in\  \mathbb{R}^2}$，$\mathcal{H}$当中有无限多个方程，但我们可以把这些个方程归为两类。一类是把$x_1$判断成圈圈的，另一类是把$x_1$判断为叉叉的。</p>
<p><img src="/imgs/vc-dimension-one/1point2lines.png" alt="" title="1point2lines.png"></p>
<p>&emsp;&emsp;那如果我们手中有2个数据点$x_1$和$x_2$呢？这样的话$\mathcal{H}$中无数条直线可以分为4类。用这4类线对$x_1$和$x_2$进行预测，一共能产生4种不同的结果。</p>
<p><img src="/imgs/vc-dimension-one/2points4lines.png" alt="" title="2points4lines.png"></p>
<p>&emsp;&emsp;那如果我们手中有3个数据点$x_1$、$x_2$和$x_3$呢？</p>
<p><img src="/imgs/vc-dimension-one/3points8lines.png" alt="" title="3points8lines.png">  </p>
<p>&emsp;&emsp;$\mathcal{H}$中最多有8类直线，作用于$\mathcal{D}$产生如上8种结果。</p>
<p>&emsp;&emsp;那如果我们手中有4个数据点$x_1$~$x_4$，情况又会是怎样。前面的例子中我们基本上把各个数据点的情况用排列组合的方式组合出来即可。但对于4个以上的点而言，就不是那么容易了。</p>
<p><img src="/imgs/vc-dimension-one/4points14lines.png" alt="" title="4points14lines.png"><br>&emsp;&emsp;在这16种组合中，就有两种是“直线方程”没有办法产生的结果。因此如果$\mathcal{H}$是2维空间中的所有直线，表面上看是在无数条直线方程中去挑，但由于大部分直线方程所产生的结果是一模一样的，结果不一样的直线的类别对应上面的例子分别为2类、4类、8类和14类(Effective Number of Lines)。属于同一类的直线，他们将同时遇到或不遇到bad sample，由于之前那个union bound是基于独立性的假设下的，因此$\mathcal{H}$遭遇bad sample的概率明显被夸大了。所以，我们应该把不等式改写为：</p>
<p>$$\mathbb{P}[|E_{in}(g)-E_{out}(g)|\gt \epsilon]\leq 2\cdot effective(N)\cdot exp(-2\epsilon^2N)$$</p>
<h2 id="$\mathcal{H}$作用于$\mathcal{D}$能够产生多少不同的结果？_(Dichotomies)">$\mathcal{H}$作用于$\mathcal{D}$能够产生多少不同的结果？ (Dichotomies)</h2>
<p>&emsp;&emsp;从$\mathcal{H}$中任意选取一个方程$h$，让这个$h$对$\mathcal{D}$进行二元分类，输出一个结果向量，比如对4个点进行预测，输出${\mathrm{o},\mathrm{o},\mathrm{o},\times}$，这样的一个输出向量我们称它为一个dichotomy。不难得出，一个直线方程在$\mathcal{D}$中对应一个dichotomy，但一个dichotomy至少对应一个直线方程，我们把一个dichotomy对应的所有直线方程视为一类，则effective number of lines就等于不同$\mathcal{D}$中不同的dichotomy的数量。显然这个dichotomy的数量小于等于所有数据点的排列组合数的，例如上图中画大叉的那幅图对应的排列组合，就不能成为一个dichotomy，因为它们无法由任何一条直线方程产生。（当然如果考虑的不是直线方程，则那种排列组合是可以成为一种dichotomy的）<br>&emsp;&emsp;因此我们前面要找的<br>&emsp;&emsp;$effective(N)$<br>&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;=平面中能找出多少条不同类的直线<br>&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;=$\mathcal{H}$作用于$\mathcal{D}$能产生多少不同的dichotomy。<br>&emsp;&emsp;因为不一定所有排列组合都能成为dichotomy，所以不同的dichotomy的数量一定不会超过排列组合数$2^N$，上例中如果存在三点共线的情况，则dichotomy的数量会更少，因此：<br>&emsp;&emsp;$effective(N)$<br>&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;=$\mathcal{H}$作用于$\mathcal{D}$“<strong>最多</strong>”能产生多少不同的dichotomy</p>
<h2 id="成长函数_(Growth_Function)">成长函数 (Growth Function)</h2>
<p>&emsp;&emsp;那么，$\mathcal{H}$作用于$\mathcal{D}$“<strong>最多</strong>”能产生多少种不同的dichotomy呢？这个数量与$\mathcal{H}$有关，也与数据量$N$有关。用数学式可以表达为：</p>
<p>$$max|\mathcal{H}(x_1,x_2,…,x_N)|$$</p>
<p>&emsp;&emsp;上式又称为成长函数（growth function）。在$\mathcal{H}$确定的情况下，growth function是一个与N相关的函数。以下是几种常见的Hypothesis Set的成长函数。</p>
<ol>
<li><p>Positive Rays<br><img src="/imgs/vc-dimension-one/postive_rays.png" alt="" title="postive_rays.png"><br>输入空间为一维实数空间。大于threshold a的预测+1,否则预测-1。<br>for example: 当N=4时，Positive Rays作用于$x_1$~$x_4$，共能产生5个不同的dichotomies。如下图：<br><img src="/imgs/vc-dimension-one/positive_rays_dichotomies.png" alt="" title="positive_rays_dichotomies.png"><br>不难去想，4个点，5个可能的切点，最多产生5种dichotomies。因此Positive Rays的成长函数为：<br>$$m_{\mathcal{H}(N)}=N+1$$</p>
</li>
<li><p>Positive Intervals<br><img src="/imgs/vc-dimension-one/positive_intervals.png" alt="" title="positive_intervals.png"><br>和前面的类似，只不过Positive Intervals有两个threshold，夹在两个threshold之间的预测为+1，其余预测为-1。<br>for example: 当N=4时，Positive Intervals作用于$x_1$~$x_4$，共能产生11种dichotomies。如下图：<br><img src="/imgs/vc-dimension-one/positive_intervals_dichotomies.png" alt="" title="positive_intervals_dichotomies.png"><br>同样不难去想，4个点，5个可能的切点选两个作为threshold，加上两个threshold重合产生的一种，因此Positive Intervals的成长函数为：<br>$$m_{\mathcal{H}(N)}=\binom{N+1}{2} + 1 = \frac{1}{2}N^2 + \frac{1}{2}N + 1$$</p>
</li>
<li><p>Convex Sets<br><img src="/imgs/vc-dimension-one/convex_sets.png" alt="" title="convex_sets.png"><br>任选k个点，在这k个点组成的convex多边形包围内的所有点都预测+1，否则预测-1。前面我们说到成长函数描述的是“<strong>最多</strong>”能产生的dichotomy种数，因此如果我们这N个input摆成一个圈，则这N个点的任意一种排列组合都能成为一个dichotomy。因此Convex Sets的成长函数为：<br>$$m_{\mathcal{H}(N)}=2^N$$</p>
</li>
</ol>
<h2 id="坏掉的散弹枪_(Shatter_&amp;_Break)">坏掉的散弹枪 (Shatter &amp; Break)</h2>
<p>&emsp;&emsp;当$\mathcal{H}$作用于有N个inputs的$\mathcal{D}$时，产生的dichotomies数量等于这N个点的排列组合数$2^N$时，我们就称这N个inputs被$\mathcal{H}$给shatter掉了。或者也可以说$\mathcal{H}$产生的$2^N$个dichotomies把这N个点的$2^N$种排列组合给shatter了。</p>
<p>&emsp;&emsp;这个shatter的意思似乎不太好理解，这是林老师在讨论区中的回复：</p>
<p>&emsp;&emsp;“大家對 break point 的討論很好，不過注意到 shatter 的原意是「打碎」，在此指「N 個點的所有(碎片般的)可能情形都被$\mathcal{H}$產生了」。所以$m_{\mathcal{H}}(N)=2^N$ 的情形是「shatter」。”</p>
<p>&emsp;&emsp;我从打游戏过关的角度去理解，shatter作打碎理解：</p>
<p>&emsp;&emsp;“$m_{\mathcal{H}}$是把散弹枪，在每个关卡(level N)中，他可以有$m_{\mathcal{H}}(N)$发小子弹（每发小子弹对应一种dichotomy），而你面临的是$2^N$个敌人。你得一枪打出去shatter掉所有人。对于$m_{\mathcal{H}}(N)=2N$这把散弹枪来说，第一关和第二关都还好，第三关6发小子弹shatter不掉8个人，于是它就break了。”</p>
<p>&emsp;&emsp;对于给定的成长函数$m_{\mathcal{H}}(N)$，从$N=1$出发，N慢慢增大，当增大到k时，出现$m_{\mathcal{H}}(k)\lt 2^k$的情形，则我们说k是该成长函数的break point，对于任何$N \gt k$个inputs而言，$\mathcal{H}$都没有办法再shatter他们。</p>
<p>&emsp;&emsp;不难根据成长函数得出Positive Rays成长函数的break point为2，Positive Intervals成长函数的break point为3，Convex Sets不管N多大都可以去shatter掉那N个点，因此它的成长函数没有break point。2D Perceptrons的break point为4，因为在N=3时，它都能够shatter，产生$2^3=8$种dichotomies，当N=4时，它不能够shatter，最多只能产生14 ($\lt 2^4 = 16$) 种dichotomies，因此2D Perceptrons成长函数的break point为4。</p>
<h2 id="拿什么来镇镇成长函数_(Restriction_of_Break_Point)">拿什么来镇镇成长函数 (Restriction of Break Point)</h2>
<p>&emsp;&emsp;有些$\mathcal{H}$的成长函数很容易找到，比如前面说到的Positive Rays、Positive Intervals以及Convex Sets；有些$\mathcal{H}$则没有那么容易，比如2D perceptrons，我们无法直接看出它的成长函数是什么，那么我们对于这样的$\mathcal{H}$就没辙了吗？也不完全是，至少我们手上还掌握着它的break point，能不能用这个break point干点事呢？如果没办法得到成长函数，能得到成长函数的upper bound也是不错的。</p>
<p>&emsp;&emsp;先用例子来看看，当我们完全不知道$\mathcal{H}$是什么，只知道它的break point k时，$\mathcal{H}$作用于$\mathcal{D}$“<strong>最多最多</strong>”可以产生多少这dichotomies。注意这里我用了两个“<strong>最多</strong>”，由于我们无法确切知道成长函数，因此我们用这个break point推算出的这个dichotomies的数量仍然是个高估值，这个高估值实际上是任何break point为k的$\mathcal{H}$作用于$\mathcal{D}$所真实产生的dichotomies数量的上界 (upper bound)。</p>
<p>&emsp;&emsp;举例说明，假设我们不知道某个$\mathcal{H}$的成长函数$m_{\mathcal{H}}(N)$，但知道它的break point k=2，那么$\mathcal{H}$作用于N=3的$\mathcal{D}$时，“<strong>最多最多</strong>”能产生多少种dichotomies？</p>
<p>&emsp;&emsp;从k=2我们可以知道，任意2个数据点都不能被shatter。还记得shatter的概念吗？意思就是我产生的dichotomies不能完全包含任何2个数据点所有的排列组合。让我们从1个dichotomy开始。</p>
<p>1 dichotomy <img src="/imgs/vc-dimension-one/n3k2d1.png" alt="" title="n3k2d1.png"></p>
<p>2 dichotomies <img src="/imgs/vc-dimension-one/n3k2d2.png" alt="" title="n3k2d2.png"></p>
<p>3 dichotomies <img src="/imgs/vc-dimension-one/n3k2d3.png" alt="" title="n3k2d3.png"></p>
<p>&emsp;&emsp;注意看$x_2$和$x_3$这两列，这3个dichotomies已经包含$x_2$和$x_3$这两个点所有的4种排列组合中的3种了。再多加一种，$x_2$、$x_3$就会被shatter。</p>
<p>4 dichotomies <img src="/imgs/vc-dimension-one/n3k2d4s.png" alt="" title="n3k2d4s.png"></p>
<p>&emsp;&emsp;看右边两列，$x_2$和$x_3$被shatter了。但之前说了k=2，即任意2个点不能被shatter，因此不可能产生这4种dichotomies。那我们换一个dichotomy试试看。</p>
<p>4 dichotomies <img src="/imgs/vc-dimension-one/n3k2d4.png" alt="" title="n3k2d4.png"></p>
<p>&emsp;&emsp;换了一个dichotomy之后就行了，右边2列只包含了$x_2$、$x_3$所有排列组合4种中的3种，因此那两个点没有被shatter。继续检查任意的两个点($x_1$、$x_2$)，($x_1$、$x_3$)，都没有被shatter，看来这4种dichotomies是可以的。</p>
<p>&emsp;&emsp;5个dichotomies的情形这里就不再画出来了，很容易看出不管增加怎样的dichotomy进去，都会有两个点被shatter掉。因此这里“<strong>最多最多</strong>”只能有4种dichotomies。因此$N=3$，$k=2$时的upper bound是4。我们用$B(N,k)$来表示break point为k的任意的$\mathcal{H}$作用于size为N的任意的$\mathcal{D}$所能产生的dichotomies的数量的上限（“<strong>最多最多</strong>”）。则刚刚得出的结论可以表示为$B(3,2)=4$，因为任意2个数据点不能被shatter，因此当$N=2,K=2$时，$B(2,2)\lt 4$，因此最多最多有$B(2,2)=3$。</p>
<p>&emsp;&emsp;美妙的地方马上就要到了，虽然很多时候我们无法直接得到成长函数$m_{\mathcal{H}}(N)$，但如果我们知道它的break point是多少，我们似乎还是有办法算出这个$m_{\mathcal{H}}(N)$的上界$B(N,k)$的。于是乎我们就有了新的目标，不去直接研究$m_{\mathcal{H}}(N)$，转而去研究$B(N,k)$。</p>
<p>&emsp;&emsp;前面我们已知了$B(2,2)=3$，$B(3,2)=4$。不难知道：</p>
<ul>
<li>$k=1$时，1个点(2种排列组合)都没有办法shatter，因此$B(?,1)$恒等于1。</li>
<li>$k&gt;N$时，$\mathcal{H}$一定能shatter掉$N$个点，因此它产生的dichotomies的种类等于这$N$个点所有的排列组合数$2^N$。</li>
<li>$k=N$时，从$2^N$个排列组合中移除掉一个，剩下的都可以作为dichotomies，因此它产生的dichotomies的数量“<strong>最多最多</strong>”可以是$2^N-1$。</li>
</ul>
<p>&emsp;&emsp;因此我们可以得到下表：</p>
<p><img src="/imgs/vc-dimension-one/bnk_table.png" alt="" title="bnk_table.png"></p>
<p>&emsp;&emsp;表格剩余的部分该如何填补？$B(4,3)$是否与$B(3,?)$有关呢？<br>&emsp;&emsp;此时某某实验室帮老师抛硬币的研究生要上场了，他穷举了所有可能的dichotomies，发现$B(4,3)=11$，以下是他的研究成果：</p>
<p><img src="/imgs/vc-dimension-one/n4k3d11.png" alt="" title="n4k3d11.png"></p>
<p>&emsp;&emsp;我们把这份结果做个排序：</p>
<p><img src="/imgs/vc-dimension-one/n4k3d11_ordered.png" alt="" title="n4k3d11_ordered.png"></p>
<p>&emsp;&emsp;发现秘密了没有，橙色的部分是成对出现的，只有紫色的部分是单独出现的：</p>
<p><img src="/imgs/vc-dimension-one/n4k3d11_ordered2.png" alt="" title="n4k3d11_ordered2.png"> </p>
<ol>
<li><p>如果拿掉$x_4$，只看$x_1,x_2,x_3$这3个点：<br><img src="/imgs/vc-dimension-one/n4k3d7.png" alt="" title="n4k3d7.png"><br>&emsp;&emsp;$\alpha + \beta$部分可以成为这3个点的dichotomies，因为$k=3$，所以任3个点不能够被shatter，因此有：：<br>$$\alpha + \beta \leq B(3,3)$$</p>
</li>
<li><p>再来看剩下一个$\alpha$的部分：<br><img src="/imgs/vc-dimension-one/n4k2_alpha.png" alt="" title="n4k2_alpha.png"><br>&emsp;&emsp;注意$\alpha$是之前成对存在的部分，并且$\alpha$部分不可以shatter掉任意2个点，因为如果$\alpha$部分的dichotomies可以shatter掉任意2个点，他每一行都再搭配$x_4$的两种情况，这样产生的dichotomies就能shatter掉3个点了，和break point为3相违背。所以$\alpha$不能shatter掉任2个点。因此有：<br>$$\alpha \leq B(3,2)$$</p>
</li>
</ol>
<p>&emsp;&emsp;综合上面两个不等式，我们可以得到：<br>$$B(4,3)=2\alpha + \beta \leq B(3,3) + B(3,2)$$</p>
<p>&emsp;&emsp;这样就能够把前面那张表给填完整：</p>
<p><img src="/imgs/vc-dimension-one/bnk_table_full.png" alt="" title="bnk_table_full.png"></p>
<script type="math/tex; mode=display">
\begin{aligned}
B(N,k) &= 2\alpha + \beta \\\
\alpha + \beta &\leq B(N-1,k) \\\
\alpha &\leq B(N-1,k-1) \\\
\Rightarrow B(N,k) &\leq B(N-1,k) + B(N-1,k-1)
\end{aligned}
</script>

<p>&emsp;&emsp;用数学归纳法可以证明：<br>$$B(N,k)\leq \sum_{i=0}^{k-1}\binom {N}{i}$$</p>
<p>&emsp;&emsp;当$k=1$时不等式恒成立，因此只要讨论$k\geq 2$的情形。$N=1$时，不等式成立，假设$N\leq N_{o}$时对于所有的$k$不等式都成立，则我们需要证明当$N=N_{o}+1$时，不等式也成立。根据前面得到的结论，有：</p>
<script type="math/tex; mode=display">
\begin{aligned}
B(N_{o}+1,k) &\leq B(N_{o},k) + B(N_{o},k-1) \\\
&\leq \sum_{i=0}^{k-1}\binom{N_{o}}{i}+\sum_{i=0}^{k-2}\binom{N_{o}}{i} \\\
&=1+\sum_{i=1}^{k-1}\binom{N_{o}}{i}+\sum_{i=1}^{k-1}\binom{N_{o}}{i-1} \\\
&=1+\sum_{i=1}^{k-1}[\binom{N_{o}}{i}+\binom{N_{o}}{i-1}] \\\
&=1+\sum_{i=1}^{k-1}\binom{N_{o}+1}{i}=\sum_{i=0}^{k-1}\binom{N_{o}+1}{i}
\end{aligned}
</script>

<p>&emsp;&emsp;因此当$N=N_{o}+1$时，不等式也成立。</p>
<p>&emsp;&emsp;成长函数的上界-$B(N,k)$都被bound住了，那我们的成长函数同样也可以被这个bound住，因此对于存在break point k的成长函数而言，有：<br>$$m_{\mathcal{H}}\leq \sum_{i=0}^{k-1}\binom {N}{i}$$<br>&emsp;&emsp;By the way，右手边(RHS)实际上是一个最高次项为k-1次的多项式。以2D Perceptrons为例，它的break point $k=4$，则它的成长函数会被$B(N,4)$给bound住：<br>$$m_{\mathcal{H}}\leq \sum_{i=0}^{4-1}\binom {N}{i}=\frac{1}{6}N^3+\frac{5}{6}N+1$$</p>
<h2 id="总结">总结</h2>
<p>&emsp;&emsp;上一篇说的learning的可行性，讲到，如果遇上bad sample，$E_{in}(h)$与$E_{out}(h)$就会差很多，此时learning不可行。遇到bad sample的概率与$\mathcal{H}$中方程的数量$M$以及$\mathcal{D}$中的数据量$N$有关。然而$\mathcal{H}$中方程的数量往往是无穷的(比如2D Perceptrons的$\mathcal{H}$是平面上所有的直线)，本篇则继续阐述，方程的数量看上去是无穷的，但真正有效(effective)的方程的数量却是有限的，我们可以用成长函数$m_{\mathcal{H}}(N)$来描述$\mathcal{H}$作用于$\mathcal{D}$会产生多少种有效的方程。如果用有限的成长函数去代替无限的$M$，就有：</p>
<p>$$\mathbb{P}_\mathcal{D}[BAD\ D]\leq 2m_{\mathcal{H}}(N)\cdot exp(-2\epsilon ^2N)$$</p>
<p>&emsp;&emsp;但实际上我们很难确切知道各种$\mathcal{H}$的成长函数$m_{\mathcal{H}}(N)$究竟长什么样子，我们只好通过break point去寻找成长函数的upper bound。不过这当中仍然有些情况没有考虑到，将在下一篇笔记中继续说明。</p>
]]></content>
    <category scheme="http://beader.me/tags/Machine-Learning/" term="Machine Learning"/>
    <category scheme="http://beader.me/tags/VC-Dimension，成长函数-(growth-function)/" term="VC Dimension，成长函数 (growth function)"/>
  </entry>
  <entry>
    <title type="html"><![CDATA[机器学习笔记-机器为何能够学习?]]></title>
    <link href="http://beader.me/2014/01/15/is-learning-feasible/"/>
    <id>http://beader.me/2014/01/15/is-learning-feasible/</id>
    <published>2014-01-15T04:00:00.000Z</published>
    <updated>2014-09-01T00:56:26.000Z</updated>
    <content type="html"><![CDATA[<p>笔记整理自台大林轩田老师的开放课程-<a href="https://www.coursera.org/course/ntumlone" target="_blank" rel="external">机器学习基石</a>，笔记中所有图片来自于课堂讲义。</p>
<h2 id="机器学习的基础架构_(Learning_From_Data)">机器学习的基础架构 (Learning From Data)</h2>
<p>&emsp;&emsp;银行在决定是否要通过贷款申请人的授信请求前，会根据申请人的资料对其进行风险评估，(通常银行会为其计算信用评分)，申请人状况符合银行要求时，银行通过其申请，反之则婉拒。那么银行凭借什么来判断申请人将来是否会违约呢？通过银行之前的信用贷款记录，这些记录中，有些客户发生了违约行为，其他则表现良好，银行从这些违约与非违约的记录中learning到了一些规律，然后利用这些规律，来对新申请人的违约风险进行估计。因此信用评估模型就是一个learning的问题，那么我们该如何使用历史数据做好learning呢？</p>
<p>&emsp;&emsp;下面这张图描述了learning的基础架构：</p>
<p><img src="/imgs/is-learning-feasible/basic_setup_of_the_learning_problem.png" alt="" title="basic_setup_of_the _learning_problem.png"><br><a id="more"></a></p>
<ul>
<li><p>$f:\mathcal{X} \to \mathcal{Y}$，其中$\mathcal{X}$表示输入空间，譬如下图中第一列</p>
<p> <code>(age, gender, annual salary, year in residence, year in job, current debt)</code></p>
<p> 为输入空间(6维)，而右边一列</p>
<p> <code>(23 years, female, NTD 1,000,000, 1 year, 0.5 year, 200,000)</code></p>
<p> 为该输入空间下的一个向量，每位贷款申请人对应该空间下的一个向量。$\mathcal{Y}$表示输出空间，在二元分类中，输出空间是一个1维的取值为+1或-1的空间 $\{-1,+1\}^1$，可以用-1表示非违约，+1表示违约。<br> $f$是未知的真理，是事物运转的规律，假如我们可以拥有$f$，我们就可以知道一个人到底会不会发生违约行为。但是这个$f$是不可知的，我们无法窥探其中运行的原理(函数内部构造)，我们唯一知道的是$f$在我们已知的历史数据$\mathcal{D}$当中的运行情况(把$x_n$当做$f$输入，把$y_n$当做$f$的输出)，learning要作的事情，就是找一个在$\mathcal{D}$中运行情况与$f$类似的函数，这个函数对于相同的输入，会有与$f$相同的输出，并且希望在$\mathcal{D}$之外，也就是我们未知的世界，我们找的这个函数的运行情况还能与$f$接近。</p>
<p> <img src="/imgs/is-learning-feasible/feature_vector.png" alt="" title="feature_vector.png"></p>
</li>
<li><p>$\mathcal{D}:(x_1,y_1), \dotsb,(x_N,y_N)$为训练集，该训练集有N笔数据，每笔数据由某申请人在$\mathcal{X}$中的向量和与其对应的类别构成。</p>
</li>
<li><p>$\mathcal{H}$，hypothesis set是一个由有限个或无限个方程组成的集合，算法$\mathcal{A}$只能从$\mathcal{H}$的范围内挑选方程。</p>
</li>
<li><p>$\mathcal{A}$，是一个学习算法，它能够帮助我们在$\mathcal{H}$中找到一个与$f$的判断最接近或足够接近的一个方程。当然我们可以说穷举法是一种学习算法，当$\mathcal{H}$当中candidate formula数量不多时，我们可以用穷举法来寻找。但往往candidate formula数量很大甚至是无穷的，我们就需要设计一个比较好的算法，他能够在较短时间找到我们想要的那个方程。</p>
</li>
<li><p>$g$，final hypothesis，即$\mathcal{A}$从$\mathcal{H}$中挑选的和$f$判断最接近的那个方程。</p>
</li>
</ul>
<p>&emsp;&emsp;说到底，learning在干的事情，就是从hypothesis set里面挑一个“长”的最像$f$的方程$g$，注意前面我的用词是用”它的判断接近$f$”，并不是说$g$和$f$结构很类似，(记住$f$永远是unknown的)，而是说他们的判断很一致，即$f(x)\approx g(x)$。并且这里谈到的接近，是针对训练集$\mathcal{D}$而言的，$\mathcal{D}$之外的数据他们能否表现一致，这才是我们最应该关心的问题，如果$\mathcal{D}$之外他们也能够表现一致，说明我们learning的还不错，我们有从$\mathcal{D}$上面学到东西。这时候换个角度来想，能不能有某个理论，来保证我们的$g$与$f$在$\mathcal{D}$之外也能有差不多的接近程度？</p>
<h2 id="Learning真的可行吗？_(Is_Learning_Feasible?)">Learning真的可行吗？ (Is Learning Feasible?)</h2>
<p><img src="/imgs/is-learning-feasible/a_learning_puzzle.png" alt="" title="a_learning_puzzle.png"></p>
<p>&emsp;&emsp;图片前两行为training set，对于第一行的所有样本，有$f(x)=-1$，对于第二行的所有样本，有$f(x)=+1$，那么我们能不能通过这6笔数据来猜测一下$f$是长什么样的呢？同学1和同学2利用各自的学习方法分别给出了自己的$g$ </p>
<ul>
<li>同学1训练出来的g：<br>$g_1(对称图形)=+1$<br>$g_1(非对称图形)=-1$</li>
<li>同学2训练出来的g：<br>$g_2(左上角为白色的图形)=+1$<br>$g_2(左上角为黑色的图形)=-1$</li>
</ul>
<p>&emsp;&emsp;对于training set中所有样本，有$g_1(x)=g_2(x)=f(x)$，即两个$g_1,g_2$与$f$的表现是一致的，似乎可以认为他们都”学”到了东西，但是对于测试样本来说$g_1(x)=+1,g_2(x)=-1$。真实的$f$我们无法知道，如果他们中的某一个人在所有非训练的资料中也和$f(x)$表现一致，我们才能说他们当中某个人真的学到了东西。但在目前这种情况下，我们无法说同学1学到了东西还是同学2学到了东西。</p>
<p>&emsp;&emsp;让我们再来考虑一个简单的二元分类问题。$\mathcal{X}=\{0,1\}^3$，$\mathcal{Y}=\{\mathrm{o},\times\}^1$</p>
<p><img src="/imgs/is-learning-feasible/a_simple_binary_classification_problem.png" alt="" title="a_simple_binary_classification_problem.png"></p>
<p>&emsp;&emsp;为何要举这么简单的例子呢？因为前面我们说到真实的$f$是我们无法知道的，但在上面这个简单的例子中，我们有办法把所有可能的$f$全部列举出来。</p>
<p><img src="/imgs/is-learning-feasible/no_free_lunch.png" alt="" title="no_free_lunch.png"></p>
<p>&emsp;&emsp;可能产生这样的$\mathcal{D}$的$f$只可能有8种并且只有其中的1个是正确的，这样一来，虽然我们可以保证我们的$g$在$\mathcal{D}$上的判断和真实的$f$一致，但我们无法保证在$\mathcal{D}$之外也同样如此。那么这里便值得怀疑一下机器学习的可行性，机器学习到底可不可行？</p>
<h2 id="推断未知的世界(Inferring_Something_Unknown)">推断未知的世界(Inferring Something Unknown)</h2>
<p>&emsp;&emsp;我们知道在前面简单版的learning问题中，由于我们无法推断$\mathcal{D}$之外事情，因此learning不可行。但在其他场景中，我们能否利用$\mathcal{D}$来推断$\mathcal{D}$以外的事情呢？在统计推断中，我们可以利用样本的统计量(statistic)来推断总体的参数(parameter)，譬如使用样本均值来估计总体期望。假设你想了解某一批大米(1000包)的平均重量，可以通过随机抽样抽取一定数量的样本(20包)，用这20包大米的平均重量估计这1000包大米的平均重量，虽然二者并不一定会完全相等，但也不会相差太大，并且你的样本量越大，你得到的统计量与参数之间的误差会越小。在这个例子中，未知的980包大米的平均重量与我们知道的20包大米的平均重量是有关联的，因此我们似乎可以通过$\mathcal{D}$来推断$\mathcal{D}$之外的东西。下面来一个摸球的例子：</p>
<p><img src="/imgs/is-learning-feasible/bin_sample.png" alt="" title="bin_sample.png"></p>
<ul>
<li>bin，即总体，假设$P(orange)=\mu$，$P(green)=1-\mu$。但我们无法知道$\mu$到底多大。</li>
<li>sample，即样本，数量为$N$，在抽出的$N$个小球中，orange的比例为$\nu$，green的比例为$1-\nu$，数一数就能算出$\nu$</li>
</ul>
<p>&emsp;&emsp;问题来了，这个$\nu$能不能在一定程度上代表了$\mu$。也许不能，因为即使bin中orange占多数，也可能发生这样的事情，你抽了10个小球出来但全是green的。但这种事情发生的可能性大吗？不大，并且如果我们有更多的样本(抽出更多的球)，则这种事情发生的可能性会越来越小。在概率论中，可以用<a href="http://en.wikipedia.org/wiki/Hoeffding&#39;s_inequality" target="_blank" rel="external">Hoeffding’s Inequality</a>来描述上面那件事情的概率：</p>
<p>$$\mathbb{P}[|\nu-\mu|&gt;\epsilon]\leq 2exp(-2\epsilon ^2N)$$</p>
<p>&emsp;&emsp;注：$\epsilon$是我们的容忍度，当$\mu$与$\nu$的差别小于容忍度时，我们称$\mu$与$\nu$“差不多”(PAC, probably approximately correct)，当$\mu$与$\nu$差别大于容忍度时，我们称$\mu$与$\nu$”差很多”。“差很多”这件事发生的概率越小越好，最大不会超过右边。</p>
<p>&emsp;&emsp;上面这个不等式中，控制右边数值大小的只有$\epsilon ^2$和$N$，$\epsilon ^2$减小(要求降低)与$N$(样本增加)增大都能够使坏事情发生的概率的上限减少。当上限足够小的时候，我们可以说，sample中orange的比例和bin中orange的概率<strong>差不多</strong>，如果sample中的orange比例少，则bin中的orange的比例也会比较少。</p>
<p>&emsp;&emsp;我们可以把learning与抓球这件事结合起来。</p>
<p><img src="/imgs/is-learning-feasible/marble_learning.png" alt="" title="marble_learning.png"></p>
<p>&emsp;&emsp;还记得之前说过的$f$吗？他代表未知的真理，而$h(x)是$是属于hypothesis set $\mathcal{H}$的某一个方程。对于某一个向量$x_n$：</p>
<ul>
<li>如果$h(x_n)\neq f(x_n)$，即他们判断不一致，我们记第n个小球是orange</li>
<li>如果$h(x_n)=f(x_n)$, 即他们判断是一致的，我们记第n个小球是green</li>
</ul>
<p>&emsp;&emsp;利用之前抓小球的逻辑，我们可以利用sample中orange的比例来推断总体中orange出现的概率，则同样的，我们可以利用sample中$h(x)\neq f(x)$的比例来推断总体中$h(x)\neq f(x)$的概率。这里$h(x)\neq f(x)$表示一个error，则我们可以称 $h(x)$ 在sample中出现error的比例为 $E_{in}$ (in-sample-error)，在总体中出现error的概率为 $E_{out}$ (out-of-sample-error)。则对于 $h$ 来说：</p>
<ul>
<li><p>$E_{out}(h) = \underset{x\sim P}{\epsilon} [h(x)\neq f(x)]$，$\epsilon$表示数学期望</p>
</li>
<li><p>$E_{in}(h) = \frac{1}{N}\sum_{n=1} ^ {N}[h(x_n)\neq y_n]$</p>
</li>
</ul>
<p>&emsp;&emsp;利用Hoeffding’s Inequality，我们可以写成：</p>
<p>$$\mathbb{P}[|E_{in}(h)-E_{out}(h)|\gt \epsilon]\leq 2 exp(-2\epsilon ^2N)$$</p>
<p>&emsp;&emsp;简单说来，当右边这个“上界”足够小时，我们可以说$h$在sample中的表现(错误率)与$h$在总体中的表现是差不多的。</p>
<p><img src="/imgs/is-learning-feasible/setup_of_the_learning_problem_add_components.png" alt="" title="setup_of_the_learning_problem_add_components.png"></p>
<p>&emsp;&emsp;注意这里仅仅是说，对于一个固定的$h (fixed h)$而言，$E_{in}(h)$会与$E_{out}(h)$很接近，这种情况能说是一种好的learning吗？当然不能，因为如果$E_{in}(h)$很大，则$E_{out}$也大，这样是没有意义的。因此我们的算法$\mathcal{A}$要能够自由的从$\mathcal{H}$中挑选方程，我们把$\mathcal{A}$挑选出的最好的$h$称为$g$ (final hypothesis)。因此这里就需要添加一个验证流程(Verification Flow)，这个流程使用历史数据来判断某个$h$够不够好。</p>
<p><img src="/imgs/is-learning-feasible/verification_flow.png" alt="enter image description here" title="verification_flow.png"></p>
<h2 id="不幸的状况_(Bad_Data)">不幸的状况 (Bad Data)</h2>
<p>&emsp;&emsp;前面说到，$\mathcal{A}$要能够自由的在$\mathcal{H}$中挑选它认为最适合的方程，因此这个最适合的方程就有可能是$\mathcal{H}$中的任何一个，有可能是$h_1$，有可能是$h_2$，$\mathcal{H}$中任意一个$h$都有可能成为$g$。但我们知道，我们的$\mathcal{D}$只是来自于总体的一个样本 (sample)，既然是sample，就一定会存在抽样误差。譬如你想知道一枚硬币抛出正面的概率是多少，于是你扔了5次，有一定的可能你连续扔了5个正面出来，这时候说抛出正面的概率是1，这样对吗？这当然是行不通的，因此你扔的这5次硬币，就是一个<strong>bad sample</strong>。凡是由于抽样误差所造成样本分布与总体分布相差很大的样本，我们都可以称之为<strong>bad sample</strong>。</p>
<p>&emsp;&emsp;learning同样会遇到bad sample的麻烦。比如实际上$h_1$是个很好的方程，本来能够成为$g$的，但是由于抽样误差，碰到了bad sample，造成$E_{in}(h_1)$很大，$\mathcal{A}$最终没有选择它。又比如$h_2$是个不好的方程，碰到了bad sample，碰巧$E_{in}(h_2)$又很小，导致$\mathcal{A}$错误得选择了它作为$g$。因此每个$h$都有可能遇上bad sample的烦恼。对于任意一个$h$来说，bad sample会造成他们的$| E_{in}(h) - E_{out}(h) | &gt; \epsilon$。</p>
<p>&emsp;&emsp;因此只要$\mathcal{H}$中任意个$h$遇上bad sample，我们的$\mathcal{A}$在挑选方程时就会遇到麻烦，我们的learning就有可能不太好。那么bad sample发生的概率有多大呢？</p>
<script type="math/tex; mode=display">
\begin{aligned}
\ & \mathbb{P}_{\mathcal{D}}[BAD\ \mathcal{D}] \\\
\ & = \mathbb{P}_{\mathcal{D}}[BAD\ \mathcal{D}\ for\ h_1\ or\ BAD\ \mathcal{D}\ for\ h_2\ or\ ...\ or\ BAD\ \mathcal{D}\ for\ h_M]\\\
\ & \leq \mathbb{P}_{\mathcal{D}}[BAD\ \mathcal{D}\ for\ h_1] + \mathbb{P}_{\mathcal{D}}[BAD\ \mathcal{D}\ for\ h_2]+...+\mathbb{P}_{\mathcal{D}}[BAD\ \mathcal{D}\ for\ h_M] \\\
\ & \leq 2exp(-2\epsilon ^2N) + \leq 2exp(-2\epsilon ^2N) + ... + \leq 2exp(-2\epsilon ^2N) \\\
\ & = 2Mexp(-2\epsilon ^2N)
\end{aligned}
</script>

<p>&emsp;&emsp;由此看出，learning得好不好，还与$\mathcal{H}$里面的方程数量$M$有关。当$M$是有限的时候，数据量越大，发生bad sample的可能性越低。同理如果$M$太大，我们也越容易遇到bad sample。</p>
<h2 id="总结_(Summary)">总结 (Summary)</h2>
<p>&emsp;&emsp;从概率论的角度出发，可以证明learning的确是可行的。因此，只有当$E_{in}(h)$和$E_{out}(h)$的判断很接近的时候，我们才能说learning是可行的。可行之余，倘若$E_{in}(h)$很大，这样的learning也没有太大意义，因为你的这个$h$在sample中表现不好，则他在out-of-sample中表现也不大可能会好。我们把$\mathcal{H}$中表现最好($E_{in}$最低)的那个方程选出来，记为$g$。当然如何定义“最好”，以及如何去寻找“最好”，则是后面的内容。</p>
]]></content>
    <category scheme="http://beader.me/tags/Machine-Learning/" term="Machine Learning"/>
    <category scheme="http://beader.me/tags/Hoeffding’s-Inequality/" term="Hoeffding’s Inequality"/>
  </entry>
  <entry>
    <title type="html"><![CDATA[R统计图表与好玩的字体]]></title>
    <link href="http://beader.me/2014/01/09/font-in-r-plot/"/>
    <id>http://beader.me/2014/01/09/font-in-r-plot/</id>
    <published>2014-01-09T09:30:00.000Z</published>
    <updated>2014-09-01T00:58:53.000Z</updated>
    <content type="html"><![CDATA[<p>&emsp;&emsp;本文参考至统计之都的<a href="http://cos.name/2014/01/showtext-interesting-fonts-and-graphs/" target="_blank" rel="external">showtext：字体，好玩的字体和好玩的图形</a>以及<a href="http://cos.name/cn/topic/147769" target="_blank" rel="external">R能不能绘制这种图形？？</a>。当中讨论了在R图表中显示中文以及其他自定义字体的方法。这两篇文章中使用到<code>showtext</code>这个包，但实际测试发现好像不使用<code>showtext</code>也能达到同样的效果。下面的R code我是在Mac下的RStudio下实验的，因此如果windows达不到同样的效果，可以参考上面这两篇文章。</p>
<p><a id="more"></a></p>
<h2 id="_"> </h2>
<p>&emsp;&emsp;正常情况下，R plot不能显示中文，所有中文会变成一个一个方块</p>
<figure class="highlight r"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">plot(cars, main = <span class="string">"中文"</span>)</div></pre></td></tr></table></figure>

<p><img src="/imgs/font-in-r-plot/unnamed-chunk-1.png" alt="" title="unnamed-chunk-1.png"></p>
<p>&emsp;&emsp;这是因为R无法找到正确的中文字体来显示。简单设置一下字体之后，发现中文标题可以正常显示了。这里”Kai”是系统自带的楷体字。</p>
<figure class="highlight r"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">plot(cars, main = <span class="string">"中文标题"</span>, family = <span class="string">"Kai"</span>)</div></pre></td></tr></table></figure>

<p><img src="/imgs/font-in-r-plot/unnamed-chunk-2.png" alt="" title="unnamed-chunk-2.png"></p>
<p>&emsp;&emsp;不仅仅是标题，我们往plot上增加的text也是可以设置中文字体的。</p>
<figure class="highlight r"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">set.seed(<span class="number">123</span>)</div><div class="line">plot(<span class="number">1</span>, xlim = c(-<span class="number">3</span>, <span class="number">3</span>), ylim = c(-<span class="number">3</span>, <span class="number">3</span>), type = <span class="string">"n"</span>)</div><div class="line">text(runif(<span class="number">7</span>, -<span class="number">3</span>, <span class="number">3</span>), runif(<span class="number">7</span>, -<span class="number">3</span>, <span class="number">3</span>), labels = c(<span class="string">"内"</span>, <span class="string">"容"</span>, <span class="string">"设"</span>, <span class="string">"置"</span>, </div><div class="line">    <span class="string">"为"</span>, <span class="string">"宋"</span>, <span class="string">"体"</span>), col = rgb(runif(<span class="number">7</span>), runif(<span class="number">7</span>), runif(<span class="number">7</span>)), cex = <span class="number">2</span>, </div><div class="line">    family = <span class="string">"Songti SC"</span>)</div><div class="line">title(<span class="string">"楷体的标题"</span>, family = <span class="string">"Kai"</span>)</div></pre></td></tr></table></figure>

<p><img src="/imgs/font-in-r-plot/unnamed-chunk-3.png" alt="" title="unnamed-chunk-3.png"></p>
<p>&emsp;&emsp;那么我们自己下载的字体可不可以呢？答案是可以的。如果不想安装字体，可以按照文章一开始参考的链接，使用<code>showtext</code>包来载入字体。或者跟我一样先安装字体，这样使用起来就和系统自带的字体一样了。<br>&emsp;&emsp;接下来我们来画一个不同教育程度下性别比例的横向柱状图。首先<a href="http://img.dafont.com/dl/?f=wm_people_1" target="_blank" rel="external">下载wmpeople1字体</a>。在这种字体下字母”p”将显示为男人，”u”将显示为女人。<br>&emsp;&emsp;下载并解压，会得到一个叫wmpeople1.TTF的字体文件。在Mac下直接双击该字体文件即可安装。</p>
<p><img src="/imgs/font-in-r-plot/install_font.png" alt="" title="install_font.png"></p>
<p>&emsp;&emsp;先来看看原始数据:  </p>
<table>
<thead>
<tr>
<th style="text-align:center">edu</th>
<th style="text-align:center">educode</th>
<th style="text-align:center">gender</th>
<th style="text-align:center">population</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">未上过学</td>
<td style="text-align:center">1</td>
<td style="text-align:center">m</td>
<td style="text-align:center">17464</td>
</tr>
<tr>
<td style="text-align:center">未上过学</td>
<td style="text-align:center">1</td>
<td style="text-align:center">f</td>
<td style="text-align:center">41268</td>
</tr>
<tr>
<td style="text-align:center">小  学</td>
<td style="text-align:center">2</td>
<td style="text-align:center">m</td>
<td style="text-align:center">139378</td>
</tr>
<tr>
<td style="text-align:center">小  学</td>
<td style="text-align:center">2</td>
<td style="text-align:center">f</td>
<td style="text-align:center">154854</td>
</tr>
<tr>
<td style="text-align:center">初  中</td>
<td style="text-align:center">3</td>
<td style="text-align:center">m</td>
<td style="text-align:center">236369</td>
</tr>
<tr>
<td style="text-align:center">初  中</td>
<td style="text-align:center">3</td>
<td style="text-align:center">f</td>
<td style="text-align:center">205537</td>
</tr>
<tr>
<td style="text-align:center">高  中</td>
<td style="text-align:center">4</td>
<td style="text-align:center">m</td>
<td style="text-align:center">94528</td>
</tr>
<tr>
<td style="text-align:center">高  中</td>
<td style="text-align:center">4</td>
<td style="text-align:center">f</td>
<td style="text-align:center">70521</td>
</tr>
<tr>
<td style="text-align:center">大专及以上</td>
<td style="text-align:center">5</td>
<td style="text-align:center">m</td>
<td style="text-align:center">57013</td>
</tr>
<tr>
<td style="text-align:center">大专及以上</td>
<td style="text-align:center">5</td>
<td style="text-align:center">f</td>
<td style="text-align:center">50334</td>
</tr>
</tbody>
</table>
<p>&emsp;&emsp;其中人口的单位为(千人)，之后会先除以10000转换为(千万人)。<br>&emsp;&emsp;接下来我们将使用ggplot2画图。</p>
<figure class="highlight r"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">library</span>(ggplot2)</div><div class="line"><span class="keyword">library</span>(plyr)</div><div class="line"></div><div class="line"><span class="comment"># 用来产生图形的模拟数据</span></div><div class="line">dat = read.csv(textConnection(<span class="string">'</span></div><div class="line">edu,educode,gender,population</div><div class="line">未上过学,1,m,17464</div><div class="line">未上过学,1,f,41268</div><div class="line">小  学,2,m,139378</div><div class="line">小  学,2,f,154854</div><div class="line">初  中,3,m,236369</div><div class="line">初  中,3,f,205537</div><div class="line">高  中,4,m,94528</div><div class="line">高  中,4,f,70521</div><div class="line">大专及以上,5,m,57013</div><div class="line">大专及以上,5,f,50334</div><div class="line">'));</div><div class="line">dat$int = round(dat$population / <span class="number">10000</span>);</div><div class="line"></div><div class="line"><span class="comment"># 利用plyr::ddply整理数据。我们需要得到整张图中每个小人所站的坐标，以及</span></div><div class="line"><span class="comment"># 该坐标小人的样式(男生or女生)</span></div><div class="line"></div><div class="line"><span class="comment"># 1.按照教育程度"educode"切分数据</span></div><div class="line"><span class="comment"># 2.获取对应教育程度下男生和女生的数量。并转换为横坐标的值</span></div><div class="line"><span class="comment"># 例如未上过学 educode=1当中，男生数为2，女生数为4。则我们应该产生6个坐标点，</span></div><div class="line"><span class="comment"># 这6个左边点的y轴均为1(educode)，x轴分别为1,2,3,4,5,6，其中x=1,2为男生，</span></div><div class="line"><span class="comment"># x = 3,4,5,6为女生</span></div><div class="line">gdat = ddply(dat, <span class="string">"educode"</span>, <span class="keyword">function</span>(d) {</div><div class="line">    male = d$int[d$gender == <span class="string">"m"</span>]</div><div class="line">    female = d$int[d$gender == <span class="string">"f"</span>]</div><div class="line">    data.frame(gender = c(rep(<span class="string">"m"</span>, male), rep(<span class="string">"f"</span>, female)),</div><div class="line">               x = <span class="number">1</span>:(male + female))</div><div class="line">});</div><div class="line"></div><div class="line"><span class="comment"># wmpeople1字体下p显示为男人，u显示为女人，因此把性别的字母做一下转换</span></div><div class="line">gdat$char = ifelse(gdat$gender == <span class="string">"m"</span>, <span class="string">"p"</span>, <span class="string">"u"</span>);</div><div class="line"></div><div class="line">theme_set(theme_grey(base_size = <span class="number">15</span>))</div><div class="line">ggplot(gdat, aes(x = x, y = educode)) + <span class="comment"># 设置横坐标为x，纵坐标为教育程度</span></div><div class="line">  geom_text(aes(label = char, colour = gender),</div><div class="line">            family = <span class="string">"wmpeople1"</span>, size = <span class="number">8</span>) + <span class="comment"># 设置文字字体、大小与颜色</span></div><div class="line">  scale_x_continuous(<span class="string">"\n人数（千万）"</span>) +</div><div class="line">  scale_y_discrete(<span class="string">"受教育程度\n"</span>,</div><div class="line">                   labels = unique(dat$edu[order(dat$educode)])) +</div><div class="line">  scale_colour_hue(guide = <span class="literal">FALSE</span>) +</div><div class="line">  ggtitle(<span class="string">"2012年人口统计数据\n"</span>) +</div><div class="line">  theme(text=element_text(family=<span class="string">'Songti SC'</span>)) <span class="comment"># 设置标题与坐标轴的字体</span></div></pre></td></tr></table></figure>

<p><img src="/imgs/font-in-r-plot/unnamed-chunk-4.png" alt="" title="unnamed-chunk-4.png"></p>
<p>&emsp;&emsp;标题，横纵坐标的label，以及内部的text都可以修改字体。那么坐标轴上的文字当然也是可以的。<br>&emsp;&emsp;我们来把教育程度改成<a href="http://img.dafont.com/dl/?f=memes_by_jopea302" target="_blank" rel="external">暴走字体</a>。和之前一样下载解压，得到一个Memes.ttf的字体文件，安装它。不过注意安装之后该字体的名称并不是叫Memes，如果像之前一样把字体设置为Memes会报错说找不到字体。</p>
<p><img src="/imgs/font-in-r-plot/adasdasdsdsd.png" alt="" title="adasdasdsdsd.png"></p>
<p>&emsp;&emsp;安装的时候我们可以看到该字体真实的名字叫做adasdasdsdsd(看名字就够暴走了。。。)</p>
<p>&emsp;&emsp;我们这里试着把纵坐标替换为暴走体，因为暴走字体只支持英文大小写以及数字，因此先把这几个教育程度替换为英文字母。</p>
<figure class="highlight r"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># 把edu(中文)转为educhar(字母)</span></div><div class="line">educhar = letters[dat$educode]</div><div class="line"></div><div class="line">ggplot(gdat, aes(x = x, y = educode)) +</div><div class="line">    geom_text(aes(label = char, colour = gender),</div><div class="line">              family = <span class="string">"wmpeople1"</span>, size = <span class="number">7</span>) +</div><div class="line">    scale_x_continuous(<span class="string">"\n人数（千万）"</span>) +</div><div class="line">    scale_y_discrete(<span class="string">"受教育程度\n"</span>,</div><div class="line">        labels = unique(educhar[order(dat$educode)])) +  <span class="comment"># 注意这一行与之前的不同，把dat$edu换为educhar</span></div><div class="line">    scale_colour_hue(guide = <span class="literal">FALSE</span>) +</div><div class="line">    ggtitle(<span class="string">"2012年统计数据\n"</span>) + </div><div class="line">    theme(axis.text.y=element_text(size=rel(<span class="number">4</span>), family=<span class="string">"adasdasdsdsd"</span>),   <span class="comment"># 纵坐标为暴走体</span></div><div class="line">          text=element_text(family=<span class="string">'Songti SC'</span>))  <span class="comment"># 标题以及坐标名称为宋体</span></div></pre></td></tr></table></figure>

<p><img src="/imgs/font-in-r-plot/unnamed-chunk-5.png" alt="" title="unnamed-chunk-5.png"></p>
]]></content>
    <category scheme="http://beader.me/tags/R/" term="R"/>
    <category scheme="http://beader.me/tags/统计图表/" term="统计图表"/>
  </entry>
  <entry>
    <title type="html"><![CDATA[R语言求函数的偏导数]]></title>
    <link href="http://beader.me/2014/01/05/r-partial-derivative/"/>
    <id>http://beader.me/2014/01/05/r-partial-derivative/</id>
    <published>2014-01-05T09:48:49.000Z</published>
    <updated>2014-01-15T06:07:45.000Z</updated>
    <content type="html"><![CDATA[<p>&emsp;&emsp;在Machine Learning的课上，老师讲到用gradient decent的方法解logistic regression中cost function的最小值。这当中就要涉及到对cost function的求偏导数。其实在R当中可以很方便的做到这一点。</p>
<p>&emsp;&emsp;R语言中可以使用<code>D()</code>来求一元函数的导数，用<code>deriv()</code>来求多元函数的偏导数。这两个function都在<code>package:stats</code>中，会在R启动时默认加载。</p>
<p>&emsp;&emsp;下面以一个多元函数作为demo，希望通过gradient decent的方法求最小值:</p>
<p>$$E(u,v) = e^u + e^{2v} + e^{uv} + u^2 - 2uv + 2v^2 - 3u - 2v$$</p>
<a id="more"></a>

<figure class="highlight r"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># 创建一个expression对象</span></div><div class="line">Euv = expression(exp(u) + exp(<span class="number">2</span> * v) + exp(u * v) + u^<span class="number">2</span> - <span class="number">2</span> * u * v + <span class="number">2</span> * v^<span class="number">2</span> - </div><div class="line">    <span class="number">3</span> * u - <span class="number">2</span> * v)</div><div class="line"><span class="comment"># Euv 在u以及v方向上的偏导数，设定参数func=T，这样d_Euv是一个function，</span></div><div class="line"><span class="comment"># 可以通过d_Euv(u,v)去求值，否则，d_Euv是一个expression，要使用eval(d_Euv)去求值。</span></div><div class="line"><span class="comment"># 详细参考 ?deriv</span></div><div class="line">d_Euv = deriv(Euv, c(<span class="string">"u"</span>, <span class="string">"v"</span>), func = <span class="literal">T</span>)</div><div class="line">d_Euv</div></pre></td></tr></table></figure>



<figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div></pre></td><td class="code"><pre><div class="line"><span class="preprocessor">## function (u, v) </span></div><div class="line"><span class="preprocessor">## {</span></div><div class="line"><span class="preprocessor">##     .expr1 &lt;- exp(u)</span></div><div class="line"><span class="preprocessor">##     .expr2 &lt;- 2 * v</span></div><div class="line"><span class="preprocessor">##     .expr3 &lt;- exp(.expr2)</span></div><div class="line"><span class="preprocessor">##     .expr6 &lt;- exp(u * v)</span></div><div class="line"><span class="preprocessor">##     .expr10 &lt;- 2 * u</span></div><div class="line"><span class="preprocessor">##     .value &lt;- .expr1 + .expr3 + .expr6 + u^2 - .expr10 * v + </span></div><div class="line"><span class="preprocessor">##         2 * v^2 - 3 * u - .expr2</span></div><div class="line"><span class="preprocessor">##     .grad &lt;- array(0, c(length(.value), 2L), list(NULL, c("u", </span></div><div class="line"><span class="preprocessor">##         "v")))</span></div><div class="line"><span class="preprocessor">##     .grad[, "u"] &lt;- .expr1 + .expr6 * v + .expr10 - .expr2 - </span></div><div class="line"><span class="preprocessor">##         3</span></div><div class="line"><span class="preprocessor">##     .grad[, "v"] &lt;- .expr3 * 2 + .expr6 * u - .expr10 + 2 * .expr2 - </span></div><div class="line"><span class="preprocessor">##         2</span></div><div class="line"><span class="preprocessor">##     attr(.value, "gradient") &lt;- .grad</span></div><div class="line"><span class="preprocessor">##     .value</span></div><div class="line"><span class="preprocessor">## }</span></div></pre></td></tr></table></figure>

<p>&emsp;&emsp;注意看d_Euv这个function里面的内容，<strong>d_Euv</strong>返回的<code>.value</code>是原函数<strong>Euv</strong>在(u,v)下的值，而不是偏导数。该function的末尾把偏导数作为attribute附加到这个value上。因此我们可以通过下面的方法把这个偏导数再提取出来。</p>
<figure class="highlight r"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">derivatives = <span class="keyword">function</span>(u, v) attributes(d_Euv(u, v))$gradient</div><div class="line"><span class="comment"># 求(0,0)点的偏导数</span></div><div class="line">derivatives(<span class="number">0</span>, <span class="number">0</span>)</div></pre></td></tr></table></figure>



<figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="preprocessor">##       u v</span></div><div class="line"><span class="preprocessor">## [1,] -2 0</span></div></pre></td></tr></table></figure>

<p>&emsp;&emsp;利用梯度下降法(gradient decent)解Euv最小值。下面只做示范用，只迭代5次，因此得到的值不一定是真正的最小值。利用<strong>fixed learning rate $\eta = 0.01$</strong>从$(u_0,v_0) = (0,0)$开始，对(u,v)进行更新:</p>
<script type="math/tex;mode=display">
(u_{t+1},v_{t+1}) = (u_t,v_t) - \eta \bigtriangledown E(u_t,v_t)
</script>

<figure class="highlight r"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div></pre></td><td class="code"><pre><div class="line">uv.init = as.matrix(c(u = <span class="number">0</span>, v = <span class="number">0</span>)) <span class="comment">#把(u,v)的初始值设定为(0,0)</span></div><div class="line">eta = <span class="number">0.01</span></div><div class="line">t = <span class="number">5</span>  <span class="comment"># 迭代次数</span></div><div class="line"></div><div class="line"><span class="comment"># gradient decent</span></div><div class="line">uv.t = uv.init</div><div class="line"><span class="keyword">for</span> (i <span class="keyword">in</span> <span class="number">1</span>:t) {</div><div class="line">    <span class="comment"># derivatives得到的是一个横向的矩阵，先转置成竖向的向量</span></div><div class="line">    grad = t(derivatives(uv.t[<span class="number">1</span>], uv.t[<span class="number">2</span>]))</div><div class="line">    uv.t = uv.t - eta * grad</div><div class="line">}</div><div class="line"></div><div class="line"><span class="comment"># 经过5次梯度下降后的 (u,v)</span></div><div class="line">print(uv.t)</div></pre></td></tr></table></figure>



<figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"><span class="preprocessor">##       [,1]</span></div><div class="line"><span class="preprocessor">## u 0.094140</span></div><div class="line"><span class="preprocessor">## v 0.001789</span></div></pre></td></tr></table></figure>

<p>&emsp;&emsp;函数Euv在(u,v)上的值可以利用以下两种方式去解</p>
<figure class="highlight r"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># 1. 前面说到，d_Euv(u,v)返回值其实是函数Euv的值</span></div><div class="line">print(d_Euv(uv.t[<span class="number">1</span>], uv.t[<span class="number">2</span>]))</div></pre></td></tr></table></figure>



<figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"><span class="preprocessor">## [1] 2.825</span></div><div class="line"><span class="preprocessor">## attr(,"gradient")</span></div><div class="line"><span class="preprocessor">##           u       v</span></div><div class="line"><span class="preprocessor">## [1,] -1.715 -0.0798</span></div></pre></td></tr></table></figure>



<figure class="highlight r"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># 2. 使用eval()</span></div><div class="line">u = uv.t[<span class="number">1</span>]</div><div class="line">v = uv.t[<span class="number">2</span>]</div><div class="line">print(eval(Euv))</div></pre></td></tr></table></figure>



<figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line"><span class="preprocessor">## [1] 2.825</span></div></pre></td></tr></table></figure>

]]></content>
    <category scheme="http://beader.me/tags/R/" term="R"/>
    <category scheme="http://beader.me/tags/偏导数-(partial-derivative)/" term="偏导数 (partial derivative)"/>
    <category scheme="http://beader.me/tags/梯度下降-(gradient-decent)/" term="梯度下降 (gradient decent)"/>
  </entry>
  <entry>
    <title type="html"><![CDATA[Perceptron Learning Algorithm (PLA)]]></title>
    <link href="http://beader.me/2013/12/21/perceptron-learning-algorithm/"/>
    <id>http://beader.me/2013/12/21/perceptron-learning-algorithm/</id>
    <published>2013-12-21T02:00:00.000Z</published>
    <updated>2014-09-01T01:00:33.000Z</updated>
    <content type="html"><![CDATA[<h3 id="Perceptron_是什么？">Perceptron 是什么？</h3>
<hr>
<p>&emsp;&emsp;<a href="http://zh.wikipedia.org/wiki/%E6%84%9F%E7%9F%A5%E5%99%A8" target="_blank" rel="external">Perceptron - 感知机</a>，它能够根据每笔资料的特征，把资料判断为不同的类别。令$h(x)$是一个perceptron，你给我一个$x$($x$是一个特征向量)，把$x$输入$h(x)$，它就会输出这个$x$的类别，譬如在信用违约风险预测当中，输出就可能是这个人会违约，或者不会违约。本质上讲，perceptron是一种<strong><a href="http://zh.wikipedia.org/wiki/%E7%BA%BF%E6%80%A7%E5%88%86%E7%B1%BB%E5%99%A8" target="_blank" rel="external">二元线性分类器</a></strong>，它通过对特征向量的加权求和，并把这个”和”与事先设定的门槛值(threshold)做比较，高于门槛值的输出1，低于门槛值的输出-1。  </p>
<script type="math/tex; mode=display">
f(x)=
\left\{
\begin{matrix}
1 & if\ w^T\cdot x > threshold\\
-1 & else
\end{matrix}\right.
</script>

<p><a id="more"></a><br>&emsp;&emsp;更加紧凑一些，我们可以把它写成<br>$$<br>h(x) = sign(\sum _{i=1}^{d}w_ix_i+b)<br>$$<br>&emsp;&emsp;其中$d$表示维度数，$x$是$d$维空间中的一个点,&emsp;$x=(x_1,x_2,…,x_d)$;&emsp;$sign()$输出运算结果的符号，大于0的输出+1，其余输出-1。  </p>
<p>&emsp;&emsp;细心一点可以看出，其实$\sum _{i=1}^{d}w_ix_i+b =0$在1维空间中代表一个点，在2维空间中代表一条直线，在3维空间中代表一个平面。</p>
<p>&emsp;&emsp;以2维空间为例:</p>
<p>&emsp;&emsp;对于所有满足$\sum_{i=1}^{d}w_ix_i+b&lt;0$的$x$，将落在直线一边的区域中(下图中的蓝色).</p>
<p>&emsp;&emsp;对于所有满足$\sum _{i=1}^{d}w_ix_i+b &gt; 0$的$x$，将落在直线的另一边(下图中的红色)。</p>
<p><img src="/imgs/perceptron-learning-algorithm/unnamed-chunk-1.png" alt="" title="perceptron.png"></p>
<p>&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;图1 在2维空间中perceptron是一条直线</p>
<p>&emsp;&emsp;为了方便后面计算，我们常常衍生出一个维度出来，使得$b=w_0x_0,x_0$这个维度恒等于1。则perceptron可以用矩阵的形式表示出来:</p>
<p>$$h(x)=sign(\sum _{i=0}^{d}w_ix_i)=w^Tx$$</p>
<p>&emsp;&emsp;注意: 这里要搞清楚下标的含义，上文中的$x_i$、$w_i$指$x$向量、$w$向量的第$i$个维度。因为后文都是用矩阵乘法，因此下标不再表示维度，譬如$x_n$表示某笔资料，它是一个向量，而$w_t$指第$t$次更新之后的$w$，它也是一个向量。</p>
<h3 id="Perceptron_Learning在做什么？">Perceptron Learning在做什么？</h3>
<hr>
<p>&emsp;&emsp;Perceptron Learning Algorithm的目的是要找到一个perceptron，能把正确地把不同类别的点区分开来。<br>&emsp;&emsp;在二维平面上，任何找一条直线都可以用来做perceptron，只不过有些perceptron分类能力比较好(分错的少)，有些perceptron分类能力比较差(分错的多)。</p>
<p><img src="/imgs/perceptron-learning-algorithm/unnamed-chunk-2.png" alt="" title="different perceptrons.png"></p>
<p>&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;图2 2维空间中的两个不同的perceptron</p>
<p>&emsp;&emsp;上图中是二维平面上的两个perceptron，图中圈圈代表+1的点，叉叉代表-1的点。左边的perceptron把两个叉叉错分到圈圈当中，而右边的则很完美地把圈圈和叉叉区分开来。在二维平面中存在无数个可能的perceptron，而perceptron learning的目的是找到一个好的perceptron。</p>
<p>&emsp;&emsp;假设给我们的数据是“线性可分”的，即至少存在一个perceptron，它很厉害，可以做到百分百的正确率(如图2-b)，对于任意的$(y_n,x_n)$，有$h(x_n)=y_n$。我们把这个完美的perceptron记为</p>
<p>$$h(x)=sign(w_f^Tx)$$</p>
<p>&emsp;&emsp;则Perceptron Learning要做的是，在“线性可分”的前提下，由一个初始的Perceptron $h(x)$开始，通过不断的learning，不断的调整$h(x)$的参数$w$，使他最终成为一个完美的perceptron。</p>
<h3 id="Perceptron_Learning_Algorithm_(PLA)_-_“知错就改”演算法">Perceptron Learning Algorithm (PLA) - “知错就改”演算法</h3>
<hr>
<p>&emsp;&emsp;前面说到Perceptron Learning的目的，那么既然要learning就一定有一个learning的方法(譬如随机猜就是一种方法，但它不一定是个好方法)，这个方法能够指导我们该如何去慢慢调整$w$，使$h(x)$越来越接近我们心目中完美的perceptron  $sign(w_f^Tx)$。</p>
<p>&emsp;&emsp;PLA的方法如下：</p>
<p>&emsp;&emsp;For t = 0,1,…</p>
<ol>
<li><p>找到<script type="math/tex">w_t</script>产生的一个错误点</p>
<script type="math/tex">sign(w_t^Tx_{n(t)}) \neq y_{n(t)}</script>  

<p>(注意这里x的下标不是值维度，而是数据点的编号。<script type="math/tex">y_{n(t)}</script>指第t次更新后的一个分类错误点)</p>
</li>
<li><p>用下面的方法更正这个错误:  </p>
<script type="math/tex">w_{t+1} \leftarrow w_t + y_{n(t)}x_{n(t)}</script>  

</li>
</ol>
<p>&emsp;&emsp;…直到找不到错误点，返回最后一次迭代的$w$</p>
<p>&emsp;&emsp;以下用图片展示迭代的过程，图片截至台湾大学<a href="https://www.coursera.org/instructor/htlin" target="_blank" rel="external">林轩田</a>老师<a href="https://www.coursera.org/course/ntumlone" target="_blank" rel="external">Machine Learning Foundation</a>的<a href="http://www.csie.ntu.edu.tw/~htlin/course/ml13fall/doc/02_handout.pdf" target="_blank" rel="external">讲义</a></p>
<p><img src="/imgs/perceptron-learning-algorithm/pla_demo.png" alt="" title="pla_demo.png"></p>
<p>&emsp;&emsp;图3 PLA 知“错”就“改”的过程</p>
<p>&emsp;&emsp;从图中可以看出$w$确实在PLA的指导下，慢慢接近心目中的$w_f$。</p>
<p>&emsp;&emsp;我们知道在数据线性可分的前提下，我们心目中有个完美的$w_f$，它能够完美的把圈圈和叉叉区分开来。那么如何证明PLA能够使$w$不断接近$w_f$呢？  </p>
<p>&emsp;&emsp;这里就要用到夹角余弦的公式，如果<script type="math/tex">w_t</script>更新之后的<script type="math/tex">w_{t+1}</script>与<script type="math/tex">w_f</script>之间的夹角余弦变大(夹角变小)了，则我们可以说PLA是有效的。首先我们先来看pla的几个性质:</p>
<hr>
<ol>
<li><p>因为$w_f$是完美的，因此对于任意的$x_n$，$w_f$都能把它归入正确的类别:</p>
<p><script type="math/tex">y_{n(t)}w_f^Tx_{n(t)} \geq min\ {y_nw_f^Tx_n} \gt 0</script><br><br></p>
</li>
<li><script type="math/tex">w_f^Tw_t</script>在更新了任何一个错误点<script type="math/tex">(x_{n(t)},y_{n(t)})</script>后都会增大:  

<script type="math/tex;mode=display">
\begin{equation}
\begin{split}
w_f^Tw_{t+1} &=    w_f^T(w_t+y_{n(t)}x_{n(t)}) \\\
             &\geq w_f^Tw_t + min\ {y_nw_f^Tx_n} \\\
             &\gt  w_f^Tw_t
\end{split}
\end{equation}
</script>
</li>
<li><p>看起来<script type="math/tex">w_{t+1}</script>是更接近<script type="math/tex">w_f</script>了，但他们内积的增大并不能表示他们夹角的变小，也有可能是因为<script type="math/tex">w_{t+1}</script>长度<script type="math/tex">||w_{t+1}||</script>增大了。但是<script type="math/tex">||w_{t+1}||</script>的增加是有限的:</p>
<script type="math/tex;mode=display">
\begin{equation}
\begin{split}
||w_{t+1}||^2 &=    ||w_t + y_{n(t)}x_{n(t)}||^2 \\\
              &=    ||w_t||^2 + 2y_{n(t)}w_t^Tx_{n(t)} + ||y_{n(t)}x_{n(t)}||^2\\\
              &\leq ||w_t||^2 + 0 + ||y_{n(t)}x_{n(t)}||^2 \\\
              &\leq ||w_t||^2 + max\ {||x_n||^2}
\end{split}
\end{equation}
</script>

</li>
</ol>
<hr>
<p>&emsp;&emsp;根据上面的性质，我们可以来求夹角余弦。从$w_0=0$(初始的向量)开始，经过T次错误更正，变为$w_T$，那么就有:</p>
  <script type="math/tex;mode=display">
  \begin{equation}
  \begin{split}
  w_f^Tw_T &\geq w_f^Tw_{T-1} + min\ {y_nw_f^Tx_n} \\\
           &\geq ... \\\
           &\geq w_f^Tw_0 + T\cdot min\ {y_nw_f^Tx_n} = T\cdot min\ {y_nw_f^Tx_n}
  \end{split}
  \end{equation}
 </script>

  <script type="math/tex;mode=display">
  \begin{equation}
  \begin{split}
  ||w_T||^2 &\leq ||w_{T-1}||^2 + max\ {||x_n||^2} \\\
            &\leq ... \\\
            &\leq ||w_0||^2 + T\cdot max\ {||x_n||^2} = T\cdot max\ {||x_n||^2}
  \end{split}
  \end{equation}
 </script>

<p>&emsp;&emsp;再来求$w_T$与$w_f$的夹角余弦:<br> $$<br>  \begin{equation}<br>  \begin{split}<br>  \frac{w_f^Tw_T}{||w_f||||w_T||} &amp;\geq \frac{T\cdot min\ y_nw_f^Tx_n}{||w_f||||w_T||} \<br>                    &amp;\geq \frac{T\cdot min\ y_nw_f^Tx_n}{||w_f||\cdot \sqrt{T}\cdot max\ {||x_n||^2}}<br>                    = \frac {\sqrt{T}\rho}{R}<br>  \end{split}<br>  \end{equation}<br> $$</p>
<p>&emsp;&emsp;其中$\rho = min\ y_n\frac {w_f^T}{||w_f||}x_n$与$R^2 = max\ {||x_n||^2}$都是大于0的常数。由于夹角余弦是小于等于1的，因此有:</p>
<p>$$1 \geq \frac{w_f^Tw_T}{||w_f||||w_T||} \geq \frac {\sqrt{T}\rho}{R}$$</p>
<p>&emsp;&emsp;上面的不等式告诉我们两点:</p>
<ol>
<li><p>PLA能够帮助$w_T$进步，因为$w_T$与$w_f$的夹角余弦随着更新错误点的次数T的增加而增加，$w_T$越来越接近$w_f$。</p>
</li>
<li><p>PLA会停止(halt)，因为$T \leq R^2/\rho ^2$，即当数据是线性可分时，经过有限次数的迭代，一定能找到一个能够把数据完美区分开的perceptron。</p>
</li>
</ol>
<h3 id="Pocket_Step_-_把最好的$w$放口袋">Pocket Step - 把最好的$w$放口袋</h3>
<hr>
<p>&emsp;&emsp;有时我们拿到的数据数量庞大，或是不是线性可分的，这个时候用PLA将消耗大量的时间，或是根本无法停止，这个时候我们可以使用一种委曲求全的办法，在PLA中加入pocket step。这个pocket是做什么用的呢？这个pocket会使用PLA在每次迭代中产生的$w$，带进原始数据，去计算分类错误率，并记录最好的那个$w$，譬如我们设定让PLA迭代N次就停止，则pocket返回这N次迭代中出现的最好的$w$。当N足够大的时候，pocket总能返回还不错的结果。</p>
]]></content>
    <category scheme="http://beader.me/tags/感知机-(perceptron)/" term="感知机 (perceptron)"/>
    <category scheme="http://beader.me/tags/pla/" term="pla"/>
  </entry>
  <entry>
    <title type="html"><![CDATA[AUC与ROC - 衡量分类器的好坏]]></title>
    <link href="http://beader.me/2013/12/15/auc-roc/"/>
    <id>http://beader.me/2013/12/15/auc-roc/</id>
    <published>2013-12-15T05:28:40.000Z</published>
    <updated>2014-09-01T01:02:40.000Z</updated>
    <content type="html"><![CDATA[<h3 id="二元分类器">二元分类器</h3>
<hr>
<p>&emsp;&emsp;二元分类器是指要输出(预测)的结果只有两种类别的模型。例如预测阳性/阴性，有病/没病，在银行信用评分模型中，也用来预测用户是否会违约，等等。</p>
<p>&emsp;&emsp;既然是一种预测模型，则实际情况一定是有些结果猜对了，有些结果猜错了。因为二元分类器的预测结果有两种类别(以下以阴/阳为例)，对应其真实值，则会有以下四种情形:</p>
<pre><code><span class="bullet">1. </span>预测为阳性，真实值为阴性 (伪阳性)
<span class="bullet">2. </span>预测为阴性，真实值为阳性 (伪阴性)
<span class="bullet">3. </span>预测为阴性，真实值真的为阴性 (真阴性)
<span class="bullet">4. </span>预测为阴性，真实值真的为阴性 (真阴性)
</code></pre><p><img src="/imgs/auc-roc/9686a1f19149fe16eb4b6b383904d086.png" alt="" title="9686a1f19149fe16eb4b6b383904d086.png"><br>图1.confusion matrix (混乱矩阵)</p>
<a id="more"></a>



<h3 id="ROC空间">ROC空间</h3>
<hr>
<p>&emsp;&emsp;在信号检测理论中，接收者操作特征曲线（receiver operating characteristic curve，或者叫<a href="http://zh.wikipedia.org/wiki/ROC%E6%9B%B2%E7%BA%BF" target="_blank" rel="external">ROC曲线</a>）是一种座标图式的分析工具。<br>&emsp;&emsp;要了解ROC曲线，先要了解一下ROC空间，ROC空间是一个以伪阳性率(FPR, false positive rate)为X轴，真阳性率(TPR, true positive rate)为Y轴的二维坐标系所代表平面。</p>
<ul>
<li>TPR: 真阳性率，所有阳性样本中(TP+FN)，被分类器正确判断为阳的比例。<br>  TPR = TP / (TP + FN) = TP / 所有真实值为阳性的样本个数</li>
<li>FPR: 伪阳性率，所有阴性样本中(FP+TN)，被分类器错误判断为阳的比例。<br>  FPR = FP / (FP + TN) = FP / 所有真实值为阴性的样本个数</li>
</ul>
<p>&emsp;&emsp;我们想象这样一种场景，接触阳性样本可以给我们带来“收益”，接触阴性样本则会给我们造成”成本”。<br>并且如果我们接触样本中所有的阳性样本，我们的收益是1，接触样本中的所有阴性样本，我们的成本也是1。<br>如果不接触样本，则既不产生收益也不产生成本。<br>&emsp;&emsp;自然的，如果不使用分类器，接触所有样本，则总的效益为1-1=0。现在让我们利用分类器来决定是否接触样本，分类器预测为阳，我们就去接触样本，分类器预测为阴，我们就不去接触。因为不接触样本不会产生收益或是成本，因此我们只需要看分类器预测为阳的样本。预测为阳的样本中，TP将产生 TPR 的收益， FP将产生FPR的成本。<br>&emsp;&emsp;那么一个分类器的分类效果就对应ROC空间里的一个点:<br><img src="/imgs/auc-roc/1a02adedd70816dcd49461354390aaed.png" alt="" title="1a02adedd70816dcd49461354390aaed.png"><br>&emsp;&emsp;图2.ROC空间</p>
<p>&emsp;&emsp;A,B,C三个点可以分别代表三个不同的分类器对同样的样本做预测的结果。<br>最好的方法是A，因为他的收益大于成本(TPR &gt; FPR)，最差的是C(TPR &lt; FPR)。中等的是B，相当于随机分类器。<br>这里有趣的一点是若把C以(0.5, 0.5)为中点作一个镜像，得到C’， C’的效果比A要来的好。C’相当于一个做与C预测结果完全相反的分类器。<br>&emsp;&emsp;实际的应用当中，分类器还会给出它预测某个样本为阳的概率，并且有一个事先给定的门槛值(threshold)，概率高于threshold的就预测为阳性，低于threshold的就预测为阴性。假设以下是某个分类器对id为1-10的客户的分类结果:</p>
<p>&emsp;&emsp;表1.分类器预测结果<br><img src="/imgs/auc-roc/95ccbafd95b1ef894b2711d3698d5187.png" alt="" title="95ccbafd95b1ef894b2711d3698d5187.png">  </p>
<p>&emsp;&emsp;其中probability of 1为分类器判断该样本为阳性的概率，true class为该样本的真实情况。<br>&emsp;&emsp;如果我们把threshold定位0.5，即去接触id为1~8的客户。此时  </p>
<pre><code><span class="constant">TPR</span> = TP / 所有真实值为阳性的样本个数 = <span class="number">6</span> / <span class="number">6</span> = <span class="number">1</span>
<span class="constant">FPR</span> = FP / 所有真实值为阴性的样本个数 = <span class="number">2</span> / <span class="number">4</span> = <span class="number">0.6</span>
</code></pre><p>&emsp;&emsp;同理，如果我们把threshold定位0.8，即去接触id为1~5的客户。此时  </p>
<pre><code><span class="constant">TPR</span> = TP / 所有真实值为阳性的样本个数 = <span class="number">4</span> / <span class="number">6</span> = <span class="number">0.67</span>
<span class="constant">FPR</span> = FP / 所有真实值为阴性的样本个数 = <span class="number">1</span> / <span class="number">4</span> = <span class="number">0.25</span>
</code></pre><p>&emsp;&emsp;这两个threshold分别对应ROC空间中的两个点A、B  </p>
<p><img src="/imgs/auc-roc/f3aac8b8603adb924363e766992df3cd.png" alt="" title="f3aac8b8603adb924363e766992df3cd.png"><br>&emsp;&emsp;图3.不同的threshold对应ROC空间中不同的点  </p>
<h3 id="ROC曲线">ROC曲线</h3>
<hr>
<p>&emsp;&emsp;上面的例子当中，共有10笔预测数据，则一共有11种threshold的设定方法，每一个threshold对应ROC空间中的一个点，把这些点连接起来，就成了ROC曲线。<br><img src="/imgs/auc-roc/92175e2de4a480e52938a836994e823c.png" alt="" title="92175e2de4a480e52938a836994e823c.png"><br>&emsp;&emsp;图4.ROC曲线  </p>
<p>&emsp;&emsp;这里因为数据量太少，所以曲线是一折一折的，数据量大的时候，看上去才像”曲线”。</p>
<h3 id="AUC_(Area_under_the_Curve_of_ROC)_曲线下面积">AUC (Area under the Curve of ROC) 曲线下面积</h3>
<hr>
<p>以下直接搬维基百科:</p>
<ul>
<li><p>因为是在1x1的方格里求面积，AUC必在0~1之间。</p>
</li>
<li><p>假设threshold以上是阳性，以下是阴性；</p>
</li>
<li><p>若随机抽取一个阳性样本和一个阴性样本，分类器正确判断阳性样本的值高于阴性样本之机率。(即前文当中把C做一个镜像变为C’)</p>
</li>
<li><p>简单说：AUC值越大的分类器，正确率越高。  </p>
</li>
</ul>
<p>&emsp;&emsp;从AUC判断分类器（预测模型）优劣的标准：</p>
<ul>
<li><p>AUC = 1，是完美分类器，采用这个预测模型时，不管设定什么阈值都能得出完美预测。绝大多数预测的场合，不存在完美分类器。</p>
</li>
<li><p>0.5 &lt; AUC &lt; 1，优于随机猜测。这个分类器（模型）妥善设定阈值的话，能有预测价值。</p>
</li>
<li><p>AUC = 0.5，跟随机猜测一样（例：丢铜板），模型没有预测价值。</p>
</li>
<li><p>AUC &lt; 0.5，比随机猜测还差；但只要总是反预测而行，就优于随机猜测，因此不存在AUC &lt; 0.5的情况。</p>
</li>
</ul>
<p><img src="/imgs/auc-roc/f03add592a75ef5b5e7346a5209b0cb8.png" alt="" title="f03add592a75ef5b5e7346a5209b0cb8.png"><br>&emsp;&emsp;图5.用AUC来衡量不同分类器的分类能力(更准确的说是排序能力)</p>
<h3 id="总结">总结</h3>
<hr>
<p>&emsp;&emsp;一个分类模型的分类结果的好坏取决于以下两个部分：</p>
<ol>
<li>分类模型的排序能力(能否把概率高的排前面，概率低的排后面)  </li>
<li>threshold的选择</li>
</ol>
<p>&emsp;&emsp;使用AUC来衡量分类模型的好坏，可以忽略由于threshold的选择所带来的影响，因为实际应用中，这个threshold常常由先验概率或是人为决定的。</p>
<h3 id="补充：Gini_coefficient">补充：Gini coefficient</h3>
<hr>
<p>在用SAS或者其他一些统计分析软件，用来评测分类器分类效果时，常常会看到一个叫做gini coefficient的东西，那么这个gini coefficient又是什么呢？<br>gini系数通常被用来判断收入分配公平程度，具体请参阅<a href="http://zh.wikipedia.org/wiki/%E5%9F%BA%E5%B0%BC%E7%B3%BB%E6%95%B0" target="_blank" rel="external">wikipedia-基尼系数</a>。<br><img src="/imgs/auc-roc/1a9a293ac6c97475ebb337fb32081a4d.png" alt="" title="1a9a293ac6c97475ebb337fb32081a4d.png"><br>&emsp;&emsp;图6.洛伦茨曲线与基尼系数  </p>
<p>&emsp;&emsp;Gini coefficient 是指绝对公平线(line of equality)和洛伦茨曲线(Lorenz Curve)围成的面积与绝对公平线以下面积的比例，即gini coefficient = A面积 / (A面积+B面积) 。 </p>
<p>&emsp;&emsp;用在评判分类模型的预测效力时，是指ROC曲线曲线和中线围成的面积与中线之上面积的比例。</p>
<p><img src="/imgs/auc-roc/074c46dccea3031e5ce8fcbb67453cd4.png" alt="" title="074c46dccea3031e5ce8fcbb67453cd4.png"><br>&emsp;&emsp;图7.Gini coefficient与AUC</p>
<p>因此Gini coefficient与AUC可以互相转换：  </p>
<pre><code>    gini = <span class="literal">A</span> / (<span class="literal">A</span> + B) = (AUC - C) / (<span class="literal">A</span> + B) = (AUC -<span class="number">0.5</span>) / <span class="number">0.5</span> = <span class="number">2</span>*AUC - <span class="number">1</span>
</code></pre>]]></content>
    <category scheme="http://beader.me/tags/auc/" term="auc"/>
    <category scheme="http://beader.me/tags/roc/" term="roc"/>
    <category scheme="http://beader.me/tags/gini-coefficient/" term="gini coefficient"/>
    <category scheme="http://beader.me/tags/分类器/" term="分类器"/>
  </entry>
  <entry>
    <title type="html"><![CDATA[利用智能手机加速度传感器与三轴陀螺仪预测用户活动状态]]></title>
    <link href="http://beader.me/2013/12/07/har_random_forest/"/>
    <id>http://beader.me/2013/12/07/har_random_forest/</id>
    <published>2013-12-07T11:08:49.000Z</published>
    <updated>2014-09-01T01:04:50.000Z</updated>
    <content type="html"><![CDATA[<p>本次实验是coursera上的一门课<a href="https://www.coursera.org/course/dataanalysis" target="_blank" rel="external">Data Analysis</a>的一次小作业。将使用智能手机加速度传感器与三轴陀螺仪采集到的数据，来预测用户当前的活动(walking, walking_upstairs, walking_downstairs, sitting, standing, laying)。所用的数据可以从课程提供的链接下载，<a href="https://spark-public.s3.amazonaws.com/dataanalysis/samsungData.rda" target="_blank" rel="external">https://spark-public.s3.amazonaws.com/dataanalysis/samsungData.rda</a> 。.rda为R专用的数据文件，在R中直接使用<code>load()</code>即可将其载入，比较方便。除此之外，也可到<a href="http://archive.ics.uci.edu/ml/datasets/Human+Activity+Recognition+Using+Smartphones" target="_blank" rel="external">UCI Machine Learning Repository</a>下载原始文件，当中包含dataset的中各个变量以及衍生变量的简介，数据格式为纯文本。</p>
<p><a id="more"></a></p>
<h3 id="数据整理">数据整理</h3>
<figure class="highlight r"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">getwd()</div></pre></td></tr></table></figure>

<p>先确定一下当前的工作路径。</p>
<figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">#<span class="array"># </span>[<span class="number">1</span>] <span class="string">"~/Dropbox/Homeworks/Data Analysis/DataAnalysisProject2/code"</span></div></pre></td></tr></table></figure>

<p>我把samsungData.rda放在~/Dropbox/Homeworks/Data Analysis/DataAnalysisProject2/data中，.rda文件的好处就是直接load一遍就可以把当时储存在内存中的object全部载入，而不需要再从txt或者csv文件中读取数据了。</p>
<figure class="highlight r"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">load(<span class="string">"../data/samsungData.rda"</span>)</div></pre></td></tr></table></figure>



<figure class="highlight r"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># number of rows</span></div><div class="line">nrow(samsungData)</div></pre></td></tr></table></figure>



<figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line"><span class="preprocessor">## [1] 7352</span></div></pre></td></tr></table></figure>



<figure class="highlight r"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># number of columns</span></div><div class="line">ncol(samsungData)</div></pre></td></tr></table></figure>



<figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line"><span class="preprocessor">## [1] 563</span></div></pre></td></tr></table></figure>

<p>一共有7352条记录，563个变量。</p>
<figure class="highlight r"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># number of subjects</span></div><div class="line">length(unique(samsungData$subject))</div></pre></td></tr></table></figure>



<figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line"><span class="preprocessor">## [1] 21</span></div></pre></td></tr></table></figure>

<p>这里subject代表志愿者编号，这里共有21个志愿者参与。<br>把所有变量的名称读出来，随便挑几个看看。</p>
<figure class="highlight r"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">cnames = names(samsungData)</div><div class="line"><span class="comment"># sample of features names</span></div><div class="line">sample(cnames, <span class="number">10</span>)</div></pre></td></tr></table></figure>



<figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">#<span class="array">#  </span>[<span class="number">1</span>] <span class="string">"tBodyGyroJerk-arCoeff()-Z,4"</span>  <span class="string">"fBodyAccJerk-meanFreq()-X"</span>   </div><div class="line">#<span class="array">#  </span>[<span class="number">3</span>] <span class="string">"fBodyAccJerk-skewness()-Y"</span>    <span class="string">"tBodyGyroJerk-arCoeff()-Y,3"</span> </div><div class="line">#<span class="array">#  </span>[<span class="number">5</span>] <span class="string">"fBodyGyro-bandsEnergy()-1,24"</span> <span class="string">"tBodyAcc-std()-Z"</span>            </div><div class="line">#<span class="array">#  </span>[<span class="number">7</span>] <span class="string">"tBodyGyroJerkMag-min()"</span>       <span class="string">"fBodyAccJerk-energy()-Y"</span>     </div><div class="line">#<span class="array">#  </span>[<span class="number">9</span>] <span class="string">"tBodyAccMag-mad()"</span>            <span class="string">"fBodyAcc-kurtosis()-Z"</span></div></pre></td></tr></table></figure>

<p>变量名称中有<code>-</code>，<code>()</code>，可能会使之后的分析工作出现异常，之后要把变量名处理一下。</p>
<figure class="highlight r"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># check if there are duplicated feature names</span></div><div class="line">length(unique(cnames))</div></pre></td></tr></table></figure>



<figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line"><span class="preprocessor">## [1] 479</span></div></pre></td></tr></table></figure>

<p>变量名中不重复的有479个，这里面居然会有重名的变量!!<br>原来是因为有有一些变量忘了标记它来源于加速度传感器与三轴陀螺仪中的X轴，Y轴还是Z轴了，因此重复的变量名都是三个成对地出现，比如<code>fBodyAcc-bandsEnergy()-1,8</code>，出现了三次。</p>
<figure class="highlight r"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">which(cnames == <span class="string">"fBodyAcc-bandsEnergy()-1,8"</span>)</div></pre></td></tr></table></figure>



<figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line"><span class="preprocessor">## [1] 303 317 331</span></div></pre></td></tr></table></figure>

<p>把<code>-X</code>,<code>-Y</code>,<code>-Z</code>补回去即可。前面提到要把<code>-</code>，<code>()</code>给去除掉，这里面用到一个小技巧，直接用把<code>data.frame(samsungData)</code>，即可把变量名中的怪异字符改成<code>.</code>。</p>
<figure class="highlight r"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># missing -X -Y -Z in columns 303~344, 382~423, 461~502</span></div><div class="line">cnames[<span class="number">303</span>:<span class="number">344</span>] = paste(cnames[<span class="number">303</span>:<span class="number">344</span>], rep(c(<span class="string">"-X"</span>, <span class="string">"-Y"</span>, <span class="string">"-Z"</span>), each = <span class="number">14</span>), </div><div class="line">    sep = <span class="string">""</span>)</div><div class="line">cnames[<span class="number">382</span>:<span class="number">423</span>] = paste(cnames[<span class="number">382</span>:<span class="number">423</span>], rep(c(<span class="string">"-X"</span>, <span class="string">"-Y"</span>, <span class="string">"-Z"</span>), each = <span class="number">14</span>), </div><div class="line">    sep = <span class="string">""</span>)</div><div class="line">cnames[<span class="number">461</span>:<span class="number">502</span>] = paste(cnames[<span class="number">461</span>:<span class="number">502</span>], rep(c(<span class="string">"-X"</span>, <span class="string">"-Y"</span>, <span class="string">"-Z"</span>), each = <span class="number">14</span>), </div><div class="line">    sep = <span class="string">""</span>)</div><div class="line">names(samsungData) = cnames</div><div class="line"><span class="comment"># remove '-' and '()' in column names</span></div><div class="line">samsungData = data.frame(samsungData)</div><div class="line"><span class="comment"># convert 'activity' from character to factor</span></div><div class="line">samsungData$activity = as.factor(samsungData$activity)</div></pre></td></tr></table></figure>

<p>这个数据集当中不存在空值，并且看了它的说明，每个变量都已经做过标准化处理，全部是数值型变量，数值范围 [-1,1]。因此之后就不需要对变量做其他处理了。</p>
<figure class="highlight r"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># check missing values</span></div><div class="line">sum(is.na(samsungData))</div></pre></td></tr></table></figure>



<figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line"><span class="preprocessor">## [1] 0</span></div></pre></td></tr></table></figure>

<hr>
<h3 id="数据集切分">数据集切分</h3>
<p>把原始数据集分为training set和test set。课上规定说1、3、5、6号志愿者必须在training set中，27、28、29、30必须在test set中。因此剩下的subject，就平均分配给这两个set。</p>
<figure class="highlight r"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">totSubjects = unique(samsungData$subject)</div><div class="line">trainSubjects = c(<span class="number">1</span>, <span class="number">3</span>, <span class="number">5</span>, <span class="number">6</span>)</div><div class="line">testSubjects = c(<span class="number">27</span>, <span class="number">28</span>, <span class="number">29</span>, <span class="number">30</span>)</div><div class="line"><span class="comment"># divide the remain subjects equally into train and test sets</span></div><div class="line">remainSubjects = setdiff(setdiff(totSubjects, trainSubjects), testSubjects)</div><div class="line">set.seed(<span class="number">123</span>)</div><div class="line">(trainSubjects = c(trainSubjects, sample(remainSubjects, ceiling(length(remainSubjects)/<span class="number">2</span>))))</div></pre></td></tr></table></figure>



<figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line"><span class="preprocessor">##  [1]  1  3  5  6 14 22 15 21 25  7 26</span></div></pre></td></tr></table></figure>



<figure class="highlight r"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">(testSubjects = c(testSubjects, setdiff(remainSubjects, trainSubjects)))</div></pre></td></tr></table></figure>



<figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line"><span class="preprocessor">##  [1] 27 28 29 30  8 11 16 17 19 23</span></div></pre></td></tr></table></figure>



<figure class="highlight r"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># split the data</span></div><div class="line">trainData = samsungData[samsungData$subject %<span class="keyword">in</span>% trainSubjects, ]</div><div class="line">testData = samsungData[samsungData$subject %<span class="keyword">in</span>% testSubjects, ]</div></pre></td></tr></table></figure>

<hr>
<h3 id="建模">建模</h3>
<h4 id="Classification_and_Regression_Tree_(C&amp;RT)">Classification and Regression Tree (C&amp;RT)</h4>
<p>这里直接把所有变量都拿去建模，让决策树自己去选择重要的变量。这里应该先尝试用PCA或者SVD降维一下会比较好， 但不想把过程搞的太复杂，因此降维这事情以后可以尝试一下。</p>
<figure class="highlight r"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">library</span>(tree)</div><div class="line">par(mfrow = c(<span class="number">1</span>, <span class="number">1</span>))</div><div class="line"><span class="comment"># 第562个变量是"subject"，志愿者编号，不纳入模型</span></div><div class="line">tree1 = tree(activity ~ ., data = trainData[, -<span class="number">562</span>])</div><div class="line">summary(tree1)</div></pre></td></tr></table></figure>



<figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line"><span class="preprocessor">## </span></div><div class="line"><span class="preprocessor">## Classification tree:</span></div><div class="line"><span class="preprocessor">## tree(formula = activity ~ ., data = trainData[, -562])</span></div><div class="line"><span class="preprocessor">## Variables actually used in tree construction:</span></div><div class="line"><span class="preprocessor">## [1] "fBodyAccJerk.bandsEnergy...1.16.X" "tGravityAcc.mean...X"             </span></div><div class="line"><span class="preprocessor">## [3] "tGravityAcc.max...Y"               "tBodyAcc.max...X"                 </span></div><div class="line"><span class="preprocessor">## [5] "tGravityAcc.arCoeff...Y.2"         "tBodyAccMag.arCoeff..1"           </span></div><div class="line"><span class="preprocessor">## [7] "tGravityAcc.energy...Y"            "fBodyAccMag.mad.."                </span></div><div class="line"><span class="preprocessor">## Number of terminal nodes:  9 </span></div><div class="line"><span class="preprocessor">## Residual mean deviance:  0.496 = 1880 / 3800 </span></div><div class="line"><span class="preprocessor">## Misclassification error rate: 0.0825 = 314 / 3804</span></div></pre></td></tr></table></figure>

<p>C&amp;RT最终挑选了8个变量。其中训练集的表现是错分率8.25%，测试集的表现是错分率14.43%。</p>
<figure class="highlight r"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">sum(testData$activity != predict(tree1, newdata = testData, type = <span class="string">"class"</span>))/nrow(testData)</div></pre></td></tr></table></figure>



<figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line"><span class="preprocessor">## [1] 0.1443</span></div></pre></td></tr></table></figure>

<p>看看test set的<a href="http://en.wikipedia.org/wiki/Confusion_matrix" target="_blank" rel="external">confusion matrix</a>。发现sitting和standing之间搞错的比较多，walk, walkdown, walkup之间错的也不少</p>
<figure class="highlight r"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># confusion matrix</span></div><div class="line">table(predict(tree1, newdata = testData, type = <span class="string">"class"</span>), testData$activity)</div></pre></td></tr></table></figure>



<figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line"><span class="preprocessor">##           </span></div><div class="line"><span class="preprocessor">##            laying sitting standing walk walkdown walkup</span></div><div class="line"><span class="preprocessor">##   laying      699      15        0    0        0      0</span></div><div class="line"><span class="preprocessor">##   sitting       0     528      139    0        0      0</span></div><div class="line"><span class="preprocessor">##   standing      0      94      542    0        0      0</span></div><div class="line"><span class="preprocessor">##   walk          0       0        0  472       35     59</span></div><div class="line"><span class="preprocessor">##   walkdown      0       0        0   67      419     66</span></div><div class="line"><span class="preprocessor">##   walkup        1       0        0   20       16    376</span></div></pre></td></tr></table></figure>

<p>用<a href="http://zh.wikipedia.org/wiki/%E4%BA%A4%E5%8F%89%E9%A9%97%E8%AD%89" target="_blank" rel="external">10-fold cross-validation</a>来测试一下树的内部节点个数定为多少比较合适，太少准确度低，太多容易overfitting。</p>
<figure class="highlight r"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">par(mfrow = c(<span class="number">1</span>, <span class="number">2</span>))</div><div class="line"><span class="comment"># do k-fold validation and choose the best tree</span></div><div class="line">plot(cv.tree(tree1, FUN = prune.tree, method = <span class="string">"misclass"</span>))</div><div class="line">plot(cv.tree(tree1, FUN = prune.tree, method = <span class="string">"deviance"</span>))</div></pre></td></tr></table></figure>

<p><img src="/imgs/har-random_forest/har_random_forset_fig1.png" alt=""></p>
<p>由上图可以看出，随着tree size的增大，错误率是在不断减少的，在内部节点数为8的时候最低，(又根据树的停止规则，它不会继续长超过8)，因此内部节点数为8是比较好的选择。下面的code可以把这棵树画出来:</p>
<figure class="highlight r"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">plot(tree1)</div><div class="line">text(tree1)</div></pre></td></tr></table></figure>

<p><img src="/imgs/har-random_forest/har_random_forset_fig2.png" alt=""></p>
<h4 id="Random_Forests_随机森林">Random Forests 随机森林</h4>
<p>关于随机森林的介绍，可以参考前一篇<a href="http://beader.me/2013/12/01/random-forests/" target="_blank" rel="external">《林子大了，什么树都有》</a></p>
<figure class="highlight r"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">library</span>(randomForest)</div><div class="line">rf1 = randomForest(activity ~ ., data = trainData[, -<span class="number">562</span>])</div></pre></td></tr></table></figure>



<figure class="highlight r"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">par(mfrow = c(<span class="number">1</span>, <span class="number">1</span>))</div><div class="line">varImpPlot(rf1, n.var = <span class="number">10</span>, main = <span class="string">"Random Forests: Top 10 Important Variables"</span>)</div></pre></td></tr></table></figure>

<p><img src="/imgs/har-random_forest/har_random_forset_fig3.png" alt=""></p>
<p>random forest中最重要的10个变量。</p>
<figure class="highlight r"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">(rf1)</div></pre></td></tr></table></figure>

<p>下面可以看到random forest在trainning set当中惊人的表现。1.31%的错分率。</p>
<figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div></pre></td><td class="code"><pre><div class="line"><span class="preprocessor">## </span></div><div class="line"><span class="preprocessor">## Call:</span></div><div class="line"><span class="preprocessor">##  randomForest(formula = activity ~ ., data = trainData[, -562]) </span></div><div class="line"><span class="preprocessor">##                Type of random forest: classification</span></div><div class="line"><span class="preprocessor">##                      Number of trees: 500</span></div><div class="line"><span class="preprocessor">## No. of variables tried at each split: 23</span></div><div class="line"><span class="preprocessor">## </span></div><div class="line"><span class="preprocessor">##         OOB estimate of  error rate: 1.31%</span></div><div class="line"><span class="preprocessor">## Confusion matrix:</span></div><div class="line"><span class="preprocessor">##          laying sitting standing walk walkdown walkup class.error</span></div><div class="line"><span class="preprocessor">## laying      707       0        0    0        0      0    0.000000</span></div><div class="line"><span class="preprocessor">## sitting       0     634       14    0        0      1    0.023112</span></div><div class="line"><span class="preprocessor">## standing      0      18      675    0        0      0    0.025974</span></div><div class="line"><span class="preprocessor">## walk          0       0        0  662        2      3    0.007496</span></div><div class="line"><span class="preprocessor">## walkdown      0       0        0    2      509      5    0.013566</span></div><div class="line"><span class="preprocessor">## walkup        0       0        0    0        5    567    0.008741</span></div></pre></td></tr></table></figure>

<p>再来看看test set的表现。8.37%的错分率，可见还是存在一点的overfitting，不过还好不算太严重。与之前单棵决策树相比，准确率高了不少。</p>
<figure class="highlight r"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># confusion matrix</span></div><div class="line">table(predict(rf1, newdata = testData), testData$activity)</div></pre></td></tr></table></figure>



<figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line"><span class="preprocessor">##           </span></div><div class="line"><span class="preprocessor">##            laying sitting standing walk walkdown walkup</span></div><div class="line"><span class="preprocessor">##   laying      700      15        0    0        0      0</span></div><div class="line"><span class="preprocessor">##   sitting       0     538      132    0        0      0</span></div><div class="line"><span class="preprocessor">##   standing      0      84      549    0        0      0</span></div><div class="line"><span class="preprocessor">##   walk          0       0        0  534        2     17</span></div><div class="line"><span class="preprocessor">##   walkdown      0       0        0   20      450      4</span></div><div class="line"><span class="preprocessor">##   walkup        0       0        0    5       18    480</span></div></pre></td></tr></table></figure>



<figure class="highlight r"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># misclassification error rate on test set</span></div><div class="line">sum(predict(rf1, newdata = testData) != testData$activity)/length(testData$activity)</div></pre></td></tr></table></figure>



<figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line"><span class="preprocessor">## [1] 0.08371</span></div></pre></td></tr></table></figure>

<hr>
<h3 id="总结">总结</h3>
<p>这次实验所用的数据是已经经过相关领域专家处理过的，并不是传感器采集到的raw data，这当中需要大量的相关知识，并且数据已经经过一定程度的标准化处理，因此不需要多做处理，分析难度大大降低。除此之外实验的目的是为了比较单一的决策树与随机森林这种ensemble的方法在分类准确度上的差异，因此没有费很大的功夫去想降维的方法。在实际应用中，降维是必须的，因为这种复杂的模型所需的计算量是比较大的，如果想在手机上进行预测，无法采用这么暴力的方法。除此之外，还可以尝试使用其他模型，譬如SVM，我用的是e1071这个r package，发现SVM的效果要比random forest要好一些，这里就不详述了。</p>
]]></content>
    <category scheme="http://beader.me/tags/human-activity-recognition/" term="human activity recognition"/>
    <category scheme="http://beader.me/tags/classification-tree/" term="classification tree"/>
    <category scheme="http://beader.me/tags/random-forest/" term="random forest"/>
  </entry>
  <entry>
    <title type="html"><![CDATA[Random Forests - 林子大了，什么树都有]]></title>
    <link href="http://beader.me/2013/12/01/random-forests/"/>
    <id>http://beader.me/2013/12/01/random-forests/</id>
    <published>2013-12-01T14:06:27.000Z</published>
    <updated>2013-12-08T15:15:34.000Z</updated>
    <content type="html"><![CDATA[<p>本文参考自<a href="http://blog.echen.me/2011/03/14/laymans-introduction-to-random-forests/" target="_blank" rel="external">Layman’s Introduction to Random Forests</a></p>
<p>&emsp;&emsp;在machine learning中，随机森林(Random Forests)是一个包含多个决策树(<strong>decision tree</strong>)的分类器，其输出的类别由个别树输出的类别的众数决定[1]。如何从一个比较通俗易懂的角度来理解这座森林呢?<br>&emsp;&emsp;某天你想看某部电影但你不确定自己是否会喜欢，此时你可能会问你的好友小赵，问他认为你会不会喜欢这部电影。这个小赵是个非常认真负责的人，他想先了解了解你喜欢哪种类型的电影。于是你给了小赵一份电影清单，上面写了你喜欢哪些电影，不喜欢哪些电影(a labeled training set)。于是小赵去豆瓣上查这些电影的相关信息，他把每一部电影转换成一个特征向量:(导演，编剧，主演，类型，国家，语言，片长…)，接着在小赵心目中，就建立起了一套判断准则，这个判断准则中有一系列的衡量标准，譬如<code>动作片+有xxx出演-&gt;喜欢</code>，或者<code>爱情片+悲剧-&gt;不喜欢</code>等等。此时小赵就变成了一棵<strong>decision tree</strong>。你问小赵你会不会喜欢某部电影，小赵就会根据之前建立起来的规则，猜测你喜欢或者不喜欢。<br><a id="more"></a><br>&emsp;&emsp;但是小赵只是一个人类，他并不总能很好的归纳出你的偏好。为了得到更加准确的建议，你可能不仅仅去问小赵，而是去问更多的朋友小钱、小孙、小李、小周、小吴。此时每个好友都代表一颗<strong>decision tree</strong>，而每次你想知道你是否会喜欢某部电影X时，你会问每个好友，当多数人认为你会喜欢电影X时，你才去看它。此时你就拥有了一座充满<strong>decision tree</strong>的森林，专业一点的讲法是一个<strong>ensemble classifier</strong>。<br>&emsp;&emsp;前面说到，你在建立<strong>decision tree</strong>时，需要给每个人一份记录你喜欢和不喜欢电影的清单，但是如果你给每个人一模一样电影清单，那每个人变成的<strong>decision tree</strong>也会是一模一样，这样的话问那么多人就没有意义了。为了让每棵<strong>decision</strong>都长得不一样，你打算给每个人的电影清单也不大一样。为什么要这么做呢？因为事实上连你自己也无法确保你给的电影清单是完全准确的。譬如说你告诉小赵你非常喜欢《泰坦尼克》，但有可能你喜欢它仅仅是因为看《泰坦尼克》的那天是你生日，你的心情很愉悦，你并不是真的有非常喜欢，因此这条记录是有偏差的，如果不把你喜欢《泰坦尼克》这件事告诉其他的朋友，那么其他的朋友所建立的<strong>decision tree</strong>就不会受到这个偏差的影响。因此你给每个朋友的电影清单都有略微的不同，譬如你告诉小赵你喜欢《盗梦空间》和《泰坦尼克》，讨厌《阿凡达》，但是告诉小张你喜欢《盗梦空间》，讨厌《阿凡达》但是没有提到《泰坦尼克》。这么一来小赵和小张建立的<strong>decision tree</strong>就可能会有很大的不同。<br>&emsp;&emsp;利用这种<strong>ensemble</strong>的方法，每个好友对你的了解都不一样，因此可能给出一些具有特质化的意见，小赵可能认为你喜欢看莱昂纳多主演的电影(因为你告诉他你喜欢《盗梦空间》和《泰坦尼克》)，而小张可能认为你喜欢看爱情片(因为你只告诉他你喜欢《泰坦尼克》)。<br>&emsp;&emsp;除此之外，还有一种情况，你同时喜欢某几部电影，并不是因为这几部电影都是由某个电影明星主演，而是因为其他方面的原因，对于这些电影，你并不希望你朋友在建立<strong>decision tree</strong>的时候考虑到电影主演的信息，即你希望把<strong>主演</strong>从电影的特征向量中删去。<br>&emsp;&emsp;这样一来，你的这些好友所建立的<strong>decision tree</strong>就成为了一座森林。当你想知道你是否会喜欢某部电影时，你会去问这座森林里的每一棵树，当大部分树认为你会喜欢时，你就去看这部电影。 </p>
<p>&emsp;&emsp;之前提到的电影清单以及电影的特征向量，我们把他用表格来描述一下，可以长成这样：  </p>
<table>
<thead>
<tr>
<th style="text-align:center">movie</th>
<th style="text-align:center">喜欢</th>
<th style="text-align:center">导演</th>
<th style="text-align:center">编剧</th>
<th style="text-align:center">主演</th>
<th style="text-align:center">类型</th>
<th style="text-align:center">国家</th>
<th style="text-align:center">语言</th>
<th style="text-align:center">…</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">a</td>
<td style="text-align:center">1</td>
<td style="text-align:center">xxx</td>
<td style="text-align:center">foo</td>
<td style="text-align:center">iii</td>
<td style="text-align:center">爱情</td>
<td style="text-align:center">法国</td>
<td style="text-align:center">法语</td>
<td style="text-align:center">…</td>
</tr>
<tr>
<td style="text-align:center">b</td>
<td style="text-align:center">0</td>
<td style="text-align:center">yyy</td>
<td style="text-align:center">bar</td>
<td style="text-align:center">jjj</td>
<td style="text-align:center">动作</td>
<td style="text-align:center">中国</td>
<td style="text-align:center">中文</td>
<td style="text-align:center">…</td>
</tr>
<tr>
<td style="text-align:center">c</td>
<td style="text-align:center">1</td>
<td style="text-align:center">zzz</td>
<td style="text-align:center">baz</td>
<td style="text-align:center">kkk</td>
<td style="text-align:center">悬疑</td>
<td style="text-align:center">美国</td>
<td style="text-align:center">英文</td>
<td style="text-align:center">…</td>
</tr>
<tr>
<td style="text-align:center">…</td>
<td style="text-align:center">…</td>
<td style="text-align:center">…</td>
<td style="text-align:center">…</td>
<td style="text-align:center">…</td>
<td style="text-align:center">…</td>
<td style="text-align:center">…</td>
<td style="text-align:center">…</td>
<td style="text-align:center">…</td>
</tr>
</tbody>
</table>
<p>&emsp;&emsp;那么我们可以来总结一下:</p>
<blockquote>
<p>随机森林中每一棵树建立的依据是随机的，这个随机体现在两个方面:</p>
<ol>
<li>建立模型时使用的data是随机的。(random in row - data level)</li>
<li>建立模型时使用的features是随机的。(random in column - model level)</li>
</ol>
</blockquote>
<h3 id="Reference:">Reference:</h3>
<ol>
<li><a href="http://zh.wikipedia.org/wiki/%E9%9A%8F%E6%9C%BA%E6%A3%AE%E6%9E%97" target="_blank" rel="external">wikipedia-随机森林</a></li>
</ol>
<hr>
<blockquote>
<p>下集预告：<br>实践是检验真理的唯一标准，下一次我们将实战一下 <strong>random forests</strong> ，使用智能手机的加速度传感器以及陀螺仪的数据来预测当前用户的活动状态(站立，行走，跑步….)。</p>
</blockquote>
]]></content>
    <category scheme="http://beader.me/tags/random-forest/" term="random forest"/>
  </entry>
  <entry>
    <title type="html"><![CDATA[Word Cloud in R]]></title>
    <link href="http://beader.me/2013/11/25/word-cloud-in-r/"/>
    <id>http://beader.me/2013/11/25/word-cloud-in-r/</id>
    <published>2013-11-24T20:56:49.000Z</published>
    <updated>2014-09-01T01:05:45.000Z</updated>
    <content type="html"><![CDATA[<p>本文参考自<a href="http://onertipaday.blogspot.tw/2011/07/word-cloud-in-r.html" target="_blank" rel="external">One R Tip A Day- Word Cloud in R</a>  </p>
<hr>
<h2 id="研究CRAN中R_Packages所关注的最热门的领域">研究CRAN中R Packages所关注的最热门的领域</h2>
<p>以下的例子将从<a href="http://cran.r-project.org/web/packages/available_packages_by_date.html" target="_blank" rel="external">Available CRAN Packages</a>页面上抓取目前CRAN上所有的R包，提取当中的title，及各个R包的简介，用来分析R包所涵盖的热门领域。</p>
<figure class="highlight r"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># 载入必要的R包</span></div><div class="line"><span class="keyword">require</span>(XML)</div><div class="line"><span class="keyword">require</span>(tm)</div><div class="line"></div><div class="line">u = <span class="string">"http://cran.r-project.org/web/packages/available_packages_by_date.html"</span></div><div class="line"><span class="comment"># 这个readHTMLTable可以直接读取网页中的table并转换为data frame</span></div><div class="line">t = readHTMLTable(u)[[<span class="number">1</span>]]</div><div class="line"><span class="comment"># 观察一下这个table</span></div><div class="line">head(t)</div></pre></td></tr></table></figure>

<a id="more"></a>

<figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div></pre></td><td class="code"><pre><div class="line"><span class="preprocessor">##        Date   Package </span></div><div class="line"><span class="preprocessor">## 1 2013-11-21     bbmle</span></div><div class="line"><span class="preprocessor">## 2 2013-11-21    BSagri</span></div><div class="line"><span class="preprocessor">## 3 2013-11-21   dbstats</span></div><div class="line"><span class="preprocessor">## 4 2013-11-21     Hmisc</span></div><div class="line"><span class="preprocessor">## 5 2013-11-21    lestat</span></div><div class="line"><span class="preprocessor">## 6 2013-11-21       ncf</span></div><div class="line"><span class="preprocessor">##                                                                    Title </span></div><div class="line"><span class="preprocessor">## 1                         Tools for general maximum likelihood estimation</span></div><div class="line"><span class="preprocessor">## 2 Statistical methods for safety assessment in agricultural field\ntrials</span></div><div class="line"><span class="preprocessor">## 3                                     Distance-based statistics (dbstats)</span></div><div class="line"><span class="preprocessor">## 4                                                   Harrell Miscellaneous</span></div><div class="line"><span class="preprocessor">## 5                                       A package for LEarning STATistics</span></div><div class="line"><span class="preprocessor">## 6                              spatial nonparametric covariance functions</span></div></pre></td></tr></table></figure>



<figure class="highlight r"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line"></div><div class="line"><span class="comment"># 利用Title建立语料库</span></div><div class="line">ap.corpus &lt;- Corpus(DataframeSource(data.frame(as.character(t[, <span class="number">3</span>]))))</div><div class="line"><span class="comment"># 可以用tm_map函数对语料库中的词汇进行批量修改 移除标点</span></div><div class="line">ap.corpus &lt;- tm_map(ap.corpus, removePunctuation)</div><div class="line"><span class="comment"># 英文字母转小写</span></div><div class="line">ap.corpus &lt;- tm_map(ap.corpus, tolower)</div><div class="line"><span class="comment"># 删除一些stopwords</span></div><div class="line">ap.corpus &lt;- tm_map(ap.corpus, <span class="keyword">function</span>(x) removeWords(x, stopwords(<span class="string">"english"</span>)))</div><div class="line"></div><div class="line"><span class="comment"># 可以看看stopwords都是什么</span></div><div class="line">head(stopwords(<span class="string">"english"</span>))</div></pre></td></tr></table></figure>



<figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">#<span class="array"># </span>[<span class="number">1</span>] <span class="string">"i"</span>      <span class="string">"me"</span>     <span class="string">"my"</span>     <span class="string">"myself"</span> <span class="string">"we"</span>     <span class="string">"our"</span></div></pre></td></tr></table></figure>

<p>下面要把语料库转换成一个Term-Document Matrix[1]。一个document可能是一句话，可以可能是一篇文章。Term-Document Matrix的row表示某个一个document，column表示该document各个词汇出现的次数。下面是一个Term-Document Matrix的简单例子:</p>
<ul>
<li>D1 = “I like databases”</li>
<li>D2 = “I hate databases”</li>
</ul>
<p>则Term-Document Matrix将是这样:</p>
<table>
<thead>
<tr>
<th style="text-align:center"></th>
<th style="text-align:center">I</th>
<th style="text-align:center">like</th>
<th style="text-align:center">hate</th>
<th style="text-align:center">databases    </th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center"><strong>D1</strong></td>
<td style="text-align:center">1</td>
<td style="text-align:center">1</td>
<td style="text-align:center">0</td>
<td style="text-align:center">1</td>
</tr>
<tr>
<td style="text-align:center"><strong>D2</strong></td>
<td style="text-align:center">1</td>
<td style="text-align:center">0</td>
<td style="text-align:center">1</td>
<td style="text-align:center">1  </td>
</tr>
</tbody>
</table>
<figure class="highlight r"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">ap.tdm &lt;- TermDocumentMatrix(ap.corpus)</div><div class="line">ap.m &lt;- as.matrix(ap.tdm)</div><div class="line"></div><div class="line"><span class="comment"># 计算Term-Document Matrix中每个单词的词频，并且按照词频降序排序</span></div><div class="line">ap.v &lt;- sort(rowSums(ap.m), decreasing = <span class="literal">TRUE</span>)</div><div class="line">ap.d &lt;- data.frame(word = names(ap.v), freq = ap.v)</div><div class="line">head(ap.d)</div></pre></td></tr></table></figure>



<figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line"><span class="preprocessor">##                  word freq</span></div><div class="line"><span class="preprocessor">## data             data  716</span></div><div class="line"><span class="preprocessor">## analysis     analysis  594</span></div><div class="line"><span class="preprocessor">## models         models  430</span></div><div class="line"><span class="preprocessor">## functions   functions  331</span></div><div class="line"><span class="preprocessor">## package       package  288</span></div><div class="line"><span class="preprocessor">## regression regression  246</span></div></pre></td></tr></table></figure>



<figure class="highlight r"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">require</span>(wordcloud)</div><div class="line"><span class="keyword">require</span>(RColorBrewer)</div><div class="line"><span class="comment"># 设置一个调色板</span></div><div class="line">pal2 &lt;- brewer.pal(<span class="number">8</span>, <span class="string">"Dark2"</span>)</div><div class="line"><span class="comment"># 导出成png</span></div><div class="line">png(<span class="string">"wordcloud_packages.png"</span>, width = <span class="number">1280</span>, height = <span class="number">800</span>)</div><div class="line">wordcloud(ap.d$word, ap.d$freq, scale = c(<span class="number">8</span>, <span class="number">0.2</span>), min.freq = <span class="number">3</span>, max.words = <span class="literal">Inf</span>, </div><div class="line">    random.order = <span class="literal">FALSE</span>, rot.per = <span class="number">0.15</span>, colors = pal2)</div><div class="line">dev.off()</div></pre></td></tr></table></figure>



<figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="preprocessor">## pdf </span></div><div class="line"><span class="preprocessor">##   2</span></div></pre></td></tr></table></figure>

<p>最终的输出效果如图:</p>
<p><img src="/imgs/word-cloud-in-r/wordcloud.png" alt="wordcloud"></p>
<hr>
<h2 id="Reference:">Reference:</h2>
<p>1.<a href="http://en.wikipedia.org/wiki/Document-term_matrix" target="_blank" rel="external">Term-Document Matrix</a></p>
]]></content>
    <category scheme="http://beader.me/tags/r/" term="r"/>
    <category scheme="http://beader.me/tags/text-mining/" term="text mining"/>
    <category scheme="http://beader.me/tags/word-cloud/" term="word cloud"/>
  </entry>
</feed>
